{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tanzania Tourism Prediction - Prognozy dotyczące turystyki w Tanzanii\n",
    "<h2>Autorzy:</h2><br>\n",
    "<ul>\n",
    "<li>Piotr Janiszek 247678</li>\n",
    "<li>Kacper Białek 247629</li>\n",
    "<li>Franciszek Pawlak 247756</li>\n",
    "<li>Michał Korblit 242427</li>\n",
    "</ul>"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "is_test_iteration = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "id": "7613fbacfe81bc40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Imports</h3>",
   "id": "259a10b6942f3e3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import optuna\n",
    "\n"
   ],
   "id": "a8d8b4bcc8072682",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Data loadind</h3>\n",
   "id": "bbebe9a3844f4307"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv('data/Train.csv')\n",
    "df_regions = pd.read_csv('data/regions.csv')\n",
    "\n",
    "corrections = {\n",
    "    'SWIZERLAND': 'SWITZERLAND',\n",
    "    'MALT': 'MALTA',\n",
    "    'BURGARIA': 'BULGARIA',\n",
    "    'DRC': 'CONGO (DEMOCRATIC REPUBLIC OF THE)',\n",
    "    'KOREA': 'SOUTH KOREA',\n",
    "    'SWAZILAND': 'ESWATINI',\n",
    "    'UKRAIN': 'UKRAINE',\n",
    "    'TRINIDAD TOBACCO': 'TRINIDAD AND TOBAGO',\n",
    "    'COMORO': 'COMOROS',\n",
    "    'COSTARICA': 'COSTA RICA',\n",
    "    'PHILIPINES': 'PHILIPPINES',\n",
    "    'IVORY COAST': \"CÔTE D'IVOIRE\",\n",
    "    'DJIBOUT': 'DJIBOUTI',\n",
    "    'MORROCO': 'MOROCCO',\n",
    "    'UNITED STATES OF AMERICA': 'UNITED STATES',\n",
    "    'UAE': 'UNITED ARAB EMIRATES',\n",
    "    'SCOTLAND': 'UNITED KINGDOM',\n",
    "    'CAPE VERDE': 'CABO VERDE',\n",
    "}\n",
    "\n",
    "df_train['country'] = df_train['country'].replace(corrections)\n",
    "df_regions = df_regions[['name', 'sub-region']]\n",
    "df_regions = df_regions.rename(columns={'sub-region': 'region'})\n",
    "df_regions['name'] = df_regions['name'].str.upper()\n",
    "df_train = pd.merge(df_train, df_regions, how='left', left_on='country', right_on='name')\n",
    "df_train = df_train.drop(columns=['name'])\n",
    "\n",
    "print(df_train.columns)\n",
    "print(df_regions.columns)\n"
   ],
   "id": "3e60d14fd29bf9cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Data Cleaning</h3>",
   "id": "e81356a754097e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "count = df_train.isna().any(axis=1).sum()\n",
    "df_train.loc[df_train['most_impressing'].isna(), 'most_impressing'] = 'No comments'\n",
    "count2 = df_train.isna().any(axis=1).sum()\n",
    "\n",
    "mask_valid = (df_train['total_male'].notna()) & \\\n",
    "             (df_train['total_female'].notna()) & \\\n",
    "             ((df_train['total_male'] + df_train['total_female']) != 0)\n",
    "df_train.loc[mask_valid, 'total_cost_per_person'] = df_train.loc[mask_valid, 'total_cost'] / (\n",
    "            df_train.loc[mask_valid, 'total_male'] + df_train.loc[mask_valid, 'total_female'])\n",
    "mean = df_train.loc[mask_valid, 'total_cost_per_person'].mean()\n",
    "print(\"Średnia bez wierszy z zerową sumą:\", mean)\n",
    "\n",
    "\n",
    "# idzie przez kolumny z nan\n",
    "for index, row in df_train[df_train.isna().any(axis=1)].iterrows():\n",
    "    if (pd.isna(row['total_male']) | pd.isna(row['total_female'])) & (not (pd.isna(row['total_male']) & pd.isna(row['total_female']))):\n",
    "        if pd.isna(row['total_male']):\n",
    "            difference = row['total_cost'] - (row['total_female'] * mean)\n",
    "            person_left = difference / mean\n",
    "            person_left = round(person_left, 0)\n",
    "\n",
    "            if person_left < 0:\n",
    "                person_left = 0\n",
    "            #print(f\"Inserting {person_left}\")\n",
    "            df_train.loc[index, 'total_male'] = person_left\n",
    "\n",
    "        else:\n",
    "            difference = row['total_cost'] - (row['total_male'] * mean)\n",
    "\n",
    "            person_left = difference / mean\n",
    "            person_left = round(person_left, 0)\n",
    "\n",
    "            if person_left < 0:\n",
    "                person_left = 0\n",
    "            #print(f\"Inserting {person_left}\")\n",
    "            df_train.loc[index, 'total_female'] = person_left\n",
    "\n",
    "for index, row in df_train[df_train.isna().any(axis=1)].iterrows():\n",
    "    # Completing the travel_with field with the value Alone, when the number of people shows that he/she travels alone\n",
    "    if pd.isna(row['travel_with']) & ((row['total_male'] + row['total_female']) == 1):\n",
    "        df_train.loc[index, 'travel_with'] = 'Alone'\n"
   ],
   "id": "da3f9d979344bc1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Random forest imputation\n",
    "mask_valid = (df_train['total_male'].notna()) & \\\n",
    "             (df_train['total_female'].notna()) & \\\n",
    "             ((df_train['total_male'] + df_train['total_female']) != 0)\n",
    "df_train.loc[mask_valid, 'total_cost_per_person'] = df_train.loc[mask_valid, 'total_cost'] / (\n",
    "            df_train.loc[mask_valid, 'total_male'] + df_train.loc[mask_valid, 'total_female'])\n",
    "mean = df_train.loc[mask_valid, 'total_cost_per_person'].mean()\n",
    "\n",
    "features = ['region', 'age_group', 'total_female',\n",
    "       'total_male', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "# Określ dozwolone kategorie - Model 1\n",
    "allowed_categories = ['Friends/Relatives', 'Children']\n",
    "# Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "df_train_imp = df_train[df_train['travel_with'].isin(allowed_categories)].copy()\n",
    "X_train = pd.get_dummies(df_train_imp[features], columns=features_cat)\n",
    "y_train = df_train_imp['travel_with']\n",
    "# Trenowanie modelu\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Określ dozwolone kategorie - Model 2\n",
    "allowed_categories2 = ['Children', 'Friends/Relatives', 'Spouse', 'Spouse and Children']\n",
    "# Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "df_train_imp2 = df_train[df_train['travel_with'].isin(allowed_categories2)].copy()\n",
    "X_train2 = pd.get_dummies(df_train_imp2[features], columns=features_cat)\n",
    "y_train2 = df_train_imp2['travel_with']\n",
    "# Trenowanie modelu\n",
    "rf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf2.fit(X_train2, y_train2)\n",
    "\n",
    "# Określ dozwolone kategorie - Model 3\n",
    "features3 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat3 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "def designate_sex(row):\n",
    "    if (row['travel_with'] == 'Alone') and ((row['total_male'] + row['total_female']) == 1):\n",
    "        # Jeśli total_male == 1, przyjmujemy, że to mężczyzna, w przeciwnym razie kobieta\n",
    "        return 'Male' if row['total_male'] == 1 else 'Female'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_train['gender'] = df_train.apply(designate_sex, axis=1)\n",
    "df_model = df_train[df_train['gender'].notna()].copy()\n",
    "\n",
    "# Wybór cech (features) i zmienna docelowa (target)\n",
    "X_train3 = df_model[features3]\n",
    "y_train3 = df_model['gender']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), features_cat3)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "model_pipeline.fit(X_train3, y_train3)\n",
    "\n",
    "# Predykcja płci dzieci - Model 4\n",
    "features4 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat4 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "df_train['male_children'] = df_train['total_male'].apply(lambda x: max(x - 1, 0))\n",
    "df_train['female_children'] = df_train['total_female'].apply(lambda x: max(x - 1, 0))\n",
    "\n",
    "df_filtered = df_train[(df_train['total_male'] > 0) & (df_train['total_female'] > 0)].copy()\n",
    "df_children_model = df_filtered[df_filtered['travel_with'] == 'Spouse and Children'].copy()\n",
    "\n",
    "# Przygotowanie macierzy cech\n",
    "X_train_child = pd.get_dummies(df_children_model[features4], columns=features_cat4)\n",
    "\n",
    "# Przygotowanie macierzy target – dwie kolumny: liczba dzieci mężczyzn i dzieci kobiet\n",
    "y_train_child = df_children_model[['male_children', 'female_children']]\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "multioutput_rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "# Trenowanie modelu\n",
    "multioutput_rf.fit(X_train_child, y_train_child)\n",
    "\n",
    "# Predykcja podziału dzieci oraz dorosłego - Model 5\n",
    "features5 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat5 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "df_filtered = df_train[(df_train['total_male'] + df_train['total_female'] > 1)].copy()\n",
    "df_children_only_model = df_filtered[df_filtered['travel_with'] == 'Children'].copy()\n",
    "\n",
    "X_train_child_only = pd.get_dummies(df_children_only_model[features5], columns=features_cat5)\n",
    "y_train_child_only = df_children_only_model[['total_male', 'total_female']]\n",
    "\n",
    "multioutput_rf_children_only = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "multioutput_rf_children_only.fit(X_train_child_only, y_train_child_only)\n",
    "\n",
    "# Predykcja podziału przyjaciół - Model 6\n",
    "features6 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat6 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "df_filtered = df_train[(df_train['total_male'] + df_train['total_female'] > 1)].copy()\n",
    "df_friend_model = df_filtered[df_filtered['travel_with'] == 'Friends/Relatives'].copy()\n",
    "\n",
    "X_train_friends = pd.get_dummies(df_friend_model[features6], columns=features_cat6)\n",
    "y_train_friends = df_friend_model[['total_male', 'total_female']]\n",
    "\n",
    "multioutput_rf_friends = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "multioutput_rf_friends.fit(X_train_friends, y_train_friends)\n",
    "\n",
    "# predykcja kategori  - Model 7\n",
    "features7 = ['region', 'age_group', 'total_female',\n",
    "       'total_male', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat7 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "\n",
    "allowed_categories7 = ['Friends/Relatives', 'Children', 'Spouse and Children']\n",
    "# Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "df_train_imp7 = df_train[df_train['travel_with'].isin(allowed_categories7)].copy()\n",
    "X_train7 = pd.get_dummies(df_train_imp7[features7], columns=features_cat7)\n",
    "y_train7 = df_train_imp7['travel_with']\n",
    "# Trenowanie modelu\n",
    "rf7 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf7.fit(X_train7, y_train7)\n",
    "\n",
    "# Predykcja podziału dla wycieczki widmo - Model 8\n",
    "features8 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat8 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "df_filtered_phantom = df_train[(df_train['total_male'] + df_train['total_female'] > 1)].copy()\n",
    "df_phantom_model = df_filtered_phantom[(df_filtered_phantom['travel_with'] == 'Friends/Relatives') | (df_filtered_phantom['travel_with'] == 'Spouse and Children') | (df_filtered_phantom['travel_with'] == 'Children')].copy()\n",
    "\n",
    "X_train_phantom = pd.get_dummies(df_phantom_model[features8], columns=features_cat8)\n",
    "y_train_phantom = df_phantom_model[['total_male', 'total_female']]\n",
    "\n",
    "multioutput_rf_phantom = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "multioutput_rf_phantom.fit(X_train_phantom, y_train_phantom)\n",
    "\n",
    "# Predykcja travel with dla rekordu widmo\n",
    "allowed_categories8 = ['Children', 'Friends/Relatives', 'Spouse and Children']\n",
    "# Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "df_train_imp8 = df_train[df_train['travel_with'].isin(allowed_categories8)].copy()\n",
    "X_train8 = pd.get_dummies(df_train_imp8[features8], columns=features_cat8)\n",
    "y_train8 = df_train_imp8['travel_with']\n",
    "# Trenowanie modelu\n",
    "rf8 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf8.fit(X_train8, y_train8)\n",
    "\n",
    "# Imputation\n",
    "for index, row in df_train[df_train.isna().any(axis=1)].iterrows():\n",
    "    # Completing the travel_with field with the estimated value when only one gender participated in the trip and the number of people exceeds one\n",
    "    if (pd.isna(row['travel_with']) & ((row['total_male'] + row['total_female']) > 1) &\n",
    "            ((row['total_male'] == 0 | pd.isna(row['total_male'])) | (row['total_female'] == 0 | pd.isna(row['total_female'])))):\n",
    "\n",
    "        dummy_df = pd.get_dummies(row[features], columns=features_cat)\n",
    "        dummy_df.columns = dummy_df.columns.astype(str)\n",
    "        dummy_df = dummy_df.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "        predicted_value = rf.predict(dummy_df)\n",
    "\n",
    "        df_train.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "    elif (row['total_male'] + row['total_female'] > 1) & (pd.isna(row['travel_with'])):\n",
    "\n",
    "        dummy_df = pd.get_dummies(row[features], columns=features_cat)\n",
    "        dummy_df.columns = dummy_df.columns.astype(str)\n",
    "        dummy_df = dummy_df.reindex(columns=X_train2.columns, fill_value=0)\n",
    "\n",
    "        predicted_value = rf2.predict(dummy_df)\n",
    "\n",
    "        print(predicted_value[0])\n",
    "        df_train.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "    elif (not pd.isna(row['travel_with'])) & (row['total_male'] == 0 | pd.isna(row['total_male'])) & (row['total_female'] == 0 | pd.isna(row['total_female'])):\n",
    "        print(\"Brakuje liczby osob\")\n",
    "        persons = round(row['total_cost'] / mean, 0)\n",
    "\n",
    "        if row['travel_with'] == 'Alone':\n",
    "            print('predykcja płci')\n",
    "\n",
    "            input_df = row[features3].to_frame().T\n",
    "\n",
    "            predicted_value = model_pipeline.predict(input_df)\n",
    "\n",
    "            if predicted_value[0] == 'Male':\n",
    "                df_train.loc[index, 'total_male'] = 1\n",
    "                df_train.loc[index, 'total_female'] = 0\n",
    "            else:\n",
    "                df_train.loc[index, 'total_male'] = 0\n",
    "                df_train.loc[index, 'total_female'] = 1\n",
    "        elif row['travel_with'] == 'Spouse':\n",
    "            df_train.loc[index, 'total_male'] = 1\n",
    "            df_train.loc[index, 'total_female'] = 1\n",
    "        elif row['travel_with'] == 'Spouse and Children':\n",
    "            df_train.loc[index, 'total_male'] = 1\n",
    "            df_train.loc[index, 'total_female'] = 1\n",
    "            if persons < 3:\n",
    "                persons = 3\n",
    "            print(f\"Predykcja płci dziecka, przy {persons} wszystkich osobach\")\n",
    "            persons = persons - 2\n",
    "\n",
    "            # Przygotowanie danych wejściowych dla modelu\n",
    "            dummy_df = pd.get_dummies(row[features3], columns=features_cat3)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_child.columns, fill_value=0)\n",
    "\n",
    "            # Surowe predykcje liczby dzieci dla obu płci\n",
    "            pred = multioutput_rf.predict(dummy_df)\n",
    "            pred_male, pred_female = pred[0, 0], pred[0, 1]\n",
    "\n",
    "            # Skalowanie predykcji do znanej liczby dzieci (persons)\n",
    "            pred_sum = pred_male + pred_female\n",
    "            if pred_sum == 0:\n",
    "                ratio_male = 0.5  # zabezpieczenie, gdyby suma była zerowa\n",
    "            else:\n",
    "                ratio_male = pred_male / pred_sum\n",
    "            ratio_female = 1 - ratio_male\n",
    "\n",
    "            # Obliczenie ostatecznej liczby dzieci danej płci\n",
    "            male_children_final = round(persons * ratio_male)\n",
    "            female_children_final = persons - male_children_final\n",
    "\n",
    "            # Dodanie przewidywanej liczby dzieci do dorosłych\n",
    "            df_train.loc[index, 'total_male'] += male_children_final\n",
    "            df_train.loc[index, 'total_female'] += female_children_final\n",
    "\n",
    "            print(f\"Predykcja: {male_children_final} chłopców oraz {female_children_final} dziewczyn, przy {persons} żądanych osobach\")\n",
    "\n",
    "\n",
    "        elif row['travel_with'] == 'Children':\n",
    "            print(\"Predykcja płci dziecka\")\n",
    "\n",
    "            if persons < 2:\n",
    "                persons = 2\n",
    "\n",
    "            # Przygotowanie danych wejściowych dla modelu\n",
    "            dummy_df = pd.get_dummies(row[features5], columns=features_cat5)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_child_only.columns, fill_value=0)\n",
    "\n",
    "            # Surowe predykcje liczby dzieci dla obu płci\n",
    "            pred = multioutput_rf_children_only.predict(dummy_df)\n",
    "            pred_male, pred_female = pred[0, 0], pred[0, 1]\n",
    "\n",
    "            # Skalowanie predykcji do znanej liczby osób (persons)\n",
    "            pred_sum = pred_male + pred_female\n",
    "            if pred_sum == 0:\n",
    "                ratio_male = 0.5  # zabezpieczenie, gdyby suma była zerowa\n",
    "            else:\n",
    "                ratio_male = pred_male / pred_sum\n",
    "            ratio_female = 1 - ratio_male\n",
    "\n",
    "            # Obliczenie ostatecznej liczby dzieci danej płci\n",
    "            male_final = round(persons * ratio_male)\n",
    "            female_final = persons - male_final\n",
    "\n",
    "            # Przypisanie przewidywanej liczby dzieci\n",
    "            df_train.loc[index, 'total_male'] = male_final\n",
    "            df_train.loc[index, 'total_female'] = female_final\n",
    "\n",
    "            print(f\"Predykcja: {male_final} chłopców oraz {female_final} dziewczyn, przy {persons} żądanych osobach\")\n",
    "\n",
    "        elif row['travel_with'] == 'Friends/Relatives':\n",
    "            print(\"Predykcja podziału\")\n",
    "\n",
    "            if persons < 2:\n",
    "                persons = 2\n",
    "\n",
    "            # Przygotowanie danych wejściowych dla modelu\n",
    "            dummy_df = pd.get_dummies(row[features6], columns=features_cat6)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_friends.columns, fill_value=0)\n",
    "\n",
    "            # Surowe predykcje liczby osób dla obu płci\n",
    "            pred = multioutput_rf_friends.predict(dummy_df)\n",
    "            pred_male, pred_female = pred[0, 0], pred[0, 1]\n",
    "\n",
    "            # Skalowanie predykcji do znanej liczby osób (persons)\n",
    "            pred_sum = pred_male + pred_female\n",
    "            if pred_sum == 0:\n",
    "                ratio_male = 0.5  # zabezpieczenie, gdyby suma była zerowa\n",
    "            else:\n",
    "                ratio_male = pred_male / pred_sum\n",
    "            ratio_female = 1 - ratio_male\n",
    "\n",
    "            # Obliczenie ostatecznej liczby osób danej płci\n",
    "            male_final = round(persons * ratio_male)\n",
    "            female_final = persons - male_final\n",
    "\n",
    "            # Przypisanie przewidywanej liczby osób\n",
    "            df_train.loc[index, 'total_male'] = male_final\n",
    "            df_train.loc[index, 'total_female'] = female_final\n",
    "\n",
    "            print(f\"Predykcja: {male_final} mężczyzn oraz {female_final} kobiet, przy {persons} żądanych osobach\")\n",
    "        else:\n",
    "            print(f\"{row['travel_with']}\"\n",
    "              f\"\\n{row['total_male']}\"\n",
    "              f\"\\n{row['total_female']}\")\n",
    "\n",
    "    elif pd.isna(row['travel_with']) & ((row['total_male'] + row['total_female']) == 0):\n",
    "        persons = round(row['total_cost'] / mean, 0)\n",
    "\n",
    "        if persons <= 1:\n",
    "            print('predykcja płci')\n",
    "            df_train.loc[index, 'travel_with'] = 'Alone'\n",
    "\n",
    "            input_df = row[features3].to_frame().T\n",
    "\n",
    "            predicted_value = model_pipeline.predict(input_df)\n",
    "\n",
    "            if predicted_value[0] == 'Male':\n",
    "                df_train.loc[index, 'total_male'] = 1\n",
    "                df_train.loc[index, 'total_female'] = 0\n",
    "            else:\n",
    "                df_train.loc[index, 'total_male'] = 0\n",
    "                df_train.loc[index, 'total_female'] = 1\n",
    "\n",
    "        else:\n",
    "            print('predykcja płci i rozłożenia oraz kategorii travel_with')\n",
    "\n",
    "            # predykcja podziału osób\n",
    "            print(f\"Predykcja: {persons} osob\")\n",
    "\n",
    "            # Przygotowanie danych wejściowych dla modelu\n",
    "            dummy_df = pd.get_dummies(row[features8], columns=features_cat8)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_phantom.columns, fill_value=0)\n",
    "\n",
    "            # Surowe predykcje liczby osób dla obu płci\n",
    "            pred = multioutput_rf_phantom.predict(dummy_df)\n",
    "            pred_male, pred_female = pred[0, 0], pred[0, 1]\n",
    "\n",
    "            # Skalowanie predykcji do znanej liczby osób (persons)\n",
    "            pred_sum = pred_male + pred_female\n",
    "            if pred_sum == 0:\n",
    "                ratio_male = 0.5  # zabezpieczenie, gdyby suma była zerowa\n",
    "            else:\n",
    "                ratio_male = pred_male / pred_sum\n",
    "            ratio_female = 1 - ratio_male\n",
    "\n",
    "            # Obliczenie ostatecznej liczby osób danej płci\n",
    "            male_final = round(persons * ratio_male)\n",
    "            female_final = persons - male_final\n",
    "\n",
    "            # Przypisanie przewidywanej liczby osób\n",
    "            df_train.loc[index, 'total_male'] = male_final\n",
    "            df_train.loc[index, 'total_female'] = female_final\n",
    "\n",
    "            row['total_male'] = male_final\n",
    "            row['total_female'] = female_final\n",
    "\n",
    "            print(f\"Predykcja: {male_final} mężczyzn oraz {female_final} kobiet, przy {persons} żądanych osobach\")\n",
    "\n",
    "            # predykcja kategorii\n",
    "            dummy_df = pd.get_dummies(row[features8], columns=features_cat8)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train8.columns, fill_value=0)\n",
    "\n",
    "            predicted_value = rf8.predict(dummy_df)\n",
    "\n",
    "            df_train.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "\n",
    "    elif pd.isna(row['travel_with']):\n",
    "        print(f\"{row['travel_with']}\"\n",
    "              f\"\\n{row['total_male']}\"\n",
    "              f\"\\n{row['total_female']}\")\n",
    "\n",
    "\n",
    "mask_valid = (df_train['total_male'].notna()) & \\\n",
    "             (df_train['total_female'].notna()) & \\\n",
    "             ((df_train['total_male'] + df_train['total_female']) != 0)\n",
    "df_train.loc[mask_valid, 'total_cost_per_person'] = df_train.loc[mask_valid, 'total_cost'] / (\n",
    "            df_train.loc[mask_valid, 'total_male'] + df_train.loc[mask_valid, 'total_female'])\n",
    "\n",
    "count3 = df_train.isna().any(axis=1).sum()\n",
    "\n",
    "print(\"Początkowa liczba wierszy z brakującymi wartościami:\", count)\n",
    "print(\"Liczba wierszy z brakującymi wartościami po uzupełnieniu braków w kolumnie most_impressing:\", count2)\n",
    "print(\"Końcowa liczba wierszy z brakującymi wartościami:\", count3)\n",
    "\n",
    "for column in df_train.columns:\n",
    "    empty_count = df_train[column].isna().sum()\n",
    "\n",
    "    if empty_count > 0:\n",
    "        print(f\"Column '{column}' has {empty_count} empty fields (NaN).\")\n",
    "\n",
    "for index, row in df_train[df_train.isna().any(axis=1)].iterrows():\n",
    "    #print(row)\n",
    "    pass\n",
    "\n",
    "unique_countries_nan = df_train[df_train['region'].isna()]['country'].unique()\n",
    "print(\"Państwa bez regionu: \", unique_countries_nan)\n",
    "\n",
    "\n",
    "df_train.drop([\"most_impressing\", \"gender\"], axis=1, inplace=True)\n",
    "# df_train.drop([\"gender\"], axis=1, inplace=True)\n",
    "df_train[\"total_people\"] = df_train[\"total_male\"] + df_train[\"total_female\"]\n",
    "df_train['night_total'] = df_train['night_zanzibar'] + df_train['night_mainland']\n"
   ],
   "id": "2c3c86783ea1b685",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ustawienia walidacji krzyżowej",
   "id": "723de26a2ea5ea91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n"
   ],
   "id": "7e604bce95144678",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tablice do przechowywania meta-features",
   "id": "8a68e53a5f2ef4e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "oof_preds_xgb = np.zeros(len(df_train))\n",
    "oof_preds_cat = np.zeros(len(df_train))\n",
    "oof_preds_lgb = np.zeros(len(df_train))\n",
    "meta_features = np.zeros((len(df_train), 3))\n"
   ],
   "id": "8546bc77dc3de5f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Przygotowanie danych",
   "id": "30a44c9d665c5036"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(df_train.columns)\n",
    "# Konwersja kolumn `object` na `category`\n",
    "for col in df_train.select_dtypes(include=['object']).columns:\n",
    "    df_train[col] = df_train[col].astype('category')\n",
    "    new_col = \"cat_\" + col\n",
    "    df_train.rename(columns={col: new_col}, inplace=True)\n",
    "\n",
    "print(df_train.columns)\n",
    "# Przygotowanie danych wejściowych\n",
    "X = df_train.drop(columns=[\"total_cost\", \"cat_ID\", 'total_cost_per_person', 'male_children', 'female_children', \"cat_country\"])  # Dane wejściowe\n",
    "y = df_train[\"total_cost\"]\n",
    "print(X.columns)\n",
    "cat_features = [col for col in X.columns if col.startswith('cat_')]\n",
    "\n",
    "# Podział na zbiory treningowy i walidacyjny\n",
    "if is_test_iteration:\n",
    "    X, X_valid_end, y, y_valid_end = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    y = y.reset_index(drop=True)\n",
    "\n",
    "print(f\"Liczba wierszy w zbiorze danych treningowych: {len(X)}\")\n",
    "print(f\"Liczba wierszy w zbiorze wartości: {len(y)}\")\n"
   ],
   "id": "1da02e7ad811ae1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dostrajanie XGBoost",
   "id": "d663ceaeba67a8c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    # Zakresy hiperparametrów\n",
    "    params = {\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        # 'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        # 'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        # 'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "\n",
    "        'random_state': seed,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.17, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 2),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.5),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.5),\n",
    "        'verbose': 0,\n",
    "        'enable_categorical': True\n",
    "    }\n",
    "\n",
    "    # Inicjalizacja modelu z parametrami\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Obliczenie metryki\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=kf,\n",
    "        scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "\n",
    "    mae = -scores.mean()\n",
    "\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Tworzenie i optymalizacja dla XGBoost\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=150, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE:\", study.best_value)\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n"
   ],
   "id": "8b79938095dda5bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dostrajanie CatBoost",
   "id": "8aeafa9d12083824"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Strojenie modelu CatBoost\n",
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        # 'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        # 'depth': trial.suggest_int('depth', 3, 10),\n",
    "        # 'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        # 'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "\n",
    "        'leaf_estimation_iterations': 1,\n",
    "        'iterations': trial.suggest_int('iterations', 150, 600),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.15, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 8),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 6.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 128),\n",
    "        'random_state': seed,\n",
    "        'verbose': False,\n",
    "        'cat_features': cat_features,\n",
    "        'boosting_type': 'Ordered',\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "\n",
    "    # KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, cv=kf, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    mae = -scores.mean()\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Tworzenie i optymalizacja dla CatBoost\n",
    "study_catboost = optuna.create_study(direction='minimize')\n",
    "study_catboost.optimize(objective_catboost, n_trials=5, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE dla CatBoost:\", study_catboost.best_value)\n",
    "print(\"Najlepsze parametry dla CatBoost:\", study_catboost.best_params)\n"
   ],
   "id": "f0a85a0150daf42b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dostrajanie LightGBM",
   "id": "23300a476d9c269f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Strojenie modelu LightGBM\n",
    "def objective_lightgbm(trial):\n",
    "    params = {\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        # 'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        # 'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        # 'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        # 'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 8, 40),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "        'random_state': seed\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "\n",
    "    # KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, cv=kf, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    mae = -scores.mean()\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Tworzenie i optymalizacja dla LightGBM\n",
    "study_lightgbm = optuna.create_study(direction='minimize')\n",
    "study_lightgbm.optimize(objective_lightgbm, n_trials=250, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE dla LightGBM:\", study_lightgbm.best_value)\n",
    "print(\"Najlepsze parametry dla LightGBM:\", study_lightgbm.best_params)\n"
   ],
   "id": "f25a7d72eaa15d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dostrajanie Ada Boost",
   "id": "3735ec91fa5ef077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective_adaboost(trial):\n",
    "    # Parametry AdaBoost\n",
    "    params = {\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0, log=True),\n",
    "\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.6, log=True),\n",
    "        'loss': trial.suggest_categorical('loss', ['linear', 'square', 'exponential']),\n",
    "        'random_state': seed\n",
    "    }\n",
    "\n",
    "    num_features = [col for col in X.columns if col not in cat_features]\n",
    "\n",
    "    # Preprocessor (OneHot na kategoriach, passtrough na liczbowych)\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', num_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Pipeline: preprocess + model\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', AdaBoostRegressor(**params))\n",
    "    ])\n",
    "\n",
    "    # KFold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, cv=kf, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    mae = -scores.mean()\n",
    "    return mae\n",
    "\n",
    "# Tworzenie i optymalizacja dla AdaBoostRegressor z pipeline\n",
    "study_adaboost = optuna.create_study(direction='minimize')\n",
    "study_adaboost.optimize(objective_adaboost, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE dla AdaBoost:\", study_adaboost.best_value)\n",
    "print(\"Najlepsze parametry dla AdaBoost:\", study_adaboost.best_params)\n",
    "\n"
   ],
   "id": "e0aaaa15eede1541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dostrajanie HGBM",
   "id": "3a33c21f9a31ff42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cat_idx = [X.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "def objective_histgbm(trial):\n",
    "    params = {\n",
    "        # \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        # \"max_iter\": trial.suggest_int(\"max_iter\", 100, 1000),\n",
    "        # \"max_depth\": trial.suggest_int(\"max_depth\", 3, 16),\n",
    "        # \"l2_regularization\": trial.suggest_float(\"l2_regularization\", 0.0, 5.0),\n",
    "        # \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 15, 150),\n",
    "        # \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 5, 30),\n",
    "\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 100, 600),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"l2_regularization\": trial.suggest_float(\"l2_regularization\", 0.0, 2.0),\n",
    "        \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 16, 60),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 10, 70),\n",
    "        \"random_state\": seed,\n",
    "        \"categorical_features\": cat_idx if len(cat_idx) > 0 else None\n",
    "    }\n",
    "    model = HistGradientBoostingRegressor(**params)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, cv=kf, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    mae = -scores.mean()\n",
    "    return mae\n",
    "\n",
    "# Optymalizacja\n",
    "study_histgbm = optuna.create_study(direction='minimize')\n",
    "study_histgbm.optimize(objective_histgbm, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE dla HistGBM:\", study_histgbm.best_value)\n",
    "print(\"Najlepsze parametry dla HistGBM:\", study_histgbm.best_params)\n"
   ],
   "id": "d2f8ef685a18269c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definicje modeli bazowych i ich parametrów",
   "id": "346daef877b03b2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "xgb_params_manual = {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': seed,\n",
    "              'enable_categorical': True, 'verbose': 1, 'use_label_encoder': False, 'eval_metric': 'mae'}\n",
    "cat_params_manual = {'n_estimators': 500, 'learning_rate': 0.05, 'random_state': seed, 'verbose': 1}\n",
    "lgb_params_manual = {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 6, 'random_state': seed, 'verbose': 1}\n",
    "\n",
    "xgb_params = {**study.best_params, 'enable_categorical': True, 'verbose': 1}\n",
    "cat_params = {**study_catboost.best_params, 'verbose': 1, 'leaf_estimation_iterations': 1, 'boosting_type': 'Ordered' }\n",
    "lgb_params = {**study_lightgbm.best_params, 'verbose': 1}\n",
    "ada_params = {**study_adaboost.best_params}\n",
    "hgbm_params = {**study_histgbm.best_params}\n"
   ],
   "id": "ee6f56a0360b89bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inicjalizacja modeli",
   "id": "75f8fcbecf3b1877"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_features = [col for col in X.columns if col not in cat_features]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', num_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "ada_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', AdaBoostRegressor(**ada_params))\n",
    "])\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "cat_model = CatBoostRegressor(**cat_params)\n",
    "lgb_model = LGBMRegressor(**lgb_params)\n",
    "# ada_model = AdaBoostRegressor(**ada_params)\n",
    "hgbm_model = HistGradientBoostingRegressor(**hgbm_params, categorical_features=cat_idx)\n",
    "\n",
    "models = [('xgb', xgb_model), ('cat', cat_model), ('lgb', lgb_model), ('ada', ada_pipeline), ('hgbm', hgbm_model)]\n",
    "\n",
    "models_dict = {\n",
    "    \"LGBM\": lgb_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"CatBoost\": cat_model,\n",
    "    # \"AdaBoost\": ada_pipeline\n",
    "    # \"HistGBM\": hgbm_model\n",
    "}\n",
    "\n",
    "\n",
    "oof_preds = {\n",
    "    'xgb': np.zeros(len(X)),\n",
    "    'cat': np.zeros(len(X)),\n",
    "    'lgb': np.zeros(len(X)),\n",
    "    'ada': np.zeros(len(X)),\n",
    "    'hgbm': np.zeros(len(X)),\n",
    "}\n"
   ],
   "id": "ecb152c018c54a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mae_scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Trening CatBoost\n",
    "    cat_model.fit(X_train, y_train, cat_features=cat_features)\n",
    "    cat_preds = cat_model.predict(X_valid)\n",
    "    oof_preds['cat'][val_idx] = cat_preds\n",
    "    mae_scores.append(f\"CatBoost fold MAE: {mean_absolute_error(y_valid, cat_preds):.4f}\")\n",
    "\n",
    "    # Trening XGBoost\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_preds = xgb_model.predict(X_valid)\n",
    "    oof_preds['xgb'][val_idx] = xgb_preds\n",
    "    mae_scores.append(f\"XGBoost fold MAE: {mean_absolute_error(y_valid, xgb_preds):.4f}\")\n",
    "\n",
    "    # Trening LightGBM\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_preds = lgb_model.predict(X_valid)\n",
    "    oof_preds['lgb'][val_idx] = lgb_preds\n",
    "\n",
    "    mae_scores.append(f\"LightGBM fold MAE: {mean_absolute_error(y_valid, lgb_preds):.4f}\")\n",
    "\n",
    "    # Trening AdaBoostRegressor\n",
    "    ada_pipeline.fit(X_train, y_train)\n",
    "    ada_preds = ada_pipeline.predict(X_valid)\n",
    "    oof_preds['ada'][val_idx] = ada_preds\n",
    "\n",
    "    mae_scores.append(f\"AdaBoost fold MAE: {mean_absolute_error(y_valid, ada_preds):.4f}\")\n",
    "\n",
    "    hgbm_model.fit(X_train, y_train)\n",
    "    histgbm_preds = hgbm_model.predict(X_valid)\n",
    "    oof_preds['hgbm'][val_idx] = histgbm_preds\n",
    "\n",
    "    mae_scores.append(f\"HistGBM fold MAE: {mean_absolute_error(y_valid, histgbm_preds):.4f}\")\n",
    "\n",
    "\n",
    "meta_features = np.column_stack((\n",
    "    oof_preds['xgb'],  # kolumna z predykcji XGBoost\n",
    "    oof_preds['cat'],  # kolumna z predykcji CatBoost\n",
    "    oof_preds['lgb'],  # kolumna z predykcji LightGBM\n",
    "    oof_preds['ada'],  # kolumna z predykcji AdaBoost\n",
    "    oof_preds['hgbm']\n",
    "))\n",
    "\n",
    "cat_model = CatBoostRegressor(**cat_params)\n",
    "cat_model.fit(X, y, cat_features=cat_features)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "lgb_model = LGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(X, y)\n",
    "\n",
    "ada_pipeline.fit(X, y)\n",
    "\n",
    "hgbm_model = HistGradientBoostingRegressor(**hgbm_params, categorical_features=cat_idx)\n",
    "hgbm_model.fit(X, y)\n",
    "\n",
    "for score in mae_scores:\n",
    "    print(f\"Scores for learning meta model: {score}\")\n",
    "\n",
    "if is_test_iteration:\n",
    "    cat_preds = cat_model.predict(X_valid_end)\n",
    "    print(f\"CatBoost fold MAE full model: {mean_absolute_error(y_valid_end, cat_preds):.4f}\")\n",
    "\n",
    "    xgb_preds = xgb_model.predict(X_valid_end)\n",
    "    print(f\"XGBoost fold MAE full model: {mean_absolute_error(y_valid_end, xgb_preds):.4f}\")\n",
    "\n",
    "    lgb_preds = lgb_model.predict(X_valid_end)\n",
    "    print(f\"LightGBM fold MAE full model: {mean_absolute_error(y_valid_end, lgb_preds):.4f}\")\n",
    "\n",
    "    ada_preds = ada_pipeline.predict(X_valid_end)\n",
    "    print(f\"AdaBoost fold MAE full model: {mean_absolute_error(y_valid_end, ada_preds):.4f}\")\n",
    "\n",
    "    histgbm_preds = hgbm_model.predict(X_valid_end)\n",
    "    print(f\"HistGBM fold MAE full model: {mean_absolute_error(y_valid_end, histgbm_preds):.4f}\")\n",
    "\n"
   ],
   "id": "5ddf2db60597d7bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature importances",
   "id": "7597a77bca5eac44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for name, model in models_dict.items():\n",
    "    print(f\"\\nFeature importances for {name}:\")\n",
    "    importances = model.feature_importances_\n",
    "    fi = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    print(f\"\\n=== {name} – Top 10 cech ===\")\n",
    "    print(fi.head(10))\n",
    "\n",
    "    # Wykres top 20\n",
    "    plt.figure(figsize=(6,6))\n",
    "    fi.head(20).plot(kind=\"barh\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"{name} – feature_importances\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "2ee565be1216b974",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analiza SHAP",
   "id": "77b1a98a3dfb4c6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for name, model in models_dict.items():\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals = explainer.shap_values(X)\n",
    "\n",
    "    # 1) Globalne znaczenie (bar plot)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    shap.summary_plot(\n",
    "        shap_vals, X,\n",
    "        plot_type=\"bar\",\n",
    "        show=False,\n",
    "        color_bar_label=\"Mean(|SHAP value|)\"\n",
    "    )\n",
    "    ax.set_title(f\"Globalne znaczenie cech – {name}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2) Beeswarm (punktowy summary plot)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    shap.summary_plot(\n",
    "        shap_vals, X,\n",
    "        show=False\n",
    "    )\n",
    "    ax.set_title(f\"SHAP Beeswarm – {name}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3) Dependence plot dla najważniejszej cechy\n",
    "    mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
    "    top_feat = X.columns[mean_abs_shap.argmax()]\n",
    "\n",
    "    # rysujemy dependence plot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    shap.dependence_plot(\n",
    "        top_feat,\n",
    "        shap_vals,\n",
    "        X,\n",
    "        show=False,\n",
    "        dot_size=50\n",
    "    )\n",
    "    plt.title(f\"Dependence plot dla '{top_feat}' – {name}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ],
   "id": "fc68cd80a6c2e7a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "def build_meta_model(input_dim, n_neurons1, n_neurons2, n_neurons3, n_neurons4, n_neurons5,\n",
    " dropout_rate, learning_rate, weight_decay):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(n_neurons1, input_dim=input_dim, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(n_neurons2, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(n_neurons3, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(n_neurons4, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(n_neurons5, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # AdamW zamiast Adam\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    # Zakresy hiperparametrów\n",
    "    n_neurons1 = trial.suggest_int(\"n_neurons1\", 64, 128)\n",
    "    n_neurons2 = trial.suggest_int(\"n_neurons2\", 32, 96)\n",
    "    n_neurons3 = trial.suggest_int(\"n_neurons3\", 16, 64)\n",
    "    n_neurons4 = trial.suggest_int(\"n_neurons4\", 8, 32)\n",
    "    n_neurons5 = trial.suggest_int(\"n_neurons5\", 4, 16)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.3)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    epochs = 100\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 96, 128])\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
    "\n",
    "    # Walidacji krzyżowa\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(meta_features):\n",
    "        X_train, X_val = meta_features[train_idx], meta_features[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = build_meta_model(meta_features.shape[1], n_neurons1, n_neurons2, n_neurons3, n_neurons4, n_neurons5, dropout_rate, learning_rate, weight_decay)\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        # W wywołaniu fit:\n",
    "        model.fit(X_train, y_train,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=0,\n",
    "                  callbacks=[early_stop, reduce_lr])\n",
    "\n",
    "\n",
    "        mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "        mae_scores.append(mae)\n",
    "    print(f\"MAE: {np.mean(mae_scores)}\")\n",
    "    # Zwracamy średnią wartość błędu\n",
    "    return np.mean(mae_scores)\n",
    "\n",
    "\n",
    "# Uruchomienie optymalizacji\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Najlepsze hiperparametry: \", study.best_params)\n",
    "\n",
    "# Trenowanie finalnego modelu z najlepszymi hiperparametrami\n",
    "best_params = study.best_params\n",
    "meta_model = build_meta_model(meta_features.shape[1],\n",
    "                               best_params[\"n_neurons1\"],\n",
    "                               best_params[\"n_neurons2\"],\n",
    "                               best_params[\"n_neurons3\"],\n",
    "                               best_params[\"n_neurons4\"],\n",
    "                               best_params[\"n_neurons5\"],\n",
    "                               best_params[\"dropout_rate\"],\n",
    "                               best_params[\"learning_rate\"],\n",
    "                               best_params[\"weight_decay\"])\n",
    "early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "meta_model.fit(meta_features, y, epochs=100, batch_size=best_params[\"batch_size\"], verbose=1, callbacks=[early_stop])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_meta_model(input_dim, architecture, dropout_rate, learning_rate, weight_decay, activation='relu'):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Warstwa normalizacji na wejściu\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=(input_dim,)))\n",
    "\n",
    "    # Warstwy\n",
    "    for units in architecture:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=activation))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Wyjściowa warstwa regresji\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # AdamW\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mae',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def objective_nn(trial):\n",
    "    # Architektura\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 2)  # zakres warstw\n",
    "\n",
    "    # Architektura zwężająca się - więcej neuronów na początku, mniej na końcu\n",
    "    architecture = []\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            units = trial.suggest_int(f\"units_layer{i}\", 250, 500)\n",
    "        else:\n",
    "            # Każda kolejna warstwa ma 30-70% neuronów poprzedniej\n",
    "            prev_units = architecture[-1]\n",
    "            min_units = max(16, int(prev_units * 0.35))\n",
    "            max_units = max(32, int(prev_units * 0.6))\n",
    "            units = trial.suggest_int(f\"units_layer{i}\", min_units, max_units)\n",
    "        architecture.append(units)\n",
    "\n",
    "    # Hiperparametry\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 5e-4, 5e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 96, 128])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"elu\"])\n",
    "\n",
    "    # Walidacja krzyżowa\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(meta_features):\n",
    "        X_train, X_val = meta_features[train_idx], meta_features[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Standardyzacja cech\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        model = build_meta_model(\n",
    "            meta_features.shape[1],\n",
    "            architecture,\n",
    "            dropout_rate,\n",
    "            learning_rate,\n",
    "            weight_decay,\n",
    "            activation\n",
    "        )\n",
    "\n",
    "        # callbacki i monitorowanie\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_data=(X_val_scaled, y_val),\n",
    "            epochs=200,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[early_stop, reduce_lr]\n",
    "        )\n",
    "\n",
    "        # Ocena modelu na zbiorze walidacyjnym\n",
    "        mae = model.evaluate(X_val_scaled, y_val, verbose=0)[0]\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    print(f\"Architektura: {architecture}, MAE: {mean_mae:.4f}\")\n",
    "    return mean_mae\n",
    "\n",
    "# Uruchomienie optymalizacji\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_nn, n_trials=5)\n",
    "\n",
    "print(f\"Najlepsza wartość MAE: {study.best_value:.4f}\")\n",
    "print(\"Najlepsze hiperparametry:\", study.best_params)\n",
    "\n",
    "# Trenowanie finalnego modelu z najlepszymi hiperparametrami\n",
    "best_params = study.best_params\n",
    "\n",
    "# Architektura z najlepszych parametrów\n",
    "best_architecture = []\n",
    "for i in range(best_params[\"n_layers\"]):\n",
    "    best_architecture.append(best_params[f\"units_layer{i}\"])\n",
    "\n",
    "# Standardyzacja całego zbioru dla finalnego modelu\n",
    "scaler = StandardScaler()\n",
    "meta_features_scaled = scaler.fit_transform(meta_features)\n",
    "\n",
    "# Budowa i trenowanie najlepszego modelu\n",
    "meta_model = build_meta_model(\n",
    "    meta_features.shape[1],\n",
    "    best_architecture,\n",
    "    best_params[\"dropout_rate\"],\n",
    "    best_params[\"learning_rate\"],\n",
    "    best_params[\"weight_decay\"],\n",
    "    best_params[\"activation\"]\n",
    ")\n",
    "\n",
    "# Callbacki dla finalnego treningu\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Trenowanie finalnego modelu\n",
    "history = meta_model.fit(\n",
    "    meta_features_scaled, y,\n",
    "    epochs=200,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Wizualizacja procesu uczenia\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss podczas treningu')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoka')\n",
    "\n",
    "if 'val_loss' in history.history:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Walidacyjny loss')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoka')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "10b02a06efca814",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predykcje\n",
    "def space_formatter(x, pos):\n",
    "    return f\"{int(x):,}\".replace(',', ' ')\n",
    "\n",
    "predictions_combined = []\n",
    "\n",
    "if is_test_iteration:\n",
    "    # Predykcje bazowych modeli na danych nowych\n",
    "    preds_xgb = xgb_model.predict(X_valid_end)\n",
    "    preds_cat = cat_model.predict(X_valid_end)\n",
    "    preds_lgb = lgb_model.predict(X_valid_end)\n",
    "    preds_ada = ada_pipeline.predict(X_valid_end)\n",
    "    preds_hgbm = hgbm_model.predict(X_valid_end)\n",
    "\n",
    "    # Łączenie predykcji w macierz dla meta-modelu\n",
    "    meta_features_new = np.column_stack((preds_xgb, preds_cat, preds_lgb, preds_ada, preds_hgbm))\n",
    "\n",
    "    # Zastosowanie skalera przed przekazaniem do meta-modelu\n",
    "    meta_features_new_scaled = scaler.transform(meta_features_new)\n",
    "\n",
    "    # Predykcja finalna skalowanego modelu meta\n",
    "    predictions = meta_model.predict(meta_features_new_scaled)\n",
    "\n",
    "    mae = mean_absolute_error(y_valid_end, predictions)\n",
    "    print(f\"Średni błąd bezwzględny: {mae:.2f}\")\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.hist(predictions, bins=20, alpha=0.7, color='blue', label='Predykcje')\n",
    "    plt.xlabel(\"Wartość błędu\")\n",
    "    plt.ylabel(\"Liczebność\")\n",
    "    plt.title(\"Histogram predykcji\")\n",
    "    plt.ticklabel_format(style='plain', axis='x')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(space_formatter))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Wykres porównawczy predykcji vs wartości rzeczywiste\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(y_valid_end, predictions, alpha=0.5)\n",
    "    plt.plot([y_valid_end.min(), y_valid_end.max()], [y_valid_end.min(), y_valid_end.max()], 'r--')\n",
    "    plt.xlabel('Wartości rzeczywiste')\n",
    "    plt.ylabel('Predykcje')\n",
    "    plt.title('Porównanie predykcji z wartościami rzeczywistymi')\n",
    "    plt.ticklabel_format(style='plain')\n",
    "\n",
    "    # Formatowanie osi\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(space_formatter))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(space_formatter))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "7dc22f06bfbb58c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if is_test_iteration:\n",
    "    x = np.arange(len(y_valid_end))\n",
    "    y_valid_array = y_valid_end.to_numpy().flatten()\n",
    "    predictions_array = np.array(predictions).flatten()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Wykres punktowy dla wartości rzeczywistych (niebieski)\n",
    "    plt.scatter(x, y_valid_end, color='blue', label='Wartość rzeczywista')\n",
    "\n",
    "    # Wykres punktowy dla wartości estymowanych (czerwony)\n",
    "    plt.scatter(x, predictions, color='red', label='Wartość estymowana')\n",
    "\n",
    "    # Dla każdej pary linia łącząca punkty\n",
    "    for i in range(len(x)):\n",
    "        plt.plot([x[i], x[i]], [y_valid_array[i], predictions_array[i]], color='gray', linewidth=0.5)\n",
    "\n",
    "    plt.xlabel('Indeks')\n",
    "    plt.ylabel('Wartość')\n",
    "    plt.title(f'Porównanie wartości rzeczywistych i estymowanych')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(space_formatter))\n",
    "\n",
    "    plt.show()\n"
   ],
   "id": "9998da652c2e985e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not is_test_iteration:\n",
    "    valid_preds = pd.read_csv('data/Test.csv')\n",
    "\n",
    "    corrections = {\n",
    "        'SWIZERLAND': 'SWITZERLAND',\n",
    "        'UNITED STATES OF AMERICA': 'UNITED STATES',\n",
    "        'COMORO': 'COMOROS',\n",
    "        'MALT': 'MALTA',\n",
    "        'UAE': 'UNITED ARAB EMIRATES',\n",
    "        'UKRAIN': 'UKRAINE',\n",
    "        'DRC': 'CONGO (DEMOCRATIC REPUBLIC OF THE)',\n",
    "        'SWAZILAND': 'ESWATINI',\n",
    "        'COSTARICA': 'COSTA RICA',\n",
    "        'SCOTLAND': 'UNITED KINGDOM',\n",
    "        'PHILIPINES': 'PHILIPPINES',\n",
    "        'BOSNIA': 'BOSNIA AND HERZEGOVINA',\n",
    "        'CAPE VERDE': 'CABO VERDE',\n",
    "        'MORROCO': 'MOROCCO',\n",
    "        'SOMALI': 'SOMALIA',\n",
    "        'KOREA': 'SOUTH KOREA',\n",
    "        'SAUD ARABIA': 'SAUDI ARABIA',\n",
    "    }\n",
    "\n",
    "    valid_preds['country'] = valid_preds['country'].replace(corrections)\n",
    "\n",
    "    valid_preds = pd.merge(valid_preds, df_regions, how='left', left_on='country', right_on='name')\n",
    "    valid_preds = valid_preds.drop(columns=['name'])\n",
    "    print(valid_preds.columns)\n",
    "\n",
    "    # Check before cleansing\n",
    "    for column in valid_preds.columns:\n",
    "        empty_count = valid_preds[column].isna().sum()\n",
    "\n",
    "        if empty_count > 0:\n",
    "            print(f\"Column '{column}' has {empty_count} empty fields (NaN).\")\n",
    "\n",
    "    valid_preds.loc[valid_preds['most_impressing'].isna(), 'most_impressing'] = 'No comments'\n",
    "\n",
    "    # Travel with imputation\n",
    "    features_tw = ['region', 'age_group', 'total_female',\n",
    "           'total_male', 'purpose', 'main_activity', 'info_source',\n",
    "           'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "           'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "           'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "           'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "    features_cat_tw = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                    'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                    'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                    'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "\n",
    "    # Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "    valid_preds_no_nan = valid_preds.dropna()\n",
    "\n",
    "    allowed_categories = ['Children', 'Friends/Relatives', 'Spouse', 'Spouse and Children']\n",
    "    valid_preds_imp = valid_preds_no_nan[valid_preds_no_nan['travel_with'].isin(allowed_categories)].copy()\n",
    "    X_train_tw = pd.get_dummies(valid_preds_imp[features_tw], columns=features_cat_tw)\n",
    "    y_train_tw = valid_preds_imp['travel_with']\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    rf_tw = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_tw.fit(X_train_tw, y_train_tw)\n",
    "\n",
    "    print(\"Cleansing test dataset\")\n",
    "    for index, row in valid_preds[valid_preds.isna().any(axis=1)].iterrows():\n",
    "        if pd.isna(row['travel_with']) & (row['total_male'] + row['total_female'] == 1) & (not pd.isna(row['total_male'])) & (not pd.isna(row['total_female'])):\n",
    "            valid_preds.loc[index, 'travel_with'] = 'Alone'\n",
    "        if pd.isna(row['travel_with']) & (row['total_male'] + row['total_female'] > 1) & (not pd.isna(row['total_male'])) & (not pd.isna(row['total_female'])):\n",
    "            dummy_df = pd.get_dummies(row[features_tw], columns=features_cat_tw)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_tw.columns, fill_value=0)\n",
    "\n",
    "            predicted_value = rf_tw.predict(dummy_df)\n",
    "\n",
    "            print(f\"\\nrow\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\"\n",
    "                  f\"\\n{predicted_value[0]}\")\n",
    "\n",
    "            valid_preds.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "        if (row['travel_with'] == 'Alone') & ((pd.isna(row['total_male'])) | (pd.isna(row['total_female']))):\n",
    "            if pd.isna(row['total_female']):\n",
    "                valid_preds.loc[index, 'total_female'] = 0\n",
    "\n",
    "                print(f\"\\nrow\"\n",
    "                      f\"\\n{row['travel_with']}\"\n",
    "                      f\"\\n{row['total_male']}\"\n",
    "                      f\"\\n{row['total_female']}\"\n",
    "                      f\"\\n{0}\")\n",
    "            elif pd.isna(row['total_female']):\n",
    "                valid_preds.loc[index, 'total_male'] = 0\n",
    "\n",
    "                print(f\"\\nrow\"\n",
    "                      f\"\\n{row['travel_with']}\"\n",
    "                      f\"\\n{row['total_male']}\"\n",
    "                      f\"\\n{row['total_female']}\"\n",
    "                      f\"\\n{0}\")\n",
    "            elif pd.isna(row['total_male']) & pd.isna(row['total_female']):\n",
    "                # Gentleman\n",
    "                valid_preds.loc[index, 'total_male'] = 0\n",
    "                valid_preds.loc[index, 'total_female'] = 1\n",
    "\n",
    "        elif (pd.isna(row['total_female'])) & (row['total_male'] > 0):\n",
    "\n",
    "            if row['total_male'] > 1:\n",
    "                valid_preds.loc[index, 'total_female'] = 0\n",
    "            else:\n",
    "                valid_preds.loc[index, 'total_female'] = 1\n",
    "\n",
    "            print(f\"\\nrow\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\"\n",
    "                  f\"\\n{0}\")\n",
    "\n",
    "        elif (pd.isna(row['total_male'])) & (row['total_female'] > 0):\n",
    "\n",
    "            if row['total_female'] > 1:\n",
    "                valid_preds.loc[index, 'total_male'] = 0\n",
    "            else:\n",
    "                valid_preds.loc[index, 'total_male'] = 1\n",
    "\n",
    "            print(f\"\\nrow\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\"\n",
    "                  f\"\\n{0}\")\n",
    "\n",
    "        if (row['total_male'] + row['total_female'] == 0) & (not pd.isna(row['total_male'])) & (not pd.isna(row['total_female'])):\n",
    "\n",
    "            # Gentleman\n",
    "            valid_preds.loc[index, 'total_female'] = 1\n",
    "\n",
    "            if pd.isna(row['travel_with']):\n",
    "                dummy_df = pd.get_dummies(row[features_tw], columns=features_cat_tw)\n",
    "                dummy_df.columns = dummy_df.columns.astype(str)\n",
    "                dummy_df = dummy_df.reindex(columns=X_train_tw.columns, fill_value=0)\n",
    "\n",
    "                predicted_value = rf_tw.predict(dummy_df)\n",
    "\n",
    "                valid_preds.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "            print(f\"\\nrow widmo\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\")\n",
    "\n",
    "    # Check after cleansing\n",
    "    for index, row in valid_preds[valid_preds.isna().any(axis=1)].iterrows():\n",
    "        if pd.isna(row['travel_with']) | pd.isna(row['total_male']) | pd.isna(row['total_female']):\n",
    "            print(f\"\\nrow\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\")\n",
    "\n",
    "    valid_preds['total_people'] = valid_preds['total_male'] + valid_preds['total_female']\n",
    "    valid_preds['night_total'] = valid_preds['night_zanzibar'] + valid_preds['night_mainland']\n",
    "\n",
    "    for column in valid_preds.columns:\n",
    "        empty_count = valid_preds[column].isna().sum()\n",
    "\n",
    "        if empty_count > 0:\n",
    "            print(f\"Column '{column}' has {empty_count} empty fields (NaN).\")\n",
    "\n",
    "\n",
    "    unique_countries_nan = valid_preds[valid_preds['region'].isna()]['country'].unique()\n",
    "    print(\"\\nPaństwa bez regionu: \", unique_countries_nan)\n",
    "\n",
    "    for col in valid_preds.select_dtypes(include=['object']).columns:\n",
    "        if col != \"ID\":\n",
    "            valid_preds[col] = valid_preds[col].astype('category')\n",
    "            new_col = \"cat_\" + str(col)\n",
    "            valid_preds.rename(columns={col: new_col}, inplace=True)\n",
    "    print(valid_preds.columns)\n",
    "    valid_preds.drop(columns=['cat_most_impressing', 'cat_country'], inplace=True)\n",
    "    print(valid_preds.columns)\n"
   ],
   "id": "294053240eb1fde3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Challange predictions",
   "id": "3e8a21a5971fd217"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not is_test_iteration:\n",
    "    id_valid = valid_preds['ID']\n",
    "    X_valid_preds = valid_preds.drop('ID', axis=1)\n",
    "\n",
    "    # Predykcje bazowych modeli\n",
    "    preds_xgb = xgb_model.predict(X_valid_preds)\n",
    "    preds_cat = cat_model.predict(X_valid_preds)\n",
    "    preds_lgb = lgb_model.predict(X_valid_preds)\n",
    "    preds_ada = ada_pipeline.predict(X_valid_preds)\n",
    "    preds_hgbm = hgbm_model.predict(X_valid_preds)\n",
    "\n",
    "    # Łączenie predykcji dla meta modelu\n",
    "    meta_features_new = np.column_stack((preds_xgb, preds_cat, preds_lgb, preds_ada, preds_hgbm))\n",
    "\n",
    "    # Przeskalowanie\n",
    "    meta_features_new_scaled = scaler.transform(meta_features_new)\n",
    "\n",
    "    # Predykcja finalna\n",
    "    predictions = meta_model.predict(meta_features_new_scaled)\n",
    "\n",
    "    if predictions.ndim > 1 and predictions.shape[1] == 1:\n",
    "        predictions = predictions.ravel()\n",
    "\n",
    "    # Finalne dane predykcji\n",
    "    results = pd.DataFrame({\n",
    "        'ID': id_valid,\n",
    "        'total_cost': predictions\n",
    "    })\n",
    "\n",
    "\n",
    "    results.to_csv('data/submission.csv', index=False)\n"
   ],
   "id": "f6e5be0615358343",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
