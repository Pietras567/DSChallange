{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Tanzania Tourism Prediction - Prognozy dotyczące turystyki w Tanzanii\n",
    "<h2>Autorzy:</h2><br>\n",
    "<ul>\n",
    "<li>Piotr Janiszek 247678</li>\n",
    "<li>Kacper Białek 247629</li>\n",
    "<li>Franciszek Pawlak 247756</li>\n",
    "<li>Michał Korblit 242427</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7613fbacfe81bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_test_iteration = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a10b6942f3e3c",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d8b4bcc8072682",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import lightgbm as lgb\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import optuna\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebe9a3844f4307",
   "metadata": {},
   "source": [
    "<h3>Data loadind</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e60d14fd29bf9cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/Train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df_regions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/regions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df_distance \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/distance_to_TZ.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/Train.csv')\n",
    "df_regions = pd.read_csv('data/regions.csv')\n",
    "df_distance = pd.read_csv('data/distance_to_TZ.csv')\n",
    "df_regions = df_regions.merge(df_distance, on='alpha-2', how='left')\n",
    "\n",
    "df_gdp = pd.read_csv('data/gdp-per-capita-worldbank.csv')\n",
    "df_gdp = df_gdp.rename(columns={'Code': 'alpha-3', 'GDP per capita, PPP (constant 2021 international $)': 'gdp_per_capita_usd'})\n",
    "df_gdp = df_gdp.drop(columns=['time', 'Entity', 'Year'])\n",
    "df_regions = df_regions.merge(df_gdp, on='alpha-3', how='left')\n",
    "\n",
    "corrections = {\n",
    "    'SWIZERLAND': 'SWITZERLAND',\n",
    "    'MALT': 'MALTA',\n",
    "    'BURGARIA': 'BULGARIA',\n",
    "    'DRC': 'CONGO (DEMOCRATIC REPUBLIC OF THE)',\n",
    "    'KOREA': 'SOUTH KOREA',\n",
    "    'SWAZILAND': 'ESWATINI',\n",
    "    'UKRAIN': 'UKRAINE',\n",
    "    'TRINIDAD TOBACCO': 'TRINIDAD AND TOBAGO',\n",
    "    'COMORO': 'COMOROS',\n",
    "    'COSTARICA': 'COSTA RICA',\n",
    "    'PHILIPINES': 'PHILIPPINES',\n",
    "    'IVORY COAST': \"CÔTE D'IVOIRE\",\n",
    "    'DJIBOUT': 'DJIBOUTI',\n",
    "    'MORROCO': 'MOROCCO',\n",
    "    'UNITED STATES OF AMERICA': 'UNITED STATES',\n",
    "    'UAE': 'UNITED ARAB EMIRATES',\n",
    "    'SCOTLAND': 'UNITED KINGDOM',\n",
    "    'CAPE VERDE': 'CABO VERDE',\n",
    "}\n",
    "\n",
    "df_train['country'] = df_train['country'].replace(corrections)\n",
    "df_regions = df_regions[['name', 'sub-region', 'distance_km', 'gdp_per_capita_usd']]\n",
    "df_regions = df_regions.rename(columns={'sub-region': 'region'})\n",
    "df_regions['name'] = df_regions['name'].str.upper()\n",
    "df_train = pd.merge(df_train, df_regions, how='left', left_on='country', right_on='name')\n",
    "df_train = df_train.drop(columns=['name'])\n",
    "\n",
    "print(df_train.columns)\n",
    "print(df_regions.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81356a754097e7",
   "metadata": {},
   "source": [
    "<h3>Data Cleaning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f9d979344bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_train.isna().any(axis=1).sum()\n",
    "df_train.loc[df_train['most_impressing'].isna(), 'most_impressing'] = 'No comments'\n",
    "count2 = df_train.isna().any(axis=1).sum()\n",
    "\n",
    "mask_valid = (df_train['total_male'].notna()) & \\\n",
    "             (df_train['total_female'].notna()) & \\\n",
    "             ((df_train['total_male'] + df_train['total_female']) != 0)\n",
    "df_train.loc[mask_valid, 'total_cost_per_person'] = df_train.loc[mask_valid, 'total_cost'] / (\n",
    "            df_train.loc[mask_valid, 'total_male'] + df_train.loc[mask_valid, 'total_female'])\n",
    "mean = df_train.loc[mask_valid, 'total_cost_per_person'].mean()\n",
    "print(\"Średnia bez wierszy z zerową sumą:\", mean)\n",
    "\n",
    "\n",
    "# idzie przez kolumny z nan\n",
    "for index, row in df_train[df_train.isna().any(axis=1)].iterrows():\n",
    "    if (pd.isna(row['total_male']) | pd.isna(row['total_female'])) & (not (pd.isna(row['total_male']) & pd.isna(row['total_female']))):\n",
    "        if pd.isna(row['total_male']):\n",
    "            difference = row['total_cost'] - (row['total_female'] * mean)\n",
    "            person_left = difference / mean\n",
    "            person_left = round(person_left, 0)\n",
    "\n",
    "            if person_left < 0:\n",
    "                person_left = 0\n",
    "            #print(f\"Inserting {person_left}\")\n",
    "            df_train.loc[index, 'total_male'] = person_left\n",
    "\n",
    "        else:\n",
    "            difference = row['total_cost'] - (row['total_male'] * mean)\n",
    "\n",
    "            person_left = difference / mean\n",
    "            person_left = round(person_left, 0)\n",
    "\n",
    "            if person_left < 0:\n",
    "                person_left = 0\n",
    "            #print(f\"Inserting {person_left}\")\n",
    "            df_train.loc[index, 'total_female'] = person_left\n",
    "\n",
    "for index, row in df_train[df_train.isna().any(axis=1)].iterrows():\n",
    "    # Completing the travel_with field with the value Alone, when the number of people shows that he/she travels alone\n",
    "    if pd.isna(row['travel_with']) & ((row['total_male'] + row['total_female']) == 1):\n",
    "        df_train.loc[index, 'travel_with'] = 'Alone'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c86783ea1b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest imputation\n",
    "mask_valid = (df_train['total_male'].notna()) & \\\n",
    "             (df_train['total_female'].notna()) & \\\n",
    "             ((df_train['total_male'] + df_train['total_female']) != 0)\n",
    "df_train.loc[mask_valid, 'total_cost_per_person'] = df_train.loc[mask_valid, 'total_cost'] / (\n",
    "            df_train.loc[mask_valid, 'total_male'] + df_train.loc[mask_valid, 'total_female'])\n",
    "mean = df_train.loc[mask_valid, 'total_cost_per_person'].mean()\n",
    "\n",
    "features = ['region', 'age_group', 'total_female',\n",
    "       'total_male', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "# Określ dozwolone kategorie - Model 1\n",
    "allowed_categories = ['Friends/Relatives', 'Children']\n",
    "# Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "df_train_imp = df_train[df_train['travel_with'].isin(allowed_categories)].copy()\n",
    "X_train = pd.get_dummies(df_train_imp[features], columns=features_cat)\n",
    "y_train = df_train_imp['travel_with']\n",
    "# Trenowanie modelu\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Określ dozwolone kategorie - Model 2\n",
    "allowed_categories2 = ['Children', 'Friends/Relatives', 'Spouse', 'Spouse and Children']\n",
    "# Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "df_train_imp2 = df_train[df_train['travel_with'].isin(allowed_categories2)].copy()\n",
    "X_train2 = pd.get_dummies(df_train_imp2[features], columns=features_cat)\n",
    "y_train2 = df_train_imp2['travel_with']\n",
    "# Trenowanie modelu\n",
    "rf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf2.fit(X_train2, y_train2)\n",
    "\n",
    "# Określ dozwolone kategorie - Model 3\n",
    "features3 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat3 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "def designate_sex(row):\n",
    "    if (row['travel_with'] == 'Alone') and ((row['total_male'] + row['total_female']) == 1):\n",
    "        # Jeśli total_male == 1, przyjmujemy, że to mężczyzna, w przeciwnym razie kobieta\n",
    "        return 'Male' if row['total_male'] == 1 else 'Female'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_train['gender'] = df_train.apply(designate_sex, axis=1)\n",
    "df_model = df_train[df_train['gender'].notna()].copy()\n",
    "\n",
    "# Wybór cech (features) i zmienna docelowa (target)\n",
    "X_train3 = df_model[features3]\n",
    "y_train3 = df_model['gender']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), features_cat3)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "model_pipeline.fit(X_train3, y_train3)\n",
    "\n",
    "# Predykcja płci dzieci - Model 4\n",
    "features4 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat4 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "df_train['male_children'] = df_train['total_male'].apply(lambda x: max(x - 1, 0))\n",
    "df_train['female_children'] = df_train['total_female'].apply(lambda x: max(x - 1, 0))\n",
    "\n",
    "df_filtered = df_train[(df_train['total_male'] > 0) & (df_train['total_female'] > 0)].copy()\n",
    "df_children_model = df_filtered[df_filtered['travel_with'] == 'Spouse and Children'].copy()\n",
    "\n",
    "# Przygotowanie macierzy cech\n",
    "X_train_child = pd.get_dummies(df_children_model[features4], columns=features_cat4)\n",
    "\n",
    "# Przygotowanie macierzy target – dwie kolumny: liczba dzieci mężczyzn i dzieci kobiet\n",
    "y_train_child = df_children_model[['male_children', 'female_children']]\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "multioutput_rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "# Trenowanie modelu\n",
    "multioutput_rf.fit(X_train_child, y_train_child)\n",
    "\n",
    "# Predykcja podziału dzieci oraz dorosłego - Model 5\n",
    "features5 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat5 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "df_filtered = df_train[(df_train['total_male'] + df_train['total_female'] > 1)].copy()\n",
    "df_children_only_model = df_filtered[df_filtered['travel_with'] == 'Children'].copy()\n",
    "\n",
    "X_train_child_only = pd.get_dummies(df_children_only_model[features5], columns=features_cat5)\n",
    "y_train_child_only = df_children_only_model[['total_male', 'total_female']]\n",
    "\n",
    "multioutput_rf_children_only = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "multioutput_rf_children_only.fit(X_train_child_only, y_train_child_only)\n",
    "\n",
    "# Predykcja podziału przyjaciół - Model 6\n",
    "features6 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat6 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "df_filtered = df_train[(df_train['total_male'] + df_train['total_female'] > 1)].copy()\n",
    "df_friend_model = df_filtered[df_filtered['travel_with'] == 'Friends/Relatives'].copy()\n",
    "\n",
    "X_train_friends = pd.get_dummies(df_friend_model[features6], columns=features_cat6)\n",
    "y_train_friends = df_friend_model[['total_male', 'total_female']]\n",
    "\n",
    "multioutput_rf_friends = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "multioutput_rf_friends.fit(X_train_friends, y_train_friends)\n",
    "\n",
    "# predykcja kategori  - Model 7\n",
    "features7 = ['region', 'age_group', 'total_female',\n",
    "       'total_male', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat7 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "\n",
    "allowed_categories7 = ['Friends/Relatives', 'Children', 'Spouse and Children']\n",
    "# Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "df_train_imp7 = df_train[df_train['travel_with'].isin(allowed_categories7)].copy()\n",
    "X_train7 = pd.get_dummies(df_train_imp7[features7], columns=features_cat7)\n",
    "y_train7 = df_train_imp7['travel_with']\n",
    "# Trenowanie modelu\n",
    "rf7 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf7.fit(X_train7, y_train7)\n",
    "\n",
    "# Predykcja podziału dla wycieczki widmo - Model 8\n",
    "features8 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source',\n",
    "       'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "       'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "       'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "       'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing',\n",
    "       'total_cost']\n",
    "\n",
    "features_cat8 = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "df_filtered_phantom = df_train[(df_train['total_male'] + df_train['total_female'] > 1)].copy()\n",
    "df_phantom_model = df_filtered_phantom[(df_filtered_phantom['travel_with'] == 'Friends/Relatives') | (df_filtered_phantom['travel_with'] == 'Spouse and Children') | (df_filtered_phantom['travel_with'] == 'Children')].copy()\n",
    "\n",
    "X_train_phantom = pd.get_dummies(df_phantom_model[features8], columns=features_cat8)\n",
    "y_train_phantom = df_phantom_model[['total_male', 'total_female']]\n",
    "\n",
    "multioutput_rf_phantom = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "multioutput_rf_phantom.fit(X_train_phantom, y_train_phantom)\n",
    "\n",
    "# Predykcja travel with dla rekordu widmo\n",
    "allowed_categories8 = ['Children', 'Friends/Relatives', 'Spouse and Children']\n",
    "# Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "df_train_imp8 = df_train[df_train['travel_with'].isin(allowed_categories8)].copy()\n",
    "X_train8 = pd.get_dummies(df_train_imp8[features8], columns=features_cat8)\n",
    "y_train8 = df_train_imp8['travel_with']\n",
    "# Trenowanie modelu\n",
    "rf8 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf8.fit(X_train8, y_train8)\n",
    "\n",
    "# Imputation\n",
    "for index, row in df_train[df_train.isna().any(axis=1)].iterrows():\n",
    "    # Completing the travel_with field with the estimated value when only one gender participated in the trip and the number of people exceeds one\n",
    "    if (pd.isna(row['travel_with']) & ((row['total_male'] + row['total_female']) > 1) &\n",
    "            ((row['total_male'] == 0 | pd.isna(row['total_male'])) | (row['total_female'] == 0 | pd.isna(row['total_female'])))):\n",
    "\n",
    "        dummy_df = pd.get_dummies(row[features], columns=features_cat)\n",
    "        dummy_df.columns = dummy_df.columns.astype(str)\n",
    "        dummy_df = dummy_df.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "        predicted_value = rf.predict(dummy_df)\n",
    "\n",
    "        df_train.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "    elif (row['total_male'] + row['total_female'] > 1) & (pd.isna(row['travel_with'])):\n",
    "\n",
    "        dummy_df = pd.get_dummies(row[features], columns=features_cat)\n",
    "        dummy_df.columns = dummy_df.columns.astype(str)\n",
    "        dummy_df = dummy_df.reindex(columns=X_train2.columns, fill_value=0)\n",
    "\n",
    "        predicted_value = rf2.predict(dummy_df)\n",
    "\n",
    "        print(predicted_value[0])\n",
    "        df_train.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "    elif (not pd.isna(row['travel_with'])) & (row['total_male'] == 0 | pd.isna(row['total_male'])) & (row['total_female'] == 0 | pd.isna(row['total_female'])):\n",
    "        print(\"Brakuje liczby osob\")\n",
    "        persons = round(row['total_cost'] / mean, 0)\n",
    "\n",
    "        if row['travel_with'] == 'Alone':\n",
    "            print('predykcja płci')\n",
    "\n",
    "            input_df = row[features3].to_frame().T\n",
    "\n",
    "            predicted_value = model_pipeline.predict(input_df)\n",
    "\n",
    "            if predicted_value[0] == 'Male':\n",
    "                df_train.loc[index, 'total_male'] = 1\n",
    "                df_train.loc[index, 'total_female'] = 0\n",
    "            else:\n",
    "                df_train.loc[index, 'total_male'] = 0\n",
    "                df_train.loc[index, 'total_female'] = 1\n",
    "        elif row['travel_with'] == 'Spouse':\n",
    "            df_train.loc[index, 'total_male'] = 1\n",
    "            df_train.loc[index, 'total_female'] = 1\n",
    "        elif row['travel_with'] == 'Spouse and Children':\n",
    "            df_train.loc[index, 'total_male'] = 1\n",
    "            df_train.loc[index, 'total_female'] = 1\n",
    "            if persons < 3:\n",
    "                persons = 3\n",
    "            print(f\"Predykcja płci dziecka, przy {persons} wszystkich osobach\")\n",
    "            persons = persons - 2\n",
    "\n",
    "            # Przygotowanie danych wejściowych dla modelu\n",
    "            dummy_df = pd.get_dummies(row[features3], columns=features_cat3)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_child.columns, fill_value=0)\n",
    "\n",
    "            # Surowe predykcje liczby dzieci dla obu płci\n",
    "            pred = multioutput_rf.predict(dummy_df)\n",
    "            pred_male, pred_female = pred[0, 0], pred[0, 1]\n",
    "\n",
    "            # Skalowanie predykcji do znanej liczby dzieci (persons)\n",
    "            pred_sum = pred_male + pred_female\n",
    "            if pred_sum == 0:\n",
    "                ratio_male = 0.5  # zabezpieczenie, gdyby suma była zerowa\n",
    "            else:\n",
    "                ratio_male = pred_male / pred_sum\n",
    "            ratio_female = 1 - ratio_male\n",
    "\n",
    "            # Obliczenie ostatecznej liczby dzieci danej płci\n",
    "            male_children_final = round(persons * ratio_male)\n",
    "            female_children_final = persons - male_children_final\n",
    "\n",
    "            # Dodanie przewidywanej liczby dzieci do dorosłych\n",
    "            df_train.loc[index, 'total_male'] += male_children_final\n",
    "            df_train.loc[index, 'total_female'] += female_children_final\n",
    "\n",
    "            print(f\"Predykcja: {male_children_final} chłopców oraz {female_children_final} dziewczyn, przy {persons} żądanych osobach\")\n",
    "\n",
    "\n",
    "        elif row['travel_with'] == 'Children':\n",
    "            print(\"Predykcja płci dziecka\")\n",
    "\n",
    "            if persons < 2:\n",
    "                persons = 2\n",
    "\n",
    "            # Przygotowanie danych wejściowych dla modelu\n",
    "            dummy_df = pd.get_dummies(row[features5], columns=features_cat5)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_child_only.columns, fill_value=0)\n",
    "\n",
    "            # Surowe predykcje liczby dzieci dla obu płci\n",
    "            pred = multioutput_rf_children_only.predict(dummy_df)\n",
    "            pred_male, pred_female = pred[0, 0], pred[0, 1]\n",
    "\n",
    "            # Skalowanie predykcji do znanej liczby osób (persons)\n",
    "            pred_sum = pred_male + pred_female\n",
    "            if pred_sum == 0:\n",
    "                ratio_male = 0.5  # zabezpieczenie, gdyby suma była zerowa\n",
    "            else:\n",
    "                ratio_male = pred_male / pred_sum\n",
    "            ratio_female = 1 - ratio_male\n",
    "\n",
    "            # Obliczenie ostatecznej liczby dzieci danej płci\n",
    "            male_final = round(persons * ratio_male)\n",
    "            female_final = persons - male_final\n",
    "\n",
    "            # Przypisanie przewidywanej liczby dzieci\n",
    "            df_train.loc[index, 'total_male'] = male_final\n",
    "            df_train.loc[index, 'total_female'] = female_final\n",
    "\n",
    "            print(f\"Predykcja: {male_final} chłopców oraz {female_final} dziewczyn, przy {persons} żądanych osobach\")\n",
    "\n",
    "        elif row['travel_with'] == 'Friends/Relatives':\n",
    "            print(\"Predykcja podziału\")\n",
    "\n",
    "            if persons < 2:\n",
    "                persons = 2\n",
    "\n",
    "            # Przygotowanie danych wejściowych dla modelu\n",
    "            dummy_df = pd.get_dummies(row[features6], columns=features_cat6)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_friends.columns, fill_value=0)\n",
    "\n",
    "            # Surowe predykcje liczby osób dla obu płci\n",
    "            pred = multioutput_rf_friends.predict(dummy_df)\n",
    "            pred_male, pred_female = pred[0, 0], pred[0, 1]\n",
    "\n",
    "            # Skalowanie predykcji do znanej liczby osób (persons)\n",
    "            pred_sum = pred_male + pred_female\n",
    "            if pred_sum == 0:\n",
    "                ratio_male = 0.5  # zabezpieczenie, gdyby suma była zerowa\n",
    "            else:\n",
    "                ratio_male = pred_male / pred_sum\n",
    "            ratio_female = 1 - ratio_male\n",
    "\n",
    "            # Obliczenie ostatecznej liczby osób danej płci\n",
    "            male_final = round(persons * ratio_male)\n",
    "            female_final = persons - male_final\n",
    "\n",
    "            # Przypisanie przewidywanej liczby osób\n",
    "            df_train.loc[index, 'total_male'] = male_final\n",
    "            df_train.loc[index, 'total_female'] = female_final\n",
    "\n",
    "            print(f\"Predykcja: {male_final} mężczyzn oraz {female_final} kobiet, przy {persons} żądanych osobach\")\n",
    "        else:\n",
    "            print(f\"{row['travel_with']}\"\n",
    "              f\"\\n{row['total_male']}\"\n",
    "              f\"\\n{row['total_female']}\")\n",
    "\n",
    "    elif pd.isna(row['travel_with']) & ((row['total_male'] + row['total_female']) == 0):\n",
    "        persons = round(row['total_cost'] / mean, 0)\n",
    "\n",
    "        if persons <= 1:\n",
    "            print('predykcja płci')\n",
    "            df_train.loc[index, 'travel_with'] = 'Alone'\n",
    "\n",
    "            input_df = row[features3].to_frame().T\n",
    "\n",
    "            predicted_value = model_pipeline.predict(input_df)\n",
    "\n",
    "            if predicted_value[0] == 'Male':\n",
    "                df_train.loc[index, 'total_male'] = 1\n",
    "                df_train.loc[index, 'total_female'] = 0\n",
    "            else:\n",
    "                df_train.loc[index, 'total_male'] = 0\n",
    "                df_train.loc[index, 'total_female'] = 1\n",
    "\n",
    "        else:\n",
    "            print('predykcja płci i rozłożenia oraz kategorii travel_with')\n",
    "\n",
    "            # predykcja podziału osób\n",
    "            print(f\"Predykcja: {persons} osob\")\n",
    "\n",
    "            # Przygotowanie danych wejściowych dla modelu\n",
    "            dummy_df = pd.get_dummies(row[features8], columns=features_cat8)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_phantom.columns, fill_value=0)\n",
    "\n",
    "            # Surowe predykcje liczby osób dla obu płci\n",
    "            pred = multioutput_rf_phantom.predict(dummy_df)\n",
    "            pred_male, pred_female = pred[0, 0], pred[0, 1]\n",
    "\n",
    "            # Skalowanie predykcji do znanej liczby osób (persons)\n",
    "            pred_sum = pred_male + pred_female\n",
    "            if pred_sum == 0:\n",
    "                ratio_male = 0.5  # zabezpieczenie, gdyby suma była zerowa\n",
    "            else:\n",
    "                ratio_male = pred_male / pred_sum\n",
    "            ratio_female = 1 - ratio_male\n",
    "\n",
    "            # Obliczenie ostatecznej liczby osób danej płci\n",
    "            male_final = round(persons * ratio_male)\n",
    "            female_final = persons - male_final\n",
    "\n",
    "            # Przypisanie przewidywanej liczby osób\n",
    "            df_train.loc[index, 'total_male'] = male_final\n",
    "            df_train.loc[index, 'total_female'] = female_final\n",
    "\n",
    "            row['total_male'] = male_final\n",
    "            row['total_female'] = female_final\n",
    "\n",
    "            print(f\"Predykcja: {male_final} mężczyzn oraz {female_final} kobiet, przy {persons} żądanych osobach\")\n",
    "\n",
    "            # predykcja kategorii\n",
    "            dummy_df = pd.get_dummies(row[features8], columns=features_cat8)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train8.columns, fill_value=0)\n",
    "\n",
    "            predicted_value = rf8.predict(dummy_df)\n",
    "\n",
    "            df_train.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "\n",
    "    elif pd.isna(row['travel_with']):\n",
    "        print(f\"{row['travel_with']}\"\n",
    "              f\"\\n{row['total_male']}\"\n",
    "              f\"\\n{row['total_female']}\")\n",
    "\n",
    "\n",
    "mask_valid = (df_train['total_male'].notna()) & \\\n",
    "             (df_train['total_female'].notna()) & \\\n",
    "             ((df_train['total_male'] + df_train['total_female']) != 0)\n",
    "df_train.loc[mask_valid, 'total_cost_per_person'] = df_train.loc[mask_valid, 'total_cost'] / (\n",
    "            df_train.loc[mask_valid, 'total_male'] + df_train.loc[mask_valid, 'total_female'])\n",
    "\n",
    "count3 = df_train.isna().any(axis=1).sum()\n",
    "\n",
    "print(\"Początkowa liczba wierszy z brakującymi wartościami:\", count)\n",
    "print(\"Liczba wierszy z brakującymi wartościami po uzupełnieniu braków w kolumnie most_impressing:\", count2)\n",
    "print(\"Końcowa liczba wierszy z brakującymi wartościami:\", count3)\n",
    "\n",
    "for column in df_train.columns:\n",
    "    empty_count = df_train[column].isna().sum()\n",
    "\n",
    "    if empty_count > 0:\n",
    "        print(f\"Column '{column}' has {empty_count} empty fields (NaN).\")\n",
    "\n",
    "for index, row in df_train[df_train.isna().any(axis=1)].iterrows():\n",
    "    #print(row)\n",
    "    pass\n",
    "\n",
    "unique_countries_nan = df_train[df_train['region'].isna()]['country'].unique()\n",
    "print(\"Państwa bez regionu: \", unique_countries_nan)\n",
    "\n",
    "\n",
    "df_train.drop([\"most_impressing\", \"gender\"], axis=1, inplace=True)\n",
    "# df_train.drop([\"gender\"], axis=1, inplace=True)\n",
    "df_train[\"total_people\"] = df_train[\"total_male\"] + df_train[\"total_female\"]\n",
    "df_train['night_total'] = df_train['night_zanzibar'] + df_train['night_mainland']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723de26a2ea5ea91",
   "metadata": {},
   "source": [
    "Ustawienia walidacji krzyżowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e604bce95144678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68e53a5f2ef4e6",
   "metadata": {},
   "source": [
    "Tablice do przechowywania meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546bc77dc3de5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds_xgb = np.zeros(len(df_train))\n",
    "oof_preds_cat = np.zeros(len(df_train))\n",
    "oof_preds_lgb = np.zeros(len(df_train))\n",
    "meta_features = np.zeros((len(df_train), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a44c9d665c5036",
   "metadata": {},
   "source": [
    "Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da02e7ad811ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns)\n",
    "# Konwersja kolumn `object` na `category`\n",
    "for col in df_train.select_dtypes(include=['object']).columns:\n",
    "    df_train[col] = df_train[col].astype('category')\n",
    "    new_col = \"cat_\" + col\n",
    "    df_train.rename(columns={col: new_col}, inplace=True)\n",
    "\n",
    "print(df_train.columns)\n",
    "# Przygotowanie danych wejściowych\n",
    "X = df_train.drop(columns=[\"total_cost\", \"cat_ID\", 'total_cost_per_person', 'male_children', 'female_children', \"cat_country\"])  # Dane wejściowe\n",
    "y = df_train[\"total_cost\"]\n",
    "print(X.columns)\n",
    "cat_features = [col for col in X.columns if col.startswith('cat_')]\n",
    "\n",
    "# Podział na zbiory treningowy i walidacyjny\n",
    "if is_test_iteration:\n",
    "    X, X_valid_end, y, y_valid_end = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    y = y.reset_index(drop=True)\n",
    "\n",
    "print(f\"Liczba wierszy w zbiorze danych treningowych: {len(X)}\")\n",
    "print(f\"Liczba wierszy w zbiorze wartości: {len(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d663ceaeba67a8c7",
   "metadata": {},
   "source": [
    "Dostrajanie XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79938095dda5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Zakresy hiperparametrów\n",
    "    params = {\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        # 'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        # 'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        # 'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "\n",
    "        'random_state': seed,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.17, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 2),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.5),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.5),\n",
    "        'verbose': 0,\n",
    "        'enable_categorical': True\n",
    "    }\n",
    "\n",
    "    # Inicjalizacja modelu z parametrami\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Obliczenie metryki\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=kf,\n",
    "        scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "\n",
    "    mae = -scores.mean()\n",
    "\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Tworzenie i optymalizacja dla XGBoost\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=150, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE:\", study.best_value)\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeafa9d12083824",
   "metadata": {},
   "source": [
    "Dostrajanie CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a85a0150daf42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strojenie modelu CatBoost\n",
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        # 'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        # 'depth': trial.suggest_int('depth', 3, 10),\n",
    "        # 'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        # 'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "\n",
    "        'leaf_estimation_iterations': 1,\n",
    "        'iterations': trial.suggest_int('iterations', 150, 600),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.15, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 8),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 6.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 128),\n",
    "        'random_state': seed,\n",
    "        'verbose': False,\n",
    "        'cat_features': cat_features,\n",
    "        'boosting_type': 'Ordered',\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "\n",
    "    # KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, cv=kf, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    mae = -scores.mean()\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Tworzenie i optymalizacja dla CatBoost\n",
    "study_catboost = optuna.create_study(direction='minimize')\n",
    "study_catboost.optimize(objective_catboost, n_trials=5, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE dla CatBoost:\", study_catboost.best_value)\n",
    "print(\"Najlepsze parametry dla CatBoost:\", study_catboost.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23300a476d9c269f",
   "metadata": {},
   "source": [
    "Dostrajanie LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a7d72eaa15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strojenie modelu LightGBM\n",
    "def objective_lightgbm(trial):\n",
    "    params = {\n",
    "        # 'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        # 'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        # 'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        # 'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
    "        # 'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        # 'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        # 'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 8, 40),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "        'random_state': seed\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "\n",
    "    # KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, cv=kf, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    mae = -scores.mean()\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Tworzenie i optymalizacja dla LightGBM\n",
    "study_lightgbm = optuna.create_study(direction='minimize')\n",
    "study_lightgbm.optimize(objective_lightgbm, n_trials=250, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE dla LightGBM:\", study_lightgbm.best_value)\n",
    "print(\"Najlepsze parametry dla LightGBM:\", study_lightgbm.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735ec91fa5ef077",
   "metadata": {},
   "source": [
    "Dostrajanie Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aaaa15eede1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "def objective_adaboost(trial):\n",
    "    # Parametry AdaBoost\n",
    "    # params = {\n",
    "    #     # 'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "    #     # 'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0, log=True),\n",
    "    #\n",
    "    #     'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "    #     'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.6, log=True),\n",
    "    #     'loss': trial.suggest_categorical('loss', ['linear', 'square', 'exponential']),\n",
    "    #     'random_state': seed\n",
    "    # }\n",
    "\n",
    "    # Strojenie parametrów bazowego drzewa\n",
    "    base_params = {\n",
    "        'max_depth': trial.suggest_int('base_max_depth', 2, 8),\n",
    "        'min_samples_split': trial.suggest_int('base_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('base_min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical(\n",
    "            'base_max_features', [None, 'sqrt', 'log2']\n",
    "        ),\n",
    "        'min_weight_fraction_leaf': trial.suggest_float(\n",
    "            'min_weight_fraction_leaf', 0.0, 0.5\n",
    "        ),\n",
    "        'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1),\n",
    "        'random_state': seed\n",
    "    }\n",
    "    base_est = DecisionTreeRegressor(**base_params)\n",
    "\n",
    "    # Strojenie AdaBoost\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0, log=True),\n",
    "        'loss': trial.suggest_categorical(\n",
    "            'loss', ['linear', 'square', 'exponential']\n",
    "        ),\n",
    "        'estimator': base_est,\n",
    "        'random_state': seed\n",
    "    }\n",
    "\n",
    "\n",
    "    num_features = [col for col in X.columns if col not in cat_features]\n",
    "\n",
    "    # Preprocessor (OneHot na kategoriach, passtrough na liczbowych)\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', num_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Pipeline: preprocess + model\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', AdaBoostRegressor(**params))\n",
    "    ])\n",
    "\n",
    "    # KFold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, cv=kf, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    mae = -scores.mean()\n",
    "    return mae\n",
    "\n",
    "# Tworzenie i optymalizacja dla AdaBoostRegressor z pipeline\n",
    "study_adaboost = optuna.create_study(direction='minimize')\n",
    "study_adaboost.optimize(objective_adaboost, n_trials=150, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE dla AdaBoost:\", study_adaboost.best_value)\n",
    "print(\"Najlepsze parametry dla AdaBoost:\", study_adaboost.best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33c21f9a31ff42",
   "metadata": {},
   "source": [
    "Dostrajanie HGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8ef685a18269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idx = [X.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "def objective_histgbm(trial):\n",
    "    params = {\n",
    "        # \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        # \"max_iter\": trial.suggest_int(\"max_iter\", 100, 1000),\n",
    "        # \"max_depth\": trial.suggest_int(\"max_depth\", 3, 16),\n",
    "        # \"l2_regularization\": trial.suggest_float(\"l2_regularization\", 0.0, 5.0),\n",
    "        # \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 15, 150),\n",
    "        # \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 5, 30),\n",
    "\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 100, 600),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"l2_regularization\": trial.suggest_float(\"l2_regularization\", 0.0, 2.0),\n",
    "        \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 16, 60),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 10, 70),\n",
    "        \"random_state\": seed,\n",
    "        \"categorical_features\": cat_idx if len(cat_idx) > 0 else None\n",
    "    }\n",
    "    model = HistGradientBoostingRegressor(**params)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, cv=kf, scoring='neg_mean_absolute_error'\n",
    "    )\n",
    "    mae = -scores.mean()\n",
    "    return mae\n",
    "\n",
    "# Optymalizacja\n",
    "study_histgbm = optuna.create_study(direction='minimize')\n",
    "study_histgbm.optimize(objective_histgbm, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"Najlepsza wartość MAE dla HistGBM:\", study_histgbm.best_value)\n",
    "print(\"Najlepsze parametry dla HistGBM:\", study_histgbm.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346daef877b03b2b",
   "metadata": {},
   "source": [
    "Definicje modeli bazowych i ich parametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f56a0360b89bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_params_manual = {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': seed,\n",
    "              'enable_categorical': True, 'verbose': 1, 'use_label_encoder': False, 'eval_metric': 'mae'}\n",
    "cat_params_manual = {'n_estimators': 500, 'learning_rate': 0.05, 'random_state': seed, 'verbose': 1}\n",
    "lgb_params_manual = {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 6, 'random_state': seed, 'verbose': 1}\n",
    "\n",
    "xgb_params = {**study.best_params, 'enable_categorical': True, 'verbose': 1}\n",
    "cat_params = {**study_catboost.best_params, 'verbose': 1, 'leaf_estimation_iterations': 1, 'boosting_type': 'Ordered' }\n",
    "lgb_params = {**study_lightgbm.best_params, 'verbose': 1}\n",
    "ada_params = {**study_adaboost.best_params}\n",
    "hgbm_params = {**study_histgbm.best_params}\n",
    "\n",
    "# Parametry drzewa DecisionTreeRegressor\n",
    "tree_param_names = [\n",
    "    'max_depth', 'min_samples_split', 'min_samples_leaf',\n",
    "    'min_weight_fraction_leaf', 'max_features', 'ccp_alpha',\n",
    "    'criterion', 'splitter', 'random_state', 'max_leaf_nodes',\n",
    "    'min_impurity_decrease'\n",
    "]\n",
    "\n",
    "# Parametry AdaBoostRegressor\n",
    "adaboost_param_names = [\n",
    "    'estimator', 'n_estimators', 'learning_rate', 'loss', 'random_state'\n",
    "]\n",
    "\n",
    "# Inicjalizacja pustych słowników\n",
    "base_params = {}\n",
    "ada_only_params = {}\n",
    "\n",
    "# Rozdzielanie parametrów\n",
    "for param, value in ada_params.items():\n",
    "    if param.startswith('base_'):\n",
    "        # Konwersja nazwy parametru drzewa (usuwanie prefiksu 'base_')\n",
    "        base_key = param.replace('base_', '')\n",
    "        base_params[base_key] = value\n",
    "    elif param in tree_param_names:\n",
    "        # Parametr drzewa bez prefiksu\n",
    "        base_params[param] = value\n",
    "    elif param in adaboost_param_names:\n",
    "        # Parametr AdaBoosta\n",
    "        ada_only_params[param] = value\n",
    "    else:\n",
    "        # Pozostałe (mogą być istotne dla AdaBoosta)\n",
    "        ada_only_params[param] = value\n",
    "\n",
    "# Tworzenie drzewa bazowego\n",
    "base_estimator = DecisionTreeRegressor(**base_params)\n",
    "\n",
    "# Ustawianie estymatora dla AdaBoosta\n",
    "ada_only_params['estimator'] = base_estimator\n",
    "\n",
    "# Usuwanie potencjalnie konfliktujących parametrów\n",
    "for param in ['min_weight_fraction_leaf', 'max_features', 'ccp_alpha']:\n",
    "    if param in ada_only_params:\n",
    "        del ada_only_params[param]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8fcbecf3b1877",
   "metadata": {},
   "source": [
    "Inicjalizacja modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb152c018c54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [col for col in X.columns if col not in cat_features]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', num_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "ada_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', AdaBoostRegressor(**ada_only_params))\n",
    "])\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "cat_model = CatBoostRegressor(**cat_params)\n",
    "lgb_model = LGBMRegressor(**lgb_params)\n",
    "# ada_model = AdaBoostRegressor(**ada_params)\n",
    "hgbm_model = HistGradientBoostingRegressor(**hgbm_params, categorical_features=cat_idx)\n",
    "\n",
    "models = [('xgb', xgb_model), ('cat', cat_model), ('lgb', lgb_model), ('ada', ada_pipeline), ('hgbm', hgbm_model)]\n",
    "\n",
    "models_dict = {\n",
    "    \"LGBM\": lgb_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"CatBoost\": cat_model,\n",
    "    # \"AdaBoost\": ada_pipeline\n",
    "    # \"HistGBM\": hgbm_model\n",
    "}\n",
    "\n",
    "\n",
    "oof_preds = {\n",
    "    'xgb': np.zeros(len(X)),\n",
    "    'cat': np.zeros(len(X)),\n",
    "    'lgb': np.zeros(len(X)),\n",
    "    'ada': np.zeros(len(X)),\n",
    "    'hgbm': np.zeros(len(X)),\n",
    "    # 'mean': np.zeros(len(X)),\n",
    "    # 'total_people': np.zeros(len(X)),\n",
    "    # 'night_total': np.zeros(len(X)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf2db60597d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_scores = []\n",
    "\n",
    "meta_model_weights = {\n",
    "    'xgb': 0,\n",
    "    'cat': 0,\n",
    "    'lgb': 0,\n",
    "    'ada': 0,\n",
    "    'hgbm': 0\n",
    "}\n",
    "\n",
    "\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Trening CatBoost\n",
    "    cat_model.fit(X_train, y_train, cat_features=cat_features)\n",
    "    cat_preds = cat_model.predict(X_valid)\n",
    "    oof_preds['cat'][val_idx] = cat_preds\n",
    "    mae_scores.append(f\"CatBoost fold MAE: {mean_absolute_error(y_valid, cat_preds):.4f}\")\n",
    "    meta_model_weights['cat'] += mean_absolute_error(y_valid, cat_preds)\n",
    "\n",
    "    # Trening XGBoost\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_preds = xgb_model.predict(X_valid)\n",
    "    oof_preds['xgb'][val_idx] = xgb_preds\n",
    "    mae_scores.append(f\"XGBoost fold MAE: {mean_absolute_error(y_valid, xgb_preds):.4f}\")\n",
    "    meta_model_weights['xgb'] += mean_absolute_error(y_valid, xgb_preds)\n",
    "\n",
    "    # Trening LightGBM\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_preds = lgb_model.predict(X_valid)\n",
    "    oof_preds['lgb'][val_idx] = lgb_preds\n",
    "    mae_scores.append(f\"LightGBM fold MAE: {mean_absolute_error(y_valid, lgb_preds):.4f}\")\n",
    "    meta_model_weights['lgb'] += mean_absolute_error(y_valid, lgb_preds)\n",
    "\n",
    "    # Trening AdaBoostRegressor\n",
    "    ada_pipeline.fit(X_train, y_train)\n",
    "    ada_preds = ada_pipeline.predict(X_valid)\n",
    "    oof_preds['ada'][val_idx] = ada_preds\n",
    "    mae_scores.append(f\"AdaBoost fold MAE: {mean_absolute_error(y_valid, ada_preds):.4f}\")\n",
    "    meta_model_weights['ada'] += mean_absolute_error(y_valid, ada_preds)\n",
    "\n",
    "    hgbm_model.fit(X_train, y_train)\n",
    "    histgbm_preds = hgbm_model.predict(X_valid)\n",
    "    oof_preds['hgbm'][val_idx] = histgbm_preds\n",
    "    mae_scores.append(f\"HistGBM fold MAE: {mean_absolute_error(y_valid, histgbm_preds):.4f}\")\n",
    "    meta_model_weights['hgbm'] += mean_absolute_error(y_valid, histgbm_preds)\n",
    "\n",
    "    # oof_preds['mean'][val_idx] = np.mean([cat_preds, xgb_preds, lgb_preds, ada_preds, histgbm_preds], axis=0)\n",
    "\n",
    "    # zapis kolumn total_nights i total_person\n",
    "    # oof_preds['night_total'][val_idx] = X_valid['night_total'].values\n",
    "    # oof_preds['total_people'][val_idx] = X_valid['total_people'].values\n",
    "\n",
    "meta_model_weights['xgb'] = meta_model_weights['xgb'] / n_splits\n",
    "meta_model_weights['cat'] = meta_model_weights['cat'] / n_splits\n",
    "meta_model_weights['lgb'] = meta_model_weights['lgb'] / n_splits\n",
    "meta_model_weights['hgbm'] = meta_model_weights['hgbm'] / n_splits\n",
    "meta_model_weights['ada'] = meta_model_weights['ada'] / n_splits\n",
    "\n",
    "order = ['xgb', 'cat', 'lgb', 'ada', 'hgbm']\n",
    "mae_vals = np.array([meta_model_weights[m] for m in order], dtype=np.float32)\n",
    "\n",
    "# Odwrotność błędu, lepszy model ma większą wagę\n",
    "inv = 1.0 / (mae_vals + 1e-8)   # dodajemy epsilon, by uniknąć dzielenia przez 0\n",
    "\n",
    "# Suma wag = 1\n",
    "weights = inv / inv.sum()\n",
    "\n",
    "\n",
    "print(\"Weights: \")\n",
    "print(meta_model_weights)\n",
    "\n",
    "meta_features = np.column_stack((\n",
    "    oof_preds['xgb'],  # kolumna z predykcji XGBoost\n",
    "    oof_preds['cat'],  # kolumna z predykcji CatBoost\n",
    "    oof_preds['lgb'],  # kolumna z predykcji LightGBM\n",
    "    oof_preds['ada'],  # kolumna z predykcji AdaBoost\n",
    "    oof_preds['hgbm'],\n",
    "    # oof_preds['mean'],\n",
    "    # oof_preds['total_people'],\n",
    "    # oof_preds['night_total']\n",
    "))\n",
    "\n",
    "cat_model = CatBoostRegressor(**cat_params)\n",
    "cat_model.fit(X, y, cat_features=cat_features)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "lgb_model = LGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(X, y)\n",
    "\n",
    "ada_pipeline.fit(X, y)\n",
    "\n",
    "hgbm_model = HistGradientBoostingRegressor(**hgbm_params, categorical_features=cat_idx)\n",
    "hgbm_model.fit(X, y)\n",
    "\n",
    "for score in mae_scores:\n",
    "    print(f\"Scores for learning meta model: {score}\")\n",
    "\n",
    "if is_test_iteration:\n",
    "    cat_preds = cat_model.predict(X_valid_end)\n",
    "    print(f\"CatBoost fold MAE full model: {mean_absolute_error(y_valid_end, cat_preds):.4f}\")\n",
    "\n",
    "    xgb_preds = xgb_model.predict(X_valid_end)\n",
    "    print(f\"XGBoost fold MAE full model: {mean_absolute_error(y_valid_end, xgb_preds):.4f}\")\n",
    "\n",
    "    lgb_preds = lgb_model.predict(X_valid_end)\n",
    "    print(f\"LightGBM fold MAE full model: {mean_absolute_error(y_valid_end, lgb_preds):.4f}\")\n",
    "\n",
    "    ada_preds = ada_pipeline.predict(X_valid_end)\n",
    "    print(f\"AdaBoost fold MAE full model: {mean_absolute_error(y_valid_end, ada_preds):.4f}\")\n",
    "\n",
    "    histgbm_preds = hgbm_model.predict(X_valid_end)\n",
    "    print(f\"HistGBM fold MAE full model: {mean_absolute_error(y_valid_end, histgbm_preds):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597a77bca5eac44",
   "metadata": {},
   "source": [
    "Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee565be1216b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models_dict.items():\n",
    "    print(f\"\\nFeature importances for {name}:\")\n",
    "    importances = model.feature_importances_\n",
    "    fi = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    print(f\"\\n=== {name} – Top 10 cech ===\")\n",
    "    print(fi.head(10))\n",
    "\n",
    "    # Wykres top 20\n",
    "    plt.figure(figsize=(6,6))\n",
    "    fi.head(20).plot(kind=\"barh\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"{name} – feature_importances\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1a98a3dfb4c6b",
   "metadata": {},
   "source": [
    "Analiza SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68cd80a6c2e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models_dict.items():\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals = explainer.shap_values(X)\n",
    "\n",
    "    # 1) Globalne znaczenie (bar plot)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    shap.summary_plot(\n",
    "        shap_vals, X,\n",
    "        plot_type=\"bar\",\n",
    "        show=False,\n",
    "        color_bar_label=\"Mean(|SHAP value|)\"\n",
    "    )\n",
    "    ax.set_title(f\"Globalne znaczenie cech – {name}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2) Beeswarm (punktowy summary plot)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    shap.summary_plot(\n",
    "        shap_vals, X,\n",
    "        show=False\n",
    "    )\n",
    "    ax.set_title(f\"SHAP Beeswarm – {name}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3) Dependence plot dla najważniejszej cechy\n",
    "    mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
    "    top_feat = X.columns[mean_abs_shap.argmax()]\n",
    "\n",
    "    # rysujemy dependence plot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    shap.dependence_plot(\n",
    "        top_feat,\n",
    "        shap_vals,\n",
    "        X,\n",
    "        show=False,\n",
    "        dot_size=50\n",
    "    )\n",
    "    plt.title(f\"Dependence plot dla '{top_feat}' – {name}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b02a06efca814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "def build_meta_model(input_dim, n_neurons1, n_neurons2, n_neurons3, n_neurons4, n_neurons5,\n",
    " dropout_rate, learning_rate, weight_decay):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(n_neurons1, input_dim=input_dim, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(n_neurons2, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(n_neurons3, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(n_neurons4, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(n_neurons5, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # AdamW zamiast Adam\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    # Zakresy hiperparametrów\n",
    "    n_neurons1 = trial.suggest_int(\"n_neurons1\", 64, 128)\n",
    "    n_neurons2 = trial.suggest_int(\"n_neurons2\", 32, 96)\n",
    "    n_neurons3 = trial.suggest_int(\"n_neurons3\", 16, 64)\n",
    "    n_neurons4 = trial.suggest_int(\"n_neurons4\", 8, 32)\n",
    "    n_neurons5 = trial.suggest_int(\"n_neurons5\", 4, 16)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.3)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    epochs = 100\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 96, 128])\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
    "\n",
    "    # Walidacji krzyżowa\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(meta_features):\n",
    "        X_train, X_val = meta_features[train_idx], meta_features[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = build_meta_model(meta_features.shape[1], n_neurons1, n_neurons2, n_neurons3, n_neurons4, n_neurons5, dropout_rate, learning_rate, weight_decay)\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        # W wywołaniu fit:\n",
    "        model.fit(X_train, y_train,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=0,\n",
    "                  callbacks=[early_stop, reduce_lr])\n",
    "\n",
    "\n",
    "        mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "        mae_scores.append(mae)\n",
    "    print(f\"MAE: {np.mean(mae_scores)}\")\n",
    "    # Zwracamy średnią wartość błędu\n",
    "    return np.mean(mae_scores)\n",
    "\n",
    "\n",
    "# Uruchomienie optymalizacji\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Najlepsze hiperparametry: \", study.best_params)\n",
    "\n",
    "# Trenowanie finalnego modelu z najlepszymi hiperparametrami\n",
    "best_params = study.best_params\n",
    "meta_model = build_meta_model(meta_features.shape[1],\n",
    "                               best_params[\"n_neurons1\"],\n",
    "                               best_params[\"n_neurons2\"],\n",
    "                               best_params[\"n_neurons3\"],\n",
    "                               best_params[\"n_neurons4\"],\n",
    "                               best_params[\"n_neurons5\"],\n",
    "                               best_params[\"dropout_rate\"],\n",
    "                               best_params[\"learning_rate\"],\n",
    "                               best_params[\"weight_decay\"])\n",
    "early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "meta_model.fit(meta_features, y, epochs=100, batch_size=best_params[\"batch_size\"], verbose=1, callbacks=[early_stop])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_meta_model(input_dim, architecture, dropout_rate, learning_rate, weight_decay, activation='relu', use_weight_layer=False, weights=None):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Warstwa normalizacji na wejściu\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=(input_dim,)))\n",
    "\n",
    "    # Warstwa ważenia\n",
    "    if use_weight_layer:\n",
    "        if weights is None or len(weights) != input_dim:\n",
    "            raise ValueError(\"Musisz podać 'weights' o długości input_dim\")\n",
    "        # stwórz tensor wag i wymuś broadcast po batchu\n",
    "        w_tensor = tf.constant(weights, shape=(1, input_dim), dtype=tf.float32)\n",
    "        model.add(\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda x: x * w_tensor,\n",
    "                name='apply_weights'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Warstwy\n",
    "    for units in architecture:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=activation))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Wyjściowa warstwa regresji\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # AdamW\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mae',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def objective_nn(trial):\n",
    "    # Architektura\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 2)  # zakres warstw\n",
    "\n",
    "    # Architektura zwężająca się - więcej neuronów na początku, mniej na końcu\n",
    "    architecture = []\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            units = trial.suggest_int(f\"units_layer{i}\", 250, 500)\n",
    "        else:\n",
    "            # Każda kolejna warstwa ma 30-70% neuronów poprzedniej\n",
    "            prev_units = architecture[-1]\n",
    "            min_units = max(16, int(prev_units * 0.35))\n",
    "            max_units = max(32, int(prev_units * 0.6))\n",
    "            units = trial.suggest_int(f\"units_layer{i}\", min_units, max_units)\n",
    "        architecture.append(units)\n",
    "\n",
    "    # Hiperparametry\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.01, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 96, 128])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"elu\"])\n",
    "\n",
    "    # Walidacja krzyżowa\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(meta_features):\n",
    "        X_train, X_val = meta_features[train_idx], meta_features[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Standardyzacja cech\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        model = build_meta_model(\n",
    "            meta_features.shape[1],\n",
    "            architecture,\n",
    "            dropout_rate,\n",
    "            learning_rate,\n",
    "            weight_decay,\n",
    "            activation,\n",
    "            use_weight_layer=True,\n",
    "            weights=weights.tolist()\n",
    "        )\n",
    "\n",
    "        # callbacki i monitorowanie\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_data=(X_val_scaled, y_val),\n",
    "            epochs=200,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[early_stop, reduce_lr]\n",
    "        )\n",
    "\n",
    "        # Ocena modelu na zbiorze walidacyjnym\n",
    "        mae = model.evaluate(X_val_scaled, y_val, verbose=0)[0]\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    print(f\"Architektura: {architecture}, MAE: {mean_mae:.4f}\")\n",
    "    return mean_mae\n",
    "\n",
    "# Uruchomienie optymalizacji\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_nn, n_trials=5)\n",
    "\n",
    "print(f\"Najlepsza wartość MAE: {study.best_value:.4f}\")\n",
    "print(\"Najlepsze hiperparametry:\", study.best_params)\n",
    "\n",
    "# Trenowanie finalnego modelu z najlepszymi hiperparametrami\n",
    "best_params = study.best_params\n",
    "\n",
    "# Architektura z najlepszych parametrów\n",
    "best_architecture = []\n",
    "for i in range(best_params[\"n_layers\"]):\n",
    "    best_architecture.append(best_params[f\"units_layer{i}\"])\n",
    "\n",
    "# Standardyzacja całego zbioru dla finalnego modelu\n",
    "scaler = StandardScaler()\n",
    "meta_features_scaled = scaler.fit_transform(meta_features)\n",
    "\n",
    "# Budowa i trenowanie najlepszego modelu\n",
    "meta_model = build_meta_model(\n",
    "    meta_features.shape[1],\n",
    "    best_architecture,\n",
    "    best_params[\"dropout_rate\"],\n",
    "    best_params[\"learning_rate\"],\n",
    "    best_params[\"weight_decay\"],\n",
    "    best_params[\"activation\"],\n",
    "    use_weight_layer=True,\n",
    "    weights=weights.tolist()\n",
    ")\n",
    "\n",
    "# Callbacki dla finalnego treningu\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Trenowanie finalnego modelu\n",
    "history = meta_model.fit(\n",
    "    meta_features_scaled, y,\n",
    "    epochs=200,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Wizualizacja procesu uczenia\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss podczas treningu')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoka')\n",
    "\n",
    "if 'val_loss' in history.history:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Walidacyjny loss')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoka')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265741292cd927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Optymalizacja Ridge\n",
    "\n",
    "def objective_ridge(trial):\n",
    "    from sklearn.linear_model import Ridge\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True)\n",
    "    model = Ridge(alpha=alpha, random_state=seed)\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "\n",
    "study_ridge = optuna.create_study(direction=\"minimize\")\n",
    "study_ridge.optimize(objective_ridge, n_trials=50)\n",
    "\n",
    "print(\"Ridge best MAE:\", study_ridge.best_value)\n",
    "print(\"Ridge best params:\", study_ridge.best_params)\n",
    "\n",
    "\n",
    "## 3. Optymalizacja Lasso\n",
    "\n",
    "def objective_lasso(trial):\n",
    "    from sklearn.linear_model import Lasso\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-4, 1.0, log=True)\n",
    "    model = Lasso(alpha=alpha, random_state=seed, max_iter=5000)\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "\n",
    "study_lasso = optuna.create_study(direction=\"minimize\")\n",
    "study_lasso.optimize(objective_lasso, n_trials=50)\n",
    "\n",
    "print(\"Lasso best MAE:\", study_lasso.best_value)\n",
    "print(\"Lasso best params:\", study_lasso.best_params)\n",
    "\n",
    "\n",
    "## 4. Optymalizacja SVR\n",
    "\n",
    "# def objective_svr(trial):\n",
    "#     from sklearn.svm import SVR\n",
    "#     C = trial.suggest_float(\"C\", 0.1, 100.0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 0.01, 1.0, log=True)\n",
    "#     gamma = trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"])\n",
    "#     model = SVR(kernel=\"rbf\", C=C, epsilon=epsilon, gamma=gamma)\n",
    "#     scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "#     return -scores.mean()\n",
    "#\n",
    "#\n",
    "# study_svr = optuna.create_study(direction=\"minimize\")\n",
    "# study_svr.optimize(objective_svr, n_trials=50)\n",
    "#\n",
    "# print(\"SVR best MAE:\", study_svr.best_value)\n",
    "# print(\"SVR best params:\", study_svr.best_params)\n",
    "\n",
    "\n",
    "\n",
    "# Funkcja celu dla optymalizacji\n",
    "def objective_elastic(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-5, 1.0, log=True)\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.01, 0.99)\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=seed, max_iter=5000)\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "# Optymalizacja\n",
    "study_elastic = optuna.create_study(direction=\"minimize\")\n",
    "study_elastic.optimize(objective_elastic, n_trials=50)\n",
    "print(\"ElasticNet best MAE:\", study_elastic.best_value)\n",
    "print(\"ElasticNet best params:\", study_elastic.best_params)\n",
    "\n",
    "\n",
    "\n",
    "# Funkcja celu dla optymalizacji\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"random_state\": seed\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "# Optymalizacja\n",
    "study_lgbm = optuna.create_study(direction=\"minimize\")\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=50)\n",
    "print(\"LightGBM best MAE:\", study_lgbm.best_value)\n",
    "print(\"LightGBM best params:\", study_lgbm.best_params)\n",
    "\n",
    "# Funkcja celu dla optymalizacji\n",
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 10.0, log=True),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-8, 10.0, log=True),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n",
    "        \"random_seed\": seed,\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(**params)\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "# Optymalizacja\n",
    "study_catboost = optuna.create_study(direction=\"minimize\")\n",
    "study_catboost.optimize(objective_catboost, n_trials=50)\n",
    "print(\"CatBoost best MAE:\", study_catboost.best_value)\n",
    "print(\"CatBoost best params:\", study_catboost.best_params)\n",
    "\n",
    "\n",
    "# Funkcja celu dla optymalizacji\n",
    "def objective_et(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
    "\n",
    "    model = ExtraTreesRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=seed,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "# Optymalizacja\n",
    "study_et = optuna.create_study(direction=\"minimize\")\n",
    "study_et.optimize(objective_et, n_trials=50)\n",
    "print(\"ExtraTrees best MAE:\", study_et.best_value)\n",
    "print(\"ExtraTrees best params:\", study_et.best_params)\n",
    "\n",
    "\n",
    "# Standardyzacja danych\n",
    "scaler_mlp = StandardScaler()\n",
    "meta_features_scaled = scaler_mlp.fit_transform(meta_features)\n",
    "\n",
    "# Funkcja celu dla optymalizacji\n",
    "def objective_mlp(trial):\n",
    "    # Architektura sieci\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    hidden_layers = []\n",
    "    for i in range(n_layers):\n",
    "        hidden_layers.append(trial.suggest_int(f\"n_units_l{i}\", 32, 256))\n",
    "\n",
    "    # Hiperparametry\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-6, 1e-2, log=True)  # Regularyzacja\n",
    "    learning_rate_init = trial.suggest_float(\"learning_rate_init\", 1e-4, 1e-1, log=True)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=tuple(hidden_layers),\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        activation=activation,\n",
    "        max_iter=2000,\n",
    "        early_stopping=True,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    scores = cross_val_score(model, meta_features_scaled, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "# Optymalizacja\n",
    "study_mlp = optuna.create_study(direction=\"minimize\")\n",
    "study_mlp.optimize(objective_mlp, n_trials=50)\n",
    "print(\"MLPRegressor best MAE:\", study_mlp.best_value)\n",
    "print(\"MLPRegressor best params:\", study_mlp.best_params)\n",
    "\n",
    "def objective_huber(trial):\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 1.1, 2.0)\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-6, 1.0, log=True)\n",
    "\n",
    "    model = HuberRegressor(\n",
    "        epsilon=epsilon,\n",
    "        alpha=alpha,\n",
    "        max_iter=2000\n",
    "    )\n",
    "\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "# Optymalizacja\n",
    "study_huber = optuna.create_study(direction=\"minimize\")\n",
    "study_huber.optimize(objective_huber, n_trials=50)\n",
    "print(\"HuberRegressor best MAE:\", study_huber.best_value)\n",
    "print(\"HuberRegressor best params:\", study_huber.best_params)\n",
    "\n",
    "\n",
    "## 5. Optymalizacja Random Forest\n",
    "\n",
    "def objective_rf(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [None, \"sqrt\", \"log2\"])\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=seed,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "\n",
    "study_rf = optuna.create_study(direction=\"minimize\")\n",
    "study_rf.optimize(objective_rf, n_trials=50)\n",
    "\n",
    "print(\"RF best MAE:\", study_rf.best_value)\n",
    "print(\"RF best params:\", study_rf.best_params)\n",
    "\n",
    "\n",
    "## 6. Optymalizacja GradientBoosting\n",
    "\n",
    "def objective_gbr(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [None, \"sqrt\", \"log2\"])\n",
    "    model = GradientBoostingRegressor(\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        max_features=max_features,\n",
    "        random_state=seed\n",
    "    )\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "\n",
    "study_gbr = optuna.create_study(direction=\"minimize\")\n",
    "study_gbr.optimize(objective_gbr, n_trials=50)\n",
    "\n",
    "print(\"GBR best MAE:\", study_gbr.best_value)\n",
    "print(\"GBR best params:\", study_gbr.best_params)\n",
    "\n",
    "\n",
    "## 7. Optymalizacja XGBoost\n",
    "\n",
    "def objective_xgb(trial):\n",
    "\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"random_state\": seed,\n",
    "        \"verbosity\": 0\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "\n",
    "study_xgb = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "\n",
    "print(\"XGB best MAE:\", study_xgb.best_value)\n",
    "print(\"XGB best params:\", study_xgb.best_params)\n",
    "\n",
    "\n",
    "## 8. Optymalizacja KNN\n",
    "\n",
    "def objective_knn(trial):\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 30)\n",
    "    weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "    p = trial.suggest_int(\"p\", 1, 5)\n",
    "    model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, p=p, n_jobs=-1)\n",
    "    scores = cross_val_score(model, meta_features, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores.mean()\n",
    "\n",
    "\n",
    "study_knn = optuna.create_study(direction=\"minimize\")\n",
    "study_knn.optimize(objective_knn, n_trials=50)\n",
    "\n",
    "print(\"KNN best MAE:\", study_knn.best_value)\n",
    "print(\"KNN best params:\", study_knn.best_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_models(train_X, train_y, valid_X, valid_y, models: dict):\n",
    "    \"\"\"\n",
    "    Trenuje każdy model na (train_X, train_y),\n",
    "    dokonuje predykcji na (valid_X),\n",
    "    zwraca:\n",
    "      - results[name] = MAE na zbiorze walidacyjnym\n",
    "      - predictions[name] = wektor predykcji dla valid_X\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    predictions = {}\n",
    "\n",
    "    scaler_mlp = StandardScaler()\n",
    "    train_X_scaled = scaler_mlp.fit_transform(train_X)\n",
    "    valid_X_scaled = scaler_mlp.transform(valid_X)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if name == \"MLP\":\n",
    "            model.fit(train_X_scaled, train_y)\n",
    "            pred = model.predict(valid_X_scaled)\n",
    "        else:\n",
    "            model.fit(train_X, train_y)\n",
    "            pred = model.predict(valid_X)\n",
    "\n",
    "        mae = mean_absolute_error(valid_y, pred)\n",
    "        results[name] = mae\n",
    "        predictions[name] = pred\n",
    "        print(f\"MAE {name}: {mae:.4f}\")\n",
    "    return results, predictions\n",
    "\n",
    "\n",
    "\n",
    "# Utworzenie parametrów dla MLPRegressor\n",
    "mlp_params = study_mlp.best_params.copy()\n",
    "\n",
    "n_layers = mlp_params.pop('n_layers', 0)\n",
    "\n",
    "# Tworzenie architektury sieci neuronowej\n",
    "hidden_layer_sizes = []\n",
    "for i in range(n_layers):\n",
    "    if f'n_units_l{i}' in mlp_params:\n",
    "        hidden_layer_sizes.append(mlp_params.pop(f'n_units_l{i}'))\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Ridge\": Ridge(**study_ridge.best_params, random_state=seed),\n",
    "    \"Lasso\": Lasso(**study_lasso.best_params, random_state=seed, max_iter=5000),\n",
    "    \"RandomForest\": RandomForestRegressor(**study_rf.best_params, random_state=seed, n_jobs=-1),\n",
    "    \"GBR\": GradientBoostingRegressor(**study_gbr.best_params, random_state=seed),\n",
    "    \"XGBoost\": xgb.XGBRegressor(**study_xgb.best_params, random_state=seed),\n",
    "    \"KNN\": KNeighborsRegressor(**study_knn.best_params, n_jobs=-1),\n",
    "    \"ElasticNet\": ElasticNet(**study_elastic.best_params, random_state=seed, max_iter=5000),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(**study_lgbm.best_params, random_state=seed),\n",
    "    \"CatBoost\": CatBoostRegressor(**study_catboost.best_params, random_seed=seed, verbose=False),\n",
    "    \"ExtraTrees\": ExtraTreesRegressor(**study_et.best_params, random_state=seed, n_jobs=-1),\n",
    "    \"MLP\": MLPRegressor(\n",
    "        hidden_layer_sizes=tuple(hidden_layer_sizes),\n",
    "        **mlp_params,\n",
    "        random_state=seed,\n",
    "        max_iter=2000,\n",
    "        early_stopping=True\n",
    "    ),\n",
    "    \"Huber\": HuberRegressor(**study_huber.best_params, max_iter=2000)\n",
    "}\n",
    "\n",
    "if is_test_iteration:\n",
    "    preds_xgb = xgb_model.predict(X_valid_end)\n",
    "    preds_cat = cat_model.predict(X_valid_end)\n",
    "    preds_lgb = lgb_model.predict(X_valid_end)\n",
    "    preds_ada = ada_pipeline.predict(X_valid_end)\n",
    "    preds_hgbm = hgbm_model.predict(X_valid_end)\n",
    "\n",
    "    meta_features_new = np.column_stack((preds_xgb, preds_cat, preds_lgb, preds_ada, preds_hgbm))\n",
    "\n",
    "    results, predictions = evaluate_models(\n",
    "        meta_features, y,\n",
    "        meta_features_new, y_valid_end,\n",
    "        models\n",
    "    )\n",
    "\n",
    "    names = list(results.keys())\n",
    "    maes  = list(results.values())\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(results.keys(), results.values(), color=\"skyblue\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"Porównanie MAE modeli\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for name, pred in predictions.items():\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.scatter(y_valid_end, pred, alpha=0.6)\n",
    "        minv, maxv = y_valid_end.min(), y_valid_end.max()\n",
    "        plt.plot([minv, maxv], [minv, maxv], \"r--\", lw=1)\n",
    "        plt.xlabel(\"y rzeczywiste (walidacja)\")\n",
    "        plt.ylabel(\"y_pred\")\n",
    "        plt.title(f\"{name}: walidacja y vs y_pred\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a200c303a7b2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Meta model sieci neuronowej łączący wszystkie meta modele\n",
    "# 1. Funkcja do generowania predykcji out-of-fold (OOF) dla każdego meta modelu\n",
    "def generate_oof_predictions(models, X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Generuje predykcje out-of-fold dla każdego modelu.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Słownik modeli (nazwa: model)\n",
    "        X (np.array): Cechy\n",
    "        y (np.array): Wartości docelowe\n",
    "        n_splits (int): Liczba podziałów w walidacji krzyżowej\n",
    "\n",
    "    Returns:\n",
    "        tuple: (oof_predictions, model_mae_scores)\n",
    "            - oof_predictions (np.array): Macierz predykcji OOF dla każdego modelu\n",
    "            - model_mae_scores (dict): Słownik z wynikami MAE dla każdego modelu\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Inicjalizacja macierzy out-of-fold\n",
    "    n_samples = X.shape[0]\n",
    "    n_models = len(models)\n",
    "    oof_predictions = np.zeros((n_samples, n_models))\n",
    "    model_mae_scores = {}\n",
    "\n",
    "    # Dla każdego podziału w walidacji krzyżowej\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx], y.iloc[val_idx] if hasattr(y, 'iloc') else y[val_idx]\n",
    "\n",
    "        print(f\"Fold {fold+1}/{n_splits}\")\n",
    "\n",
    "        # Standardyzacja danych dla modeli wymagających skalowania\n",
    "        scaler_oof = StandardScaler()\n",
    "        X_train_scaled = scaler_oof.fit_transform(X_train)\n",
    "        X_val_scaled = scaler_oof.transform(X_val)\n",
    "\n",
    "\n",
    "        # Dla każdego modelu\n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            print(f\"  Trenowanie {name}...\")\n",
    "\n",
    "            # Trenowanie modelu\n",
    "            if name == \"NN\":\n",
    "                # Callbacki dla finalnego treningu\n",
    "                reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "                early_stop = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "                # Trenowanie finalnego modelu\n",
    "                model.fit(\n",
    "                    X_train_scaled, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=best_params[\"batch_size\"],\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop, reduce_lr]\n",
    "                )\n",
    "\n",
    "                val_pred = model.predict(X_val_scaled, verbose=0)\n",
    "            elif name == \"MLP\":\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                val_pred = model.predict(X_val_scaled)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "                val_pred = model.predict(X_val)\n",
    "\n",
    "            # Upewnienie się, że predykcje mają odpowiedni kształt, konwersja na płaską tablicę\n",
    "            val_pred = np.asarray(val_pred).flatten()\n",
    "\n",
    "            # Zapisanie predykcji OOF\n",
    "            oof_predictions[val_idx, i] = val_pred\n",
    "\n",
    "            # Obliczenie MAE na zbiorze walidacyjnym\n",
    "            mae = mean_absolute_error(y_val, val_pred)\n",
    "\n",
    "            # Aktualizacja średniego MAE dla modelu\n",
    "            if name not in model_mae_scores:\n",
    "                model_mae_scores[name] = []\n",
    "            model_mae_scores[name].append(mae)\n",
    "\n",
    "            print(f\"    MAE dla {name} w fold {fold+1}: {mae:.4f}\")\n",
    "\n",
    "    # Obliczenie średniego MAE dla każdego modelu\n",
    "    for name in model_mae_scores:\n",
    "        model_mae_scores[name] = np.mean(model_mae_scores[name])\n",
    "\n",
    "    return oof_predictions, model_mae_scores\n",
    "\n",
    "\n",
    "# 2. Budowa finalnego meta modelu wykorzystującego predykcje z modeli bazowych\n",
    "def build_final_meta_model(input_dim, architecture, dropout_rate, learning_rate, weight_decay,\n",
    "                          initial_weights=None, use_weight_layer=True, activation='relu'):\n",
    "    \"\"\"\n",
    "    Buduje finalny meta model, który wykorzystuje predykcje z innych modeli.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Liczba wejściowych cech (liczba modeli bazowych)\n",
    "        architecture (list): Lista z liczbą neuronów w każdej warstwie ukrytej\n",
    "        dropout_rate (float): Wartość dropout do regularyzacji\n",
    "        learning_rate (float): Współczynnik uczenia\n",
    "        weight_decay (float): Regularyzacja wag\n",
    "        initial_weights (list, optional): Początkowe wagi do zastosowania dla modeli bazowych\n",
    "        use_weight_layer (bool): Czy używać warstwy z wagami\n",
    "        activation (str): Funkcja aktywacji\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Skompilowany model\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Warstwa normalizacji na wejściu\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=(input_dim,)))\n",
    "\n",
    "    # Warstwa ważenia predykcji modeli bazowych\n",
    "    if use_weight_layer and initial_weights is not None:\n",
    "        if len(initial_weights) != input_dim:\n",
    "            raise ValueError(f\"Wagi muszą mieć długość {input_dim}, otrzymano {len(initial_weights)}\")\n",
    "\n",
    "        # Stworzenie tensora wag\n",
    "        w_tensor = tf.constant(initial_weights, shape=(1, input_dim), dtype=tf.float32)\n",
    "\n",
    "        # Warstwa Lambda z wagami\n",
    "        model.add(\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda x: x * w_tensor,\n",
    "                name='model_weights'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Warstw ukryte\n",
    "    for units in architecture:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Warstwa wyjściowa\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mae',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# 3. Funkcja celu dla optymalizacji Optuna\n",
    "def objective_final_meta(trial, X, y, initial_weights):\n",
    "    \"\"\"\n",
    "    Funkcja celu dla optymalizacji Optuna.\n",
    "\n",
    "    Args:\n",
    "        trial: Trial Optuna\n",
    "        X (np.array): Macierz predykcji OOF\n",
    "        y (np.array): Wartości docelowe\n",
    "        initial_weights (list): Początkowe wagi dla modeli bazowych\n",
    "\n",
    "    Returns:\n",
    "        float: Średni MAE z walidacji krzyżowej\n",
    "    \"\"\"\n",
    "    # Parametry do optymalizacji\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 3)\n",
    "\n",
    "    # Architektura sieci\n",
    "    architecture = []\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            units = trial.suggest_int(f\"units_layer{i}\", 128, 356)\n",
    "        else:\n",
    "            # Każda kolejna warstwa ma mniej neuronów\n",
    "            prev_units = architecture[-1]\n",
    "            min_units = max(4, int(prev_units * 0.35))\n",
    "            max_units = max(8, int(prev_units * 0.6))\n",
    "            units = trial.suggest_int(f\"units_layer{i}\", min_units, max_units)\n",
    "        architecture.append(units)\n",
    "\n",
    "    # Pozostałe hiperparametry\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.01, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"elu\"])\n",
    "    use_weight_layer = trial.suggest_categorical(\"use_weight_layer\", [True, False])\n",
    "\n",
    "    # Walidacja krzyżowa\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    mae_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx], y.iloc[val_idx] if hasattr(y, 'iloc') else y[val_idx]\n",
    "\n",
    "        # Standardyzacja cech\n",
    "        scaler_fin = StandardScaler()\n",
    "        X_train_scaled = scaler_fin.fit_transform(X_train)\n",
    "        X_val_scaled = scaler_fin.transform(X_val)\n",
    "\n",
    "        # Budowa modelu\n",
    "        model = build_final_meta_model(\n",
    "            X.shape[1],\n",
    "            architecture,\n",
    "            dropout_rate,\n",
    "            learning_rate,\n",
    "            weight_decay,\n",
    "            initial_weights if use_weight_layer else None,\n",
    "            use_weight_layer,\n",
    "            activation\n",
    "        )\n",
    "\n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Trenowanie modelu\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_data=(X_val_scaled, y_val),\n",
    "            epochs=100,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[early_stop, reduce_lr]\n",
    "        )\n",
    "\n",
    "        # Ocena modelu\n",
    "        mae = model.evaluate(X_val_scaled, y_val, verbose=0)[1]\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    print(f\"Architektura: {architecture}, MAE: {mean_mae:.4f}\")\n",
    "    return mean_mae\n",
    "\n",
    "# 4. Główna część - wygenerowanie OOF dla modeli, optymalizacja i trenowanie\n",
    "mlp_params = study_mlp.best_params.copy()\n",
    "n_layers = mlp_params.pop('n_layers', 0)\n",
    "hidden_layer_sizes = []\n",
    "for i in range(n_layers):\n",
    "    if f'n_units_l{i}' in mlp_params:\n",
    "        hidden_layer_sizes.append(mlp_params.pop(f'n_units_l{i}'))\n",
    "\n",
    "models_dict = {\n",
    "    \"Ridge\": Ridge(**study_ridge.best_params, random_state=seed),\n",
    "    \"Lasso\": Lasso(**study_lasso.best_params, random_state=seed, max_iter=5000),\n",
    "    \"ElasticNet\": ElasticNet(**study_elastic.best_params, random_state=seed, max_iter=5000),\n",
    "    \"RandomForest\": RandomForestRegressor(**study_rf.best_params, random_state=seed, n_jobs=-1),\n",
    "    \"GBR\": GradientBoostingRegressor(**study_gbr.best_params, random_state=seed),\n",
    "    \"XGBoost\": xgb.XGBRegressor(**study_xgb.best_params, random_state=seed),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(**study_lgbm.best_params, random_state=seed),\n",
    "    \"CatBoost\": CatBoostRegressor(**study_catboost.best_params, random_seed=seed, verbose=False),\n",
    "    \"ExtraTrees\": ExtraTreesRegressor(**study_et.best_params, random_state=seed, n_jobs=-1),\n",
    "    \"KNN\": KNeighborsRegressor(**study_knn.best_params, n_jobs=-1),\n",
    "    \"Huber\": HuberRegressor(**study_huber.best_params, max_iter=2000),\n",
    "    \"MLP\": MLPRegressor(\n",
    "        hidden_layer_sizes=tuple(hidden_layer_sizes),\n",
    "        **mlp_params,\n",
    "        random_state=seed,\n",
    "        max_iter=2000,\n",
    "        early_stopping=True\n",
    "    ),\n",
    "    \"NN\": meta_model\n",
    "}\n",
    "\n",
    "\n",
    "# Generowanie predykcji OOF dla każdego modelu\n",
    "print(\"Generowanie predykcji out-of-fold dla modeli bazowych...\")\n",
    "oof_predictions, model_mae_scores = generate_oof_predictions(models_dict, meta_features, y)\n",
    "\n",
    "# Wizualizacja wyników modeli bazowych\n",
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(model_mae_scores.keys())\n",
    "mae_values = list(model_mae_scores.values())\n",
    "sorted_indices = np.argsort(mae_values)\n",
    "plt.bar(\n",
    "    range(len(model_names)),\n",
    "    [mae_values[i] for i in sorted_indices],\n",
    "    color='skyblue'\n",
    ")\n",
    "plt.xticks(\n",
    "    range(len(model_names)),\n",
    "    [model_names[i] for i in sorted_indices],\n",
    "    rotation=45\n",
    ")\n",
    "plt.title('Średnie MAE modeli bazowych (out-of-fold)')\n",
    "plt.ylabel('MAE')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Obliczenie wag inicjujących na podstawie MAE\n",
    "# Im niższe MAE, tym wyższa waga\n",
    "initial_weights = []\n",
    "sum_inverse_mae = sum(1.0 / mae for mae in model_mae_scores.values())\n",
    "for name in models_dict.keys():\n",
    "    weight = (1.0 / model_mae_scores[name]) / sum_inverse_mae\n",
    "    initial_weights.append(weight)\n",
    "\n",
    "print(\"\\nWagi inicjujące dla modeli bazowych:\")\n",
    "for name, weight in zip(models_dict.keys(), initial_weights):\n",
    "    print(f\"{name}: {weight:.4f}\")\n",
    "\n",
    "# Optymalizacja finalnego meta modelu\n",
    "print(\"\\nOptymalizacja finalnego meta modelu...\")\n",
    "final_study = optuna.create_study(direction=\"minimize\")\n",
    "final_study.optimize(\n",
    "    lambda trial: objective_final_meta(trial, oof_predictions, y, initial_weights),\n",
    "    n_trials=10\n",
    ")\n",
    "\n",
    "print(f\"\\nNajlepsze MAE dla finalnego meta modelu: {final_study.best_value:.4f}\")\n",
    "print(\"Najlepsze parametry:\", final_study.best_params)\n",
    "\n",
    "# Przygotowanie najlepszego modelu\n",
    "best_params = final_study.best_params\n",
    "\n",
    "# Budowa architektury\n",
    "best_architecture = []\n",
    "for i in range(best_params[\"n_layers\"]):\n",
    "    best_architecture.append(best_params[f\"units_layer{i}\"])\n",
    "\n",
    "# Standardyzacja dla finalnego modelu\n",
    "scaler_final = StandardScaler()\n",
    "oof_predictions_scaled = scaler_final.fit_transform(oof_predictions)\n",
    "\n",
    "# Trenowanie finalnego modelu\n",
    "print(\"\\nTrenowanie finalnego meta modelu z najlepszymi parametrami...\")\n",
    "final_meta_model = build_final_meta_model(\n",
    "    oof_predictions.shape[1],\n",
    "    best_architecture,\n",
    "    best_params[\"dropout_rate\"],\n",
    "    best_params[\"learning_rate\"],\n",
    "    best_params[\"weight_decay\"],\n",
    "    initial_weights if best_params[\"use_weight_layer\"] else None,\n",
    "    best_params[\"use_weight_layer\"],\n",
    "    best_params[\"activation\"]\n",
    ")\n",
    "\n",
    "# Callbacks dla finalnego treningu\n",
    "early_stop_final = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_final = ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Trenowanie finalnego modelu\n",
    "history_final = final_meta_model.fit(\n",
    "    oof_predictions_scaled, y,\n",
    "    epochs=200,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop_final, reduce_lr_final]\n",
    ")\n",
    "\n",
    "# Wizualizacja procesu uczenia\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_final.history['loss'], label='Train Loss (MAE)')\n",
    "\n",
    "if 'val_loss' in history_final.history:\n",
    "    plt.plot(history_final.history['val_loss'], label='Validation Loss (MAE)')\n",
    "\n",
    "plt.title('Proces uczenia finalnego meta modelu')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if is_test_iteration:\n",
    "    # Ocena finalnego modelu\n",
    "    preds_xgb = xgb_model.predict(X_valid_end)\n",
    "    preds_cat = cat_model.predict(X_valid_end)\n",
    "    preds_lgb = lgb_model.predict(X_valid_end)\n",
    "    preds_ada = ada_pipeline.predict(X_valid_end)\n",
    "    preds_hgbm = hgbm_model.predict(X_valid_end)\n",
    "\n",
    "    meta_features_new = np.column_stack((preds_xgb, preds_cat, preds_lgb, preds_ada, preds_hgbm))\n",
    "    meta_features_new_scaled = scaler.transform(meta_features_new)\n",
    "\n",
    "    # Predykcje modeli meta dla zbioru testowego\n",
    "    base_model_preds = []\n",
    "\n",
    "    # Generowanie predykcji z wszystkich modeli meta\n",
    "    for name, model in models_dict.items():\n",
    "        if name == \"NN\":\n",
    "            preds = model.predict(meta_features_new_scaled, verbose=0).flatten()\n",
    "        elif name == \"MLP\":\n",
    "            preds = model.predict(meta_features_new_scaled)\n",
    "        else:\n",
    "            preds = model.predict(meta_features_new)\n",
    "\n",
    "        base_model_preds.append(preds)\n",
    "        print(f\"MAE {name} na zbiorze testowym: {mean_absolute_error(y_valid_end, preds):.4f}\")\n",
    "\n",
    "    # Tworzenie macierzy cech dla finalnego meta modelu\n",
    "    final_meta_features = np.column_stack(base_model_preds)\n",
    "    final_meta_features_scaled = scaler_final.transform(final_meta_features)\n",
    "\n",
    "    # Predykcja finalnego meta modelu\n",
    "    y_pred_final = final_meta_model.predict(final_meta_features_scaled)\n",
    "    final_mae = mean_absolute_error(y_valid_end, y_pred_final)\n",
    "    print(f\"\\nFinalny MAE meta modelu: {final_mae:.4f}\")\n",
    "\n",
    "    # Porównanie predykcji finalnego modelu z wartościami rzeczywistymi\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_valid_end, y_pred_final, alpha=0.6, color='blue')\n",
    "    min_val, max_val = min(y.min(), y_pred_final.min()), max(y.max(), y_pred_final.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    plt.title('Porównanie predykcji finalnego meta modelu z wartościami rzeczywistymi')\n",
    "    plt.xlabel('Wartości rzeczywiste')\n",
    "    plt.ylabel('Predykcje')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Analiza ważności modeli bazowych\n",
    "    if best_params[\"use_weight_layer\"]:\n",
    "        model_weights = np.array(initial_weights)\n",
    "\n",
    "        # Wizualizacja wag modeli\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sorted_indices = np.argsort(model_weights)[::-1]  # Sortowanie malejąco\n",
    "        plt.bar(\n",
    "            range(len(model_names)),\n",
    "            [model_weights[i] for i in sorted_indices],\n",
    "            color='lightgreen'\n",
    "        )\n",
    "        plt.xticks(\n",
    "            range(len(model_names)),\n",
    "            [model_names[i] for i in sorted_indices],\n",
    "            rotation=45\n",
    "        )\n",
    "        plt.title('Wagi modeli bazowych w finalnym meta modelu')\n",
    "        plt.ylabel('Waga')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08ad7fa95dbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping dla zatrzymania treningu, gdy model przestaje się poprawiać.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, delta=0, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Ile epok czekać zanim zatrzymamy trening po braku poprawy\n",
    "            delta (float): Minimalna zmiana, aby uznać za poprawę\n",
    "            verbose (bool): Czy wyświetlać komunikaty\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Wywołanie klasy jako funkcji.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Wartość walidacyjnej straty\n",
    "            model (nn.Module): Model do zapisania\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            # Pierwszy epoka\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            # Brak poprawy\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # Jest poprawa\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Zapisuje model, gdy walidacyjna strata spada.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Zapisywanie modelu (strata spadła z {self.val_loss_min:.6f} do {val_loss:.6f})')\n",
    "        # Zapisz stan modelu\n",
    "        self.best_model_state = {k: v.cpu().detach() for k, v in model.state_dict().items()}\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Ładuje najlepszy model.\n",
    "        \"\"\"\n",
    "        if self.best_model_state is not None:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n",
    "def preprocess_original_features(X_orig, categorical_prefix='cat_'):\n",
    "    \"\"\"\n",
    "    Przetwarza oryginalne cechy, przygotowując je do użycia w modelu GNN.\n",
    "\n",
    "    Args:\n",
    "        X_orig: DataFrame z oryginalnymi cechami\n",
    "        categorical_prefix: Prefiks używany dla zmiennych kategorycznych\n",
    "\n",
    "    Returns:\n",
    "        tuple: (preprocessed_features, preprocessor, cat_dims, num_dims)\n",
    "            - preprocessed_features: przetworzone cechy\n",
    "            - preprocessor: obiekt do przetwarzania nowych danych\n",
    "            - cat_dims: wymiary cech kategorycznych po przetworzeniu\n",
    "            - num_dims: liczba cech numerycznych\n",
    "    \"\"\"\n",
    "    # Identyfikacja kolumn kategorycznych i numerycznych\n",
    "    categorical_cols = [col for col in X_orig.columns if col.startswith(categorical_prefix)]\n",
    "    numerical_cols = [col for col in X_orig.columns if col not in categorical_cols]\n",
    "\n",
    "    # Tworzenie przetwarzaczy\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', RobustScaler()),\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Łączenie przetwarzaczy\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    # Dopasowanie i transformacja\n",
    "    preprocessed_features = preprocessor.fit_transform(X_orig)\n",
    "\n",
    "    # Zapisanie wymiarów kategorycznych\n",
    "    cat_dims = 0\n",
    "    if categorical_cols:\n",
    "        cat_dims = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out().shape[0]\n",
    "\n",
    "    num_dims = len(numerical_cols)\n",
    "\n",
    "    return preprocessed_features, preprocessor, cat_dims, num_dims\n",
    "\n",
    "\n",
    "class EnhancedGNNMetaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Rozszerzony model GNN, który wykorzystuje zarówno predykcje modeli bazowych\n",
    "    jak i oryginalne cechy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pred_dim=1, feat_dim=0, hidden_channels=64,\n",
    "                 num_layers=2, dropout=0.1, initial_weights=None):\n",
    "        super(EnhancedGNNMetaModel, self).__init__()\n",
    "\n",
    "        self.pred_dim = pred_dim  # liczba modeli\n",
    "        self.feat_dim = feat_dim  # liczba oryginalnych cech\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Warstwa wejściowa dla predykcji modeli\n",
    "        self.pred_transform = nn.Linear(1, hidden_channels)\n",
    "\n",
    "        # Warstwa dla oryginalnych cech\n",
    "        if feat_dim > 0:\n",
    "            self.feat_encoder = nn.Sequential(\n",
    "                nn.Linear(feat_dim, hidden_channels),\n",
    "                nn.BatchNorm1d(hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "        # Warstwy konwolucji grafowej\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Warstwa integrująca cechy z grafu i oryginalne\n",
    "        self.integration_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * (2 if feat_dim > 0 else 1), hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Warstwy MLP końcowej predykcji\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "\n",
    "        # Warstwa wag dla modeli\n",
    "        if initial_weights is not None:\n",
    "            self.register_buffer('model_weights', torch.FloatTensor(initial_weights).view(1, -1))\n",
    "        else:\n",
    "            self.model_weights = None\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None, orig_features=None):\n",
    "        \"\"\"\n",
    "        Forward pass sieci.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor zawierający predykcje modeli [num_nodes, 1]\n",
    "            edge_index: Indeksy krawędzi grafu [2, num_edges]\n",
    "            batch: Wskaźniki batch do agregacji grafu\n",
    "            orig_features: Oryginalne cechy [batch_size, feat_dim]\n",
    "\n",
    "        Returns:\n",
    "            Tensor z ostateczną predykcją [batch_size, 1]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0) // self.pred_dim if self.pred_dim > 0 else x.size(0)\n",
    "\n",
    "        # Zastosowanie wag modeli\n",
    "        if self.model_weights is not None and self.pred_dim > 0:\n",
    "            reshaped_x = x.view(batch_size, self.pred_dim)\n",
    "            weighted_x = reshaped_x * self.model_weights\n",
    "            x = weighted_x.view(-1, 1)\n",
    "\n",
    "        # Przetwarzanie predykcji modeli przez GNN\n",
    "        x = self.pred_transform(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Warstwy konwolucji grafowej\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.1, training=self.training)\n",
    "\n",
    "        # Agregacja grafu\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "\n",
    "        graph_embedding = global_mean_pool(x, batch)\n",
    "\n",
    "        # Integracja z oryginalnymi cechami\n",
    "        if orig_features is not None and self.feat_dim > 0:\n",
    "            feat_embedding = self.feat_encoder(orig_features)\n",
    "            combined = torch.cat([graph_embedding, feat_embedding], dim=1)\n",
    "            combined = self.integration_layer(combined)\n",
    "        else:\n",
    "            combined = graph_embedding\n",
    "\n",
    "        # Końcowa predykcja\n",
    "        output = self.mlp(combined)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_enhanced_graph_dataset(model_predictions, original_features=None, y=None):\n",
    "    \"\"\"\n",
    "    Tworzy dataset grafów z predykcji modeli i oryginalnych cech.\n",
    "\n",
    "    Args:\n",
    "        model_predictions: Predykcje modeli [n_samples, n_models]\n",
    "        original_features: Przetworzone oryginalne cechy [n_samples, n_features]\n",
    "        y: Wartości docelowe (opcjonalne) [n_samples]\n",
    "\n",
    "    Returns:\n",
    "        list: Lista obiektów Data\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    n_models = model_predictions.shape[1]\n",
    "\n",
    "    # Tworzenie pełnego grafu - każdy model połączony z każdym innym\n",
    "    edge_index = []\n",
    "    for i in range(n_models):\n",
    "        for j in range(n_models):\n",
    "            if i != j:\n",
    "                edge_index.append([i, j])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    for i in range(model_predictions.shape[0]):\n",
    "        # Predykcje modeli jako cechy węzłów\n",
    "        x = torch.tensor(model_predictions[i:i + 1, :].reshape(-1, 1), dtype=torch.float)\n",
    "\n",
    "        # Oryginalne cechy jako atrybut grafu\n",
    "        graph_data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "        if original_features is not None:\n",
    "            graph_data.orig_features = torch.tensor(\n",
    "                original_features[i:i + 1, :], dtype=torch.float\n",
    "            )\n",
    "\n",
    "        if y is not None:\n",
    "            graph_data.y = torch.tensor(\n",
    "                [y.iloc[i] if hasattr(y, 'iloc') else y[i]], dtype=torch.float\n",
    "            )\n",
    "\n",
    "        data_list.append(graph_data)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "class EnhancedDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "    Rozszerzony DataLoader, który obsługuje zarówno predykcje modeli, jak i oryginalne cechy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        super(EnhancedDataLoader, self).__init__(dataset, **kwargs)\n",
    "\n",
    "        # Sprawdzenie, czy dataset zawiera oryginalne cechy\n",
    "        self.has_orig_features = hasattr(dataset[0], 'orig_features')\n",
    "\n",
    "    def collate_fn(self, data_list):\n",
    "        batch = super().collate_fn(data_list)\n",
    "\n",
    "        # Dodanie oryginalnych cech do batcha\n",
    "        if self.has_orig_features:\n",
    "            batch.orig_features = torch.cat([data.orig_features for data in data_list], dim=0)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "def train_enhanced_gnn_epoch(model, data_loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Trenuje rozszerzony model GNN przez jedną epokę.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass z oryginalnymi cechami\n",
    "        if hasattr(batch, 'orig_features'):\n",
    "            pred = model(batch.x, batch.edge_index, batch.batch, batch.orig_features)\n",
    "        else:\n",
    "            pred = model(batch.x, batch.edge_index, batch.batch)\n",
    "\n",
    "        # Obliczenie straty\n",
    "        target = batch.y.view(-1, 1)\n",
    "        loss = F.l1_loss(pred, target)  # MAE loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "def validate_enhanced_gnn(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Waliduje rozszerzony model GNN.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass z oryginalnymi cechami\n",
    "            if hasattr(batch, 'orig_features'):\n",
    "                pred = model(batch.x, batch.edge_index, batch.batch, batch.orig_features)\n",
    "            else:\n",
    "                pred = model(batch.x, batch.edge_index, batch.batch)\n",
    "\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "            targets.append(batch.y.view(-1, 1).cpu().numpy())\n",
    "\n",
    "    predictions = np.vstack(predictions)\n",
    "    targets = np.vstack(targets)\n",
    "\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    return mae\n",
    "\n",
    "\n",
    "\n",
    "def objective_enhanced_gnn_meta(trial, model_predictions, orig_features, y, initial_weights=None):\n",
    "    \"\"\"\n",
    "    Funkcja celu dla optymalizacji rozszerzonego modelu GNN.\n",
    "    \"\"\"\n",
    "    # Hiperparametry do optymalizacji\n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 100, 500)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.01, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    use_weight_layer = trial.suggest_categorical(\"use_weight_layer\", [True, False])\n",
    "\n",
    "    # Walidacja krzyżowa\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    mae_scores = []\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    for train_idx, val_idx in kf.split(model_predictions):\n",
    "        # Podział danych\n",
    "        X_train_pred, X_val_pred = model_predictions[train_idx], model_predictions[val_idx]\n",
    "        X_train_orig, X_val_orig = orig_features[train_idx], orig_features[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx], y.iloc[val_idx] if hasattr(y, 'iloc') else y[val_idx]\n",
    "\n",
    "        # Standardyzacja predykcji modeli\n",
    "        scaler_pred = RobustScaler()\n",
    "        X_train_pred_scaled = scaler_pred.fit_transform(X_train_pred)\n",
    "        X_val_pred_scaled = scaler_pred.transform(X_val_pred)\n",
    "\n",
    "        # Przygotowanie datasetów\n",
    "        weights_to_use = initial_weights if use_weight_layer else None\n",
    "        train_dataset = create_enhanced_graph_dataset(X_train_pred_scaled, X_train_orig, y_train)\n",
    "        val_dataset = create_enhanced_graph_dataset(X_val_pred_scaled, X_val_orig, y_val)\n",
    "\n",
    "        # Utworzenie loaderów\n",
    "        train_loader = EnhancedDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = EnhancedDataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Budowa modelu\n",
    "        model = EnhancedGNNMetaModel(\n",
    "            pred_dim=model_predictions.shape[1],\n",
    "            feat_dim=orig_features.shape[1] if orig_features is not None else 0,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            initial_weights=weights_to_use\n",
    "        ).to(device)\n",
    "\n",
    "        # Optymalizator\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "        # Trenowanie\n",
    "        for epoch in range(100):\n",
    "            train_loss = train_enhanced_gnn_epoch(model, train_loader, optimizer, device)\n",
    "            val_mae = validate_enhanced_gnn(model, val_loader, device)\n",
    "\n",
    "            scheduler.step(val_mae)\n",
    "            early_stopping(val_mae, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "        # Załadowanie najlepszego modelu\n",
    "        early_stopping.load_best_model(model)\n",
    "\n",
    "        # Ocena finalnego modelu\n",
    "        final_mae = validate_enhanced_gnn(model, val_loader, device)\n",
    "        mae_scores.append(final_mae)\n",
    "\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    print(f\"Enhanced GNN: Warstwy={num_layers}, Units={hidden_channels}, MAE={mean_mae:.4f}\")\n",
    "    return mean_mae\n",
    "\n",
    "\n",
    "def train_final_enhanced_gnn_model(model_predictions, orig_features, y, best_params,\n",
    "                                   initial_weights=None, test_pred=None, test_orig=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Trenuje finalny rozszerzony model GNN.\n",
    "    \"\"\"\n",
    "    # Standardyzacja predykcji modeli\n",
    "    scaler_pred = RobustScaler()\n",
    "    X_pred_scaled = scaler_pred.fit_transform(model_predictions)\n",
    "\n",
    "    # Przygotowanie danych treningowych\n",
    "    train_dataset = create_enhanced_graph_dataset(X_pred_scaled, orig_features, y)\n",
    "    train_loader = EnhancedDataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    # Przygotowanie danych testowych\n",
    "    test_loader = None\n",
    "    if test_pred is not None and y_test is not None:\n",
    "        test_pred_scaled = scaler_pred.transform(test_pred)\n",
    "        test_dataset = create_enhanced_graph_dataset(test_pred_scaled, test_orig, y_test)\n",
    "        test_loader = EnhancedDataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Trenowanie na: {device}\")\n",
    "\n",
    "    # Budowa modelu\n",
    "    weights_to_use = initial_weights if best_params['use_weight_layer'] else None\n",
    "    model = EnhancedGNNMetaModel(\n",
    "        pred_dim=model_predictions.shape[1],\n",
    "        feat_dim=orig_features.shape[1] if orig_features is not None else 0,\n",
    "        hidden_channels=best_params['hidden_channels'],\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout=best_params['dropout'],\n",
    "        initial_weights=weights_to_use\n",
    "    ).to(device)\n",
    "\n",
    "    # Optymalizator\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=20)\n",
    "\n",
    "    # Historia treningu\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_mae': [] if test_loader is not None else None\n",
    "    }\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    print(\"\\nRozpoczęcie treningu finalnego rozszerzonego modelu GNN...\")\n",
    "    for epoch in range(300):\n",
    "        # Trening\n",
    "        train_loss = train_enhanced_gnn_epoch(model, train_loader, optimizer, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "        # Walidacja\n",
    "        if test_loader is not None:\n",
    "            test_mae = validate_enhanced_gnn(model, test_loader, device)\n",
    "            history['test_mae'].append(test_mae)\n",
    "            print(f\"Epoka {epoch + 1}/300, Strata: {train_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "            scheduler.step(test_mae)\n",
    "            early_stopping(test_mae, model)\n",
    "        else:\n",
    "            print(f\"Epoka {epoch + 1}/300, Strata: {train_loss:.4f}\")\n",
    "            scheduler.step(train_loss)\n",
    "            early_stopping(train_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping w epoce {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Załadowanie najlepszego modelu\n",
    "    early_stopping.load_best_model(model)\n",
    "\n",
    "    # Ocena końcowa\n",
    "    test_mae = None\n",
    "    if test_loader is not None:\n",
    "        test_mae = validate_enhanced_gnn(model, test_loader, device)\n",
    "        print(f\"\\nKońcowy MAE na zbiorze testowym: {test_mae:.4f}\")\n",
    "\n",
    "    return model, history, test_mae, scaler_pred\n",
    "\n",
    "\n",
    "def predict_with_enhanced_gnn(model, model_predictions, orig_features, scaler_pred, device=None):\n",
    "    \"\"\"\n",
    "    Generuje predykcje przy użyciu rozszerzonego modelu GNN.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Standaryzacja predykcji modeli\n",
    "    pred_scaled = scaler_pred.transform(model_predictions)\n",
    "\n",
    "    # Przygotowanie danych\n",
    "    dataset = create_enhanced_graph_dataset(pred_scaled, orig_features)\n",
    "    loader = EnhancedDataLoader(dataset, batch_size=64)\n",
    "\n",
    "    # Generowanie predykcji\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            if hasattr(batch, 'orig_features'):\n",
    "                pred = model(batch.x, batch.edge_index, batch.batch, batch.orig_features)\n",
    "            else:\n",
    "                pred = model(batch.x, batch.edge_index, batch.batch)\n",
    "\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "\n",
    "    return np.vstack(predictions)\n",
    "\n",
    "\n",
    "\n",
    "# Przetwarzanie oryginalnych cech\n",
    "print(\"Przygotowanie oryginalnych cech...\")\n",
    "X_orig_processed, orig_preprocessor, cat_dims, num_dims = preprocess_original_features(X)\n",
    "print(\n",
    "    f\"Przetworzono cechy: {X_orig_processed.shape[1]} cech łącznie (numeryczne: {num_dims}, kategoryczne: {cat_dims})\")\n",
    "\n",
    "# Pozyskanie oryginalnych cech dla zbioru testowego\n",
    "if is_test_iteration and 'X_valid_end' in globals():\n",
    "    X_valid_orig_processed = orig_preprocessor.transform(X_valid_end)\n",
    "else:\n",
    "    X_valid_orig_processed = None\n",
    "\n",
    "# Optymalizacja rozszerzonego modelu GNN\n",
    "print(\"\\nOptymalizacja finalnego rozszerzonego meta modelu GNN...\")\n",
    "enhanced_gnn_study = optuna.create_study(direction=\"minimize\")\n",
    "enhanced_gnn_study.optimize(\n",
    "    lambda trial: objective_enhanced_gnn_meta(\n",
    "        trial,\n",
    "        oof_predictions,\n",
    "        X_orig_processed,\n",
    "        y,\n",
    "        initial_weights\n",
    "    ),\n",
    "    n_trials=5\n",
    ")\n",
    "\n",
    "print(f\"\\nNajlepsze MAE dla rozszerzonego modelu GNN: {enhanced_gnn_study.best_value:.4f}\")\n",
    "print(\"Najlepsze parametry:\", enhanced_gnn_study.best_params)\n",
    "\n",
    "# Trenowanie finalnego rozszerzonego modelu GNN\n",
    "final_enhanced_model, history_enhanced, final_enhanced_mae, scaler_pred = train_final_enhanced_gnn_model(\n",
    "    oof_predictions,\n",
    "    X_orig_processed,\n",
    "    y,\n",
    "    enhanced_gnn_study.best_params,\n",
    "    initial_weights if enhanced_gnn_study.best_params['use_weight_layer'] else None,\n",
    "    final_meta_features if is_test_iteration else None,\n",
    "    X_valid_orig_processed if is_test_iteration else None,\n",
    "    y_valid_end if is_test_iteration else None\n",
    ")\n",
    "\n",
    "# Wizualizacja procesu uczenia\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_enhanced['train_loss'], label='Train Loss')\n",
    "\n",
    "if history_enhanced['test_mae'] is not None:\n",
    "    plt.plot(history_enhanced['test_mae'], label='Test MAE')\n",
    "\n",
    "plt.title('Proces uczenia rozszerzonego meta modelu GNN')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Wartość')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Jeśli testowa iteracja, generujemy predykcje\n",
    "if is_test_iteration:\n",
    "    y_pred_enhanced = predict_with_enhanced_gnn(\n",
    "        final_enhanced_model,\n",
    "        final_meta_features,\n",
    "        X_valid_orig_processed,\n",
    "        scaler_pred\n",
    "    ).flatten()\n",
    "\n",
    "    final_enhanced_mae = mean_absolute_error(y_valid_end, y_pred_enhanced)\n",
    "    print(f\"\\nFinalny MAE rozszerzonego meta modelu GNN: {final_enhanced_mae:.4f}\")\n",
    "\n",
    "    # Porównanie predykcji z wartościami rzeczywistymi\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_valid_end, y_pred_enhanced, alpha=0.6, color='purple')\n",
    "    min_val = min(y.min(), y_pred_enhanced.min())\n",
    "    max_val = max(y.max(), y_pred_enhanced.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    plt.title('Porównanie predykcji rozszerzonego meta modelu GNN')\n",
    "    plt.xlabel('Wartości rzeczywiste')\n",
    "    plt.ylabel('Predykcje')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Porównanie wszystkich modeli\n",
    "    models_comparison = {\n",
    "        'Najlepszy model bazowy': min(model_mae_scores.values()),\n",
    "        'Standardowy meta model': final_mae,\n",
    "        'Rozszerzony GNN meta model': final_enhanced_mae\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(\n",
    "        range(len(models_comparison)),\n",
    "        list(models_comparison.values()),\n",
    "        color=['skyblue', 'blue', 'green', 'purple']\n",
    "    )\n",
    "    plt.xticks(range(len(models_comparison)), list(models_comparison.keys()), rotation=45)\n",
    "    plt.title('Porównanie MAE dla różnych podejść')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87027882a0b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_nn_model(model, features, batch_size=32, device=None):\n",
    "    \"\"\"\n",
    "    Generuje predykcje z modelu sieci neuronowej TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        model: Model TensorFlow/Keras\n",
    "        features: Cechy wejściowe do predykcji [n_samples, n_features]\n",
    "        batch_size: Rozmiar batcha dla predykcji\n",
    "        device: Parametr ignorowany (dla spójności interfejsu z funkcją GNN)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predykcje modelu sieci neuronowej\n",
    "    \"\"\"\n",
    "    # Sprawdzenie czy dane są już przeskalowane\n",
    "    if isinstance(features, np.ndarray) and features.ndim == 2:\n",
    "        X_features = features\n",
    "    else:\n",
    "        # Konwersja danych\n",
    "        X_features = np.array(features)\n",
    "\n",
    "    # Wykonanie predykcji\n",
    "    predictions = model.predict(X_features, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Upewnienie się, że wyjście ma odpowiedni format\n",
    "    if predictions.ndim > 1 and predictions.shape[1] == 1:\n",
    "        predictions = predictions.flatten()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_with_gnn_model(model, features, batch_size=64, device=None):\n",
    "    \"\"\"\n",
    "    Generuje predykcje z modelu grafowej sieci neuronowej PyTorch.\n",
    "\n",
    "    Args:\n",
    "        model: Model PyTorch GNN\n",
    "        features: Cechy wejściowe do predykcji [n_samples, n_features]\n",
    "        batch_size: Rozmiar batcha dla predykcji\n",
    "        device: Urządzenie do obliczeń (CPU/GPU)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predykcje modelu GNN\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    if isinstance(model, EnhancedGNNMetaModel):\n",
    "        n_samples = features.shape[0]\n",
    "        n_models = model.pred_dim if hasattr(model, 'pred_dim') and model.pred_dim > 0 else 2\n",
    "\n",
    "        # Tworzenie sztucznych predykcji modeli, potrzebnych do struktury grafu\n",
    "        dummy_predictions = np.zeros((n_samples, n_models))\n",
    "\n",
    "        # Standaryzacja sztucznych predykcji\n",
    "        scaler = RobustScaler()\n",
    "        dummy_scaled = scaler.fit_transform(dummy_predictions)\n",
    "\n",
    "        # Tworzenie datasetu grafowego\n",
    "        dataset = create_enhanced_graph_dataset(dummy_scaled, features)\n",
    "        loader = EnhancedDataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "        # Generowanie predykcji\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                if hasattr(batch, 'orig_features'):\n",
    "                    pred = model(batch.x, batch.edge_index, batch.batch, batch.orig_features)\n",
    "                else:\n",
    "                    pred = model(batch.x, batch.edge_index, batch.batch)\n",
    "\n",
    "                predictions.append(pred.cpu().numpy())\n",
    "\n",
    "        return np.vstack(predictions).flatten()\n",
    "    else:\n",
    "        # Dla innych modeli PyTorch, które nie są GNN\n",
    "        # Konwersja danych wejściowych na tensor\n",
    "        X_tensor = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Podział na batche dla efektywności\n",
    "        dataset = torch.utils.data.TensorDataset(X_tensor)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "        # Predykcja\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch_x = batch[0]\n",
    "                outputs = model(batch_x)\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "        return np.vstack(predictions).flatten()\n",
    "\n",
    "class AttentionFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model łączący prognozy z kilku źródeł (standardowa sieć neuronowa, GNN,\n",
    "    oryginalne cechy) przy użyciu mechanizmu uwagi (attention).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nn_dim=1, gnn_dim=1, orig_feat_dim=0,\n",
    "                 hidden_size=64, dropout=0.1):\n",
    "        super(AttentionFusionModel, self).__init__()\n",
    "\n",
    "        self.nn_dim = nn_dim\n",
    "        self.gnn_dim = gnn_dim\n",
    "        self.orig_feat_dim = orig_feat_dim\n",
    "\n",
    "        # Liczba źródeł (modeli) do łączenia\n",
    "        self.num_sources = sum([\n",
    "            1 if nn_dim > 0 else 0,  # Standardowa sieć neuronowa\n",
    "            1 if gnn_dim > 0 else 0,  # Grafowa sieć neuronowa\n",
    "            1 if orig_feat_dim > 0 else 0  # Oryginalne cechy\n",
    "        ])\n",
    "\n",
    "        # Warstwy enkodujące dla każdego źródła danych\n",
    "        if nn_dim > 0:\n",
    "            self.nn_encoder = nn.Sequential(\n",
    "                nn.Linear(nn_dim, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "        if gnn_dim > 0:\n",
    "            self.gnn_encoder = nn.Sequential(\n",
    "                nn.Linear(gnn_dim, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "        if orig_feat_dim > 0:\n",
    "            self.feat_encoder = nn.Sequential(\n",
    "                nn.Linear(orig_feat_dim, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "        # Mechanizm uwagi (attention)\n",
    "        # Query, Key, Value transformacje\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Skalowanie\n",
    "        self.scale = math.sqrt(hidden_size)\n",
    "\n",
    "        # Finalna warstwa predykcji\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, nn_pred=None, gnn_pred=None, orig_features=None):\n",
    "        \"\"\"\n",
    "        Forward pass modelu uwagi.\n",
    "\n",
    "        Args:\n",
    "            nn_pred: Predykcje ze standardowej sieci neuronowej [batch_size, nn_dim]\n",
    "            gnn_pred: Predykcje z GNN [batch_size, gnn_dim]\n",
    "            orig_features: Oryginalne cechy [batch_size, orig_feat_dim]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Finalna predykcja z połączonych źródeł [batch_size, 1]\n",
    "        \"\"\"\n",
    "        # Lista zakodowanych reprezentacji z każdego źródła\n",
    "        encoded_sources = []\n",
    "        batch_size = None\n",
    "\n",
    "        # Enkodowanie predykcji z sieci neuronowej\n",
    "        if nn_pred is not None and self.nn_dim > 0:\n",
    "            batch_size = nn_pred.size(0)\n",
    "            nn_encoded = self.nn_encoder(nn_pred)\n",
    "            encoded_sources.append(nn_encoded)\n",
    "\n",
    "        # Enkodowanie predykcji z GNN\n",
    "        if gnn_pred is not None and self.gnn_dim > 0:\n",
    "            batch_size = gnn_pred.size(0)\n",
    "            gnn_encoded = self.gnn_encoder(gnn_pred)\n",
    "            encoded_sources.append(gnn_encoded)\n",
    "\n",
    "        # Enkodowanie oryginalnych cech\n",
    "        if orig_features is not None and self.orig_feat_dim > 0:\n",
    "            batch_size = orig_features.size(0)\n",
    "            feat_encoded = self.feat_encoder(orig_features)\n",
    "            encoded_sources.append(feat_encoded)\n",
    "\n",
    "        if not encoded_sources:\n",
    "            raise ValueError(\"At least one source of predictions must be provided\")\n",
    "\n",
    "        # Konwersja listy na tensor [batch_size, num_sources, hidden_size]\n",
    "        encoded_stack = torch.stack(encoded_sources, dim=1)\n",
    "\n",
    "        # Mechanizm uwagi (attention)\n",
    "        # Transformacja Query, Key, Value\n",
    "        queries = self.query(encoded_stack)  # [batch_size, num_sources, hidden_size]\n",
    "        keys = self.key(encoded_stack)  # [batch_size, num_sources, hidden_size]\n",
    "        values = self.value(encoded_stack)  # [batch_size, num_sources, hidden_size]\n",
    "\n",
    "        # Obliczenie wag uwagi\n",
    "        # [batch_size, num_sources, num_sources]\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / self.scale\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Ważone zsumowanie wartości\n",
    "        # [batch_size, num_sources, hidden_size]\n",
    "        context_vector = torch.matmul(attention_weights, values)\n",
    "\n",
    "        # Średnia po źródłach\n",
    "        # [batch_size, hidden_size]\n",
    "        context_vector = context_vector.mean(dim=1)\n",
    "\n",
    "        # Finalna predykcja\n",
    "        output = self.output_layer(context_vector)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_attention_weights(self, nn_pred=None, gnn_pred=None, orig_features=None):\n",
    "        \"\"\"\n",
    "        Metoda do pobrania wag uwagi dla wizualizacji i interpretacji.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Macierz wag uwagi [batch_size, num_sources, num_sources]\n",
    "        \"\"\"\n",
    "        encoded_sources = []\n",
    "\n",
    "        # Enkodowanie predykcji z sieci neuronowej\n",
    "        if nn_pred is not None and self.nn_dim > 0:\n",
    "            nn_encoded = self.nn_encoder(nn_pred)\n",
    "            encoded_sources.append(nn_encoded)\n",
    "\n",
    "        # Enkodowanie predykcji z GNN\n",
    "        if gnn_pred is not None and self.gnn_dim > 0:\n",
    "            gnn_encoded = self.gnn_encoder(gnn_pred)\n",
    "            encoded_sources.append(gnn_encoded)\n",
    "\n",
    "        # Enkodowanie oryginalnych cech\n",
    "        if orig_features is not None and self.orig_feat_dim > 0:\n",
    "            feat_encoded = self.feat_encoder(orig_features)\n",
    "            encoded_sources.append(feat_encoded)\n",
    "\n",
    "        # Konwersja listy na tensor [batch_size, num_sources, hidden_size]\n",
    "        encoded_stack = torch.stack(encoded_sources, dim=1)\n",
    "\n",
    "        # Obliczenie wag uwagi\n",
    "        queries = self.query(encoded_stack)\n",
    "        keys = self.key(encoded_stack)\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / self.scale\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        return attention_weights.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def prepare_fusion_data(nn_predictions, gnn_predictions, orig_features=None):\n",
    "    \"\"\"\n",
    "    Przygotowuje dane do modelu uwagi, zapewniając spójność tensorów.\n",
    "\n",
    "    Args:\n",
    "        nn_predictions: Predykcje z sieci neuronowej\n",
    "        gnn_predictions: Predykcje z GNN\n",
    "        orig_features: Oryginalne cechy (opcjonalnie)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (nn_tensor, gnn_tensor, orig_tensor)\n",
    "    \"\"\"\n",
    "    # Konwersja na tensory PyTorch\n",
    "    nn_tensor = torch.tensor(nn_predictions, dtype=torch.float32)\n",
    "    gnn_tensor = torch.tensor(gnn_predictions, dtype=torch.float32)\n",
    "\n",
    "    # Sprawdzenie właściwego kształtu [batch_size, feature_dim]\n",
    "    if nn_tensor.dim() == 1:\n",
    "        nn_tensor = nn_tensor.unsqueeze(1)\n",
    "    if gnn_tensor.dim() == 1:\n",
    "        gnn_tensor = gnn_tensor.unsqueeze(1)\n",
    "\n",
    "    # Oryginalne cechy\n",
    "    orig_tensor = None\n",
    "    if orig_features is not None:\n",
    "        if isinstance(orig_features, np.ndarray):\n",
    "            orig_tensor = torch.tensor(orig_features, dtype=torch.float32)\n",
    "        else:\n",
    "            orig_tensor = orig_features\n",
    "\n",
    "    return nn_tensor, gnn_tensor, orig_tensor\n",
    "\n",
    "def visualize_attention_weights(model, nn_pred, gnn_pred, orig_features=None, sample_idx=None):\n",
    "    \"\"\"\n",
    "    Wizualizuje wagi uwagi dla wybranych próbek.\n",
    "\n",
    "    Args:\n",
    "        model: Wytrenowany model uwagi\n",
    "        nn_pred: Predykcje z sieci neuronowej\n",
    "        gnn_pred: Predykcje z GNN\n",
    "        orig_features: Oryginalne cechy (opcjonalnie)\n",
    "        sample_idx: Indeksy próbek do wizualizacji (None = wszystkie)\n",
    "    \"\"\"\n",
    "    # Przygotowanie danych\n",
    "    nn_tensor, gnn_tensor, orig_tensor = prepare_fusion_data(\n",
    "        nn_pred, gnn_pred, orig_features\n",
    "    )\n",
    "\n",
    "    # Wybór próbek\n",
    "    if sample_idx is not None:\n",
    "        nn_tensor = nn_tensor[sample_idx]\n",
    "        gnn_tensor = gnn_tensor[sample_idx]\n",
    "        if orig_tensor is not None:\n",
    "            orig_tensor = orig_tensor[sample_idx]\n",
    "\n",
    "    # Pobranie wag uwagi\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        attention_weights = model.get_attention_weights(nn_tensor, gnn_tensor, orig_tensor)\n",
    "\n",
    "    # Nazwy źródeł do wizualizacji\n",
    "    source_names = []\n",
    "    if model.nn_dim > 0:\n",
    "        source_names.append(\"Sieć neuronowa\")\n",
    "    if model.gnn_dim > 0:\n",
    "        source_names.append(\"Grafowa sieć\")\n",
    "    if model.orig_feat_dim > 0:\n",
    "        source_names.append(\"Oryginalne cechy\")\n",
    "\n",
    "    # Wizualizacja\n",
    "    num_samples = attention_weights.shape[0]\n",
    "    max_samples = min(5, num_samples)\n",
    "\n",
    "    plt.figure(figsize=(15, 4 * max_samples))\n",
    "\n",
    "    for i in range(max_samples):\n",
    "        plt.subplot(max_samples, 1, i + 1)\n",
    "        sns.heatmap(\n",
    "            attention_weights[i],\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"YlGnBu\",\n",
    "            xticklabels=source_names,\n",
    "            yticklabels=source_names\n",
    "        )\n",
    "        plt.title(f\"Uwaga dla próbki {i + 1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Średnie wagi uwagi dla wszystkich próbek\n",
    "    mean_weights = attention_weights.mean(axis=0)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        mean_weights,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        xticklabels=source_names,\n",
    "        yticklabels=source_names\n",
    "    )\n",
    "    plt.title(\"Średnie wagi uwagi dla wszystkich próbek\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_final_attention_fusion_model(model_predictions, orig_features, y,\n",
    "                                       nn_model=None, gnn_model=None,\n",
    "                                       hidden_size=64, dropout=0.2, lr=0.001,\n",
    "                                       batch_size=32, epochs=100, validation_ratio=0.1,\n",
    "                                       patience=15, device=None, is_test=False,\n",
    "                                       test_predictions=None, test_features=None, test_y=None):\n",
    "    \"\"\"\n",
    "    Trenuje finalny model uwagi (Attention Fusion) na całym zbiorze danych lub z walidacją.\n",
    "\n",
    "    Args:\n",
    "        model_predictions: Predykcje modeli bazowych na zbiorze treningowym [n_samples, n_models]\n",
    "        orig_features: Oryginalne cechy zbioru treningowego [n_samples, n_features]\n",
    "        y: Wartości docelowe zbioru treningowego [n_samples]\n",
    "        nn_model: Model standardowej sieci neuronowej (opcjonalnie)\n",
    "        gnn_model: Model grafowej sieci neuronowej (opcjonalnie)\n",
    "        hidden_size: Rozmiar warstw ukrytych\n",
    "        dropout: Współczynnik dropout\n",
    "        lr: Learning rate\n",
    "        batch_size: Rozmiar batcha\n",
    "        epochs: Maksymalna liczba epok\n",
    "        validation_ratio: Część danych do walidacji (używane tylko gdy is_test=False)\n",
    "        patience: Liczba epok do early stopping\n",
    "        device: Urządzenie do treningu (CPU/GPU)\n",
    "        is_test: Czy mamy dostępny zewnętrzny zbiór testowy\n",
    "        test_predictions: Predykcje modeli bazowych na zbiorze testowym (dla is_test=True)\n",
    "        test_features: Oryginalne cechy zbioru testowego (dla is_test=True)\n",
    "        test_y: Wartości docelowe zbioru testowego (dla is_test=True)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, history, test_mae)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(f\"Trening modelu uwagi na: {device}\")\n",
    "\n",
    "    # Generowanie predykcji z nn_model i gnn_model\n",
    "    if nn_model is not None and model_predictions is None:\n",
    "        print(\"Generowanie predykcji z modelu NN...\")\n",
    "        nn_train_pred = predict_with_nn_model(nn_model, orig_features).reshape(-1, 1)\n",
    "\n",
    "        if is_test and test_features is not None:\n",
    "            nn_test_pred = predict_with_nn_model(nn_model, test_features).reshape(-1, 1)\n",
    "    else:\n",
    "        nn_train_pred = model_predictions[:, 0].reshape(-1, 1)\n",
    "        nn_test_pred = test_predictions[:, 0].reshape(-1, 1) if is_test and test_predictions is not None else None\n",
    "\n",
    "    if gnn_model is not None and model_predictions is None:\n",
    "        print(\"Generowanie predykcji z modelu GNN...\")\n",
    "        gnn_train_pred = predict_with_gnn_model(gnn_model, orig_features).reshape(-1, 1)\n",
    "\n",
    "        if is_test and test_features is not None:\n",
    "            gnn_test_pred = predict_with_gnn_model(gnn_model, test_features).reshape(-1, 1)\n",
    "    else:\n",
    "        gnn_train_pred = model_predictions[:, 1].reshape(-1, 1) if model_predictions.shape[1] > 1 else nn_train_pred\n",
    "        gnn_test_pred = test_predictions[:, 1].reshape(-1, 1) if is_test and test_predictions is not None and test_predictions.shape[1] > 1 else nn_test_pred\n",
    "\n",
    "    # Konwersja wartości docelowych do numpy\n",
    "    y_np = y.values if isinstance(y, pd.Series) or isinstance(y, pd.DataFrame) else y\n",
    "    y_tensor = torch.tensor(y_np, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Przygotowanie danych treningowych\n",
    "    nn_train_tensor = torch.tensor(nn_train_pred, dtype=torch.float32)\n",
    "    gnn_train_tensor = torch.tensor(gnn_train_pred, dtype=torch.float32)\n",
    "    orig_train_tensor = torch.tensor(orig_features, dtype=torch.float32)\n",
    "\n",
    "    # Przygotowanie danych testowych\n",
    "    nn_test_tensor = torch.tensor(nn_test_pred, dtype=torch.float32) if is_test and nn_test_pred is not None else None\n",
    "    gnn_test_tensor = torch.tensor(gnn_test_pred, dtype=torch.float32) if is_test and gnn_test_pred is not None else None\n",
    "    orig_test_tensor = torch.tensor(test_features, dtype=torch.float32) if is_test and test_features is not None else None\n",
    "\n",
    "    if is_test and test_y is not None:\n",
    "        test_y_np = test_y.values if isinstance(test_y, pd.Series) or isinstance(test_y, pd.DataFrame) else test_y\n",
    "        y_test_tensor = torch.tensor(test_y_np, dtype=torch.float32).reshape(-1, 1)\n",
    "    else:\n",
    "        y_test_tensor = None\n",
    "\n",
    "    if is_test and all(x is not None for x in [nn_test_tensor, gnn_test_tensor, orig_test_tensor, y_test_tensor]):\n",
    "        # Używamy całego zbioru treningowego + zewnętrzny zbiór testowy\n",
    "        train_indices = list(range(len(y_tensor)))\n",
    "        has_validation = True\n",
    "    else:\n",
    "        # Brak zbioru testowego, tworzymy walidację z części zbioru treningowego\n",
    "        dataset_size = len(y_tensor)\n",
    "        indices = list(range(dataset_size))\n",
    "\n",
    "        if validation_ratio > 0:\n",
    "            split = int(np.floor(validation_ratio * dataset_size))\n",
    "            np.random.shuffle(indices)\n",
    "            train_indices, val_indices = indices[split:], indices[:split]\n",
    "            has_validation = True\n",
    "\n",
    "            # Tworzymy \"quasi-testowy\" zbiór z danych walidacyjnych\n",
    "            nn_test_tensor = nn_train_tensor[val_indices]\n",
    "            gnn_test_tensor = gnn_train_tensor[val_indices]\n",
    "            orig_test_tensor = orig_train_tensor[val_indices]\n",
    "            y_test_tensor = y_tensor[val_indices]\n",
    "        else:\n",
    "            # Brak walidacji, używamy całego zbioru\n",
    "            train_indices = indices\n",
    "            has_validation = False\n",
    "\n",
    "    # Inicjalizacja modelu\n",
    "    model = AttentionFusionModel(\n",
    "        nn_dim=nn_train_tensor.size(1),\n",
    "        gnn_dim=gnn_train_tensor.size(1),\n",
    "        orig_feat_dim=orig_train_tensor.size(1),\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Optymizator i funkcja straty\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.L1Loss()  # MAE loss\n",
    "\n",
    "    # Scheduler i early stopping\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=patience//3, min_lr=1e-6\n",
    "    )\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    # Historia treningu\n",
    "    history = {'train_loss': [], 'val_mae': [] if has_validation else None}\n",
    "\n",
    "    # Przygotowanie DataLoader dla treningu\n",
    "    train_dataset = TensorDataset(\n",
    "        nn_train_tensor[train_indices],\n",
    "        gnn_train_tensor[train_indices],\n",
    "        orig_train_tensor[train_indices],\n",
    "        y_tensor[train_indices]\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # DataLoader dla walidacji\n",
    "    val_loader = None\n",
    "    if has_validation:\n",
    "        val_dataset = TensorDataset(\n",
    "            nn_test_tensor,\n",
    "            gnn_test_tensor,\n",
    "            orig_test_tensor,\n",
    "            y_test_tensor\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Główna pętla treningowa\n",
    "    print(f\"Rozpoczęcie treningu modelu uwagi na {len(train_indices)} próbkach...\")\n",
    "    print(f\"Walidacja: {'Tak' if has_validation else 'Nie'}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_nn, batch_gnn, batch_orig, batch_y in train_loader:\n",
    "            # Przesłanie danych\n",
    "            batch_nn = batch_nn.to(device)\n",
    "            batch_gnn = batch_gnn.to(device)\n",
    "            batch_orig = batch_orig.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Zerowanie gradientów\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_nn, batch_gnn, batch_orig)\n",
    "\n",
    "            # Obliczenie straty\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * batch_y.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataset)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Walidacja, jeśli mamy dane walidacyjne\n",
    "        val_loss = None\n",
    "        if has_validation and val_loader is not None:\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_nn, batch_gnn, batch_orig, batch_y in val_loader:\n",
    "                    # Przesłanie danych\n",
    "                    batch_nn = batch_nn.to(device)\n",
    "                    batch_gnn = batch_gnn.to(device)\n",
    "                    batch_orig = batch_orig.to(device)\n",
    "                    batch_y = batch_y.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(batch_nn, batch_gnn, batch_orig)\n",
    "\n",
    "                    # Akumulacja straty\n",
    "                    val_loss = criterion(outputs, batch_y)\n",
    "                    total_val_loss += val_loss.item() * batch_y.size(0)\n",
    "\n",
    "            val_loss = total_val_loss / len(val_dataset)\n",
    "            history['val_mae'].append(val_loss)\n",
    "\n",
    "            print(f\"Epoka {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val MAE: {val_loss:.4f}\")\n",
    "\n",
    "            # Aktualizacja schedulera i early stopping\n",
    "            scheduler.step(val_loss)\n",
    "            early_stopping(val_loss, model)\n",
    "        else:\n",
    "            print(f\"Epoka {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "            scheduler.step(avg_train_loss)\n",
    "            early_stopping(avg_train_loss, model)\n",
    "\n",
    "        # Sprawdzenie early stopping\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping w epoce {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Załadowanie najlepszego modelu\n",
    "    early_stopping.load_best_model(model)\n",
    "\n",
    "    # Ocena końcowa, jeśli mamy dane walidacyjne\n",
    "    final_mae = None\n",
    "    if has_validation and val_loader is not None:\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_nn, batch_gnn, batch_orig, batch_y in val_loader:\n",
    "                # Przesłanie danych\n",
    "                batch_nn = batch_nn.to(device)\n",
    "                batch_gnn = batch_gnn.to(device)\n",
    "                batch_orig = batch_orig.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_nn, batch_gnn, batch_orig)\n",
    "\n",
    "                # Zbieranie predykcji i celów\n",
    "                all_preds.append(outputs.cpu().numpy())\n",
    "                all_targets.append(batch_y.cpu().numpy())\n",
    "\n",
    "        # Obliczenie końcowego MAE\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        final_mae = mean_absolute_error(all_targets, all_preds)\n",
    "        print(f\"Finalny MAE modelu uwagi: {final_mae:.4f}\")\n",
    "\n",
    "    return model, history, final_mae\n",
    "\n",
    "def generate_fusion_predictions(fusion_model, nn_predictions, gnn_predictions, orig_features, device=None):\n",
    "    \"\"\"\n",
    "    Generuje predykcje z modelu uwagi dla dowolnych danych.\n",
    "\n",
    "    Args:\n",
    "        fusion_model: Wytrenowany model uwagi\n",
    "        nn_predictions: Predykcje z sieci neuronowej\n",
    "        gnn_predictions: Predykcje z GNN\n",
    "        orig_features: Oryginalne cechy\n",
    "        device: Urządzenie do obliczeń\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predykcje modelu uwagi\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    fusion_model.eval()\n",
    "    fusion_model = fusion_model.to(device)\n",
    "\n",
    "    # Przygotowanie danych\n",
    "    nn_tensor = torch.tensor(nn_predictions, dtype=torch.float32).to(device)\n",
    "    gnn_tensor = torch.tensor(gnn_predictions, dtype=torch.float32).to(device)\n",
    "    orig_tensor = torch.tensor(orig_features, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Zapewnienie odpowiedniego kształtu\n",
    "    if nn_tensor.dim() == 1:\n",
    "        nn_tensor = nn_tensor.unsqueeze(1)\n",
    "    if gnn_tensor.dim() == 1:\n",
    "        gnn_tensor = gnn_tensor.unsqueeze(1)\n",
    "\n",
    "    # Tworzenie DataLoadera dla większej efektywności\n",
    "    dataset = TensorDataset(nn_tensor, gnn_tensor, orig_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "    # Generowanie predykcji\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_nn, batch_gnn, batch_orig in loader:\n",
    "            # Forward pass\n",
    "            outputs = fusion_model(batch_nn, batch_gnn, batch_orig)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_preds)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Pełny trening modelu uwagi\n",
    "print(\"\\nTrening finalnego modelu uwagi (Attention Fusion)...\")\n",
    "\n",
    "# Sprawdzenie czy mamy predykcje modeli\n",
    "have_model_predictions = False\n",
    "if 'oof_predictions' in globals() and oof_predictions is not None:\n",
    "    have_model_predictions = True\n",
    "    nn_train_preds = oof_predictions[:, 0].reshape(-1, 1)\n",
    "    gnn_train_preds = oof_predictions[:, 1].reshape(-1, 1) if oof_predictions.shape[1] > 1 else nn_train_preds\n",
    "    print(f\"Używanie istniejących predykcji z {oof_predictions.shape[1]} modeli bazowych\")\n",
    "\n",
    "if is_test_iteration:\n",
    "    print(\"Trenowanie modelu uwagi z użyciem zewnętrznego zbioru testowego\")\n",
    "\n",
    "    # Sprawdzenie i przygotowanie danych testowych\n",
    "    nn_test_preds = None\n",
    "    gnn_test_preds = None\n",
    "\n",
    "    if 'y_pred_final' in globals() and 'y_pred_enhanced' in globals():\n",
    "        nn_test_preds = y_pred_final.reshape(-1, 1)\n",
    "        gnn_test_preds = y_pred_enhanced.reshape(-1, 1)\n",
    "    elif 'final_meta_features' in globals():\n",
    "        nn_test_preds = final_meta_features[:, 0].reshape(-1, 1)\n",
    "        gnn_test_preds = final_meta_features[:, 1].reshape(-1, 1) if final_meta_features.shape[1] > 1 else nn_test_preds\n",
    "\n",
    "    # Trening modelu uwagi z zewnętrzną walidacją\n",
    "    fusion_model, fusion_history, fusion_mae = train_final_attention_fusion_model(\n",
    "        oof_predictions if have_model_predictions else None,\n",
    "        X_orig_processed,\n",
    "        y,\n",
    "        final_meta_model if not have_model_predictions else None,\n",
    "        final_enhanced_model if not have_model_predictions else None,\n",
    "        hidden_size=128,\n",
    "        dropout=0.2,\n",
    "        lr=0.001,\n",
    "        batch_size=32,\n",
    "        epochs=1000,\n",
    "        validation_ratio=0.1,\n",
    "        patience=20,\n",
    "        is_test=True,\n",
    "        test_predictions=np.hstack([nn_test_preds, gnn_test_preds]) if nn_test_preds is not None else None,\n",
    "        test_features=X_valid_orig_processed,\n",
    "        test_y=y_valid_end\n",
    "    )\n",
    "\n",
    "    # Wizualizacja procesu uczenia\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(fusion_history['train_loss'], label='Train Loss')\n",
    "    if fusion_history['val_mae'] is not None:\n",
    "        plt.plot(fusion_history['val_mae'], label='Val MAE')\n",
    "    plt.title('Proces uczenia finalnego modelu uwagi (Attention Fusion)')\n",
    "    plt.xlabel('Epoka')\n",
    "    plt.ylabel('Wartość')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Wizualizacja wag uwagi\n",
    "    visualize_attention_weights(\n",
    "        fusion_model,\n",
    "        nn_test_preds,\n",
    "        gnn_test_preds,\n",
    "        X_valid_orig_processed,\n",
    "        sample_idx=np.random.choice(len(nn_test_preds), size=5, replace=False)\n",
    "    )\n",
    "\n",
    "    # Porównanie wszystkich modeli\n",
    "    models_comparison = {\n",
    "        'Najlepszy model bazowy': min(model_mae_scores.values()),\n",
    "        'Standardowy meta model': final_mae,\n",
    "        'Rozszerzony GNN meta model': final_enhanced_mae,\n",
    "        'Model uwagi (Attention Fusion)': fusion_mae\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(\n",
    "        range(len(models_comparison)),\n",
    "        list(models_comparison.values()),\n",
    "        color=['skyblue', 'blue', 'green', 'red']\n",
    "    )\n",
    "    plt.xticks(range(len(models_comparison)), list(models_comparison.keys()), rotation=45)\n",
    "    plt.title('Porównanie MAE dla różnych podejść')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # Przypadek treningu finalnego modelu bez zewnętrznego zbioru testowego\n",
    "    print(\"Trenowanie finalnego modelu uwagi na pełnym zbiorze z cross-validacją\")\n",
    "\n",
    "    # Trening modelu uwagi z walidacją wewnętrzną\n",
    "    fusion_model, fusion_history, _ = train_final_attention_fusion_model(\n",
    "        oof_predictions if have_model_predictions else None,\n",
    "        X_orig_processed,\n",
    "        y,\n",
    "        final_meta_model if not have_model_predictions and 'final_meta_model' in globals() else None,\n",
    "        final_enhanced_model if not have_model_predictions and 'final_enhanced_model' in globals() else None,\n",
    "        hidden_size=128,\n",
    "        dropout=0.2,\n",
    "        lr=0.001,\n",
    "        batch_size=32,\n",
    "        epochs=1000,\n",
    "        validation_ratio=0.15,\n",
    "        patience=25,\n",
    "        is_test=False\n",
    "    )\n",
    "\n",
    "    # Wizualizacja procesu uczenia\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(fusion_history['train_loss'], label='Train Loss')\n",
    "    if fusion_history['val_mae'] is not None:\n",
    "        plt.plot(fusion_history['val_mae'], label='Val MAE')\n",
    "    plt.title('Proces uczenia finalnego modelu uwagi (Attention Fusion)')\n",
    "    plt.xlabel('Epoka')\n",
    "    plt.ylabel('Wartość')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Wizualizacja wag uwagi\n",
    "    if have_model_predictions:\n",
    "        sample_indices = np.random.choice(len(y), size=5, replace=False)\n",
    "\n",
    "        # Przygotowanie predykcji modeli\n",
    "        nn_pred_viz = oof_predictions[sample_indices, 0].reshape(-1, 1)\n",
    "        gnn_pred_viz = oof_predictions[sample_indices, 1].reshape(-1, 1) if oof_predictions.shape[1] > 1 else nn_pred_viz\n",
    "\n",
    "        visualize_attention_weights(\n",
    "            fusion_model,\n",
    "            nn_pred_viz,\n",
    "            gnn_pred_viz,\n",
    "            X_orig_processed[sample_indices],\n",
    "            sample_idx=None\n",
    "        )\n",
    "\n",
    "def cross_validate_attention_fusion(nn_predictions, gnn_predictions, orig_features, y,\n",
    "                                   hidden_size=64, dropout=0.2, lr=0.001,\n",
    "                                   n_splits=5, epochs=50, patience=10):\n",
    "    \"\"\"\n",
    "    Przeprowadza cross-walidację dla modelu uwagi.\n",
    "\n",
    "    Args:\n",
    "        nn_predictions: Predykcje z sieci neuronowej\n",
    "        gnn_predictions: Predykcje z GNN\n",
    "        orig_features: Oryginalne cechy\n",
    "        y: Wartości docelowe\n",
    "        hidden_size: Rozmiar warstw ukrytych\n",
    "        dropout: Współczynnik dropout\n",
    "        lr: Learning rate\n",
    "        n_splits: Liczba podziałów w cross-walidacji\n",
    "        epochs: Maksymalna liczba epok\n",
    "        patience: Liczba epok do early stopping\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mean_mae, std_mae, all_maes)\n",
    "    \"\"\"\n",
    "    print(f\"Rozpoczęcie {n_splits}-krotnej cross-walidacji modelu uwagi...\")\n",
    "\n",
    "    # Przygotowanie KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Przygotowanie predykcji i cech\n",
    "    nn_pred = nn_predictions.reshape(-1, 1) if nn_predictions.ndim == 1 else nn_predictions\n",
    "    gnn_pred = gnn_predictions.reshape(-1, 1) if gnn_predictions.ndim == 1 else gnn_predictions\n",
    "\n",
    "    # Listy do przechowywania wyników\n",
    "    all_maes = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(y)):\n",
    "        print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "\n",
    "        # Podział danych\n",
    "        nn_train, nn_val = nn_pred[train_idx], nn_pred[val_idx]\n",
    "        gnn_train, gnn_val = gnn_pred[train_idx], gnn_pred[val_idx]\n",
    "        orig_train, orig_val = orig_features[train_idx], orig_features[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Trening modelu\n",
    "        fusion_model, history, val_mae = train_final_attention_fusion_model(\n",
    "            None,\n",
    "            orig_train,\n",
    "            y_train,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout=dropout,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            epochs=epochs,\n",
    "            validation_ratio=0.0,\n",
    "            patience=patience,\n",
    "            is_test=True,\n",
    "            test_predictions=np.hstack([nn_val, gnn_val]),\n",
    "            test_features=orig_val,\n",
    "            test_y=y_val\n",
    "        )\n",
    "\n",
    "        all_maes.append(val_mae)\n",
    "        print(f\"Fold {fold+1} MAE: {val_mae:.4f}\")\n",
    "\n",
    "    # Obliczenie statystyk\n",
    "    mean_mae = np.mean(all_maes)\n",
    "    std_mae = np.std(all_maes)\n",
    "\n",
    "    print(f\"\\nWyniki cross-walidacji:\")\n",
    "    print(f\"Średni MAE: {mean_mae:.4f} ± {std_mae:.4f}\")\n",
    "    print(f\"Wszystkie MAE: {[f'{mae:.4f}' for mae in all_maes]}\")\n",
    "\n",
    "    return mean_mae, std_mae, all_maes\n",
    "\n",
    "if is_test_iteration:\n",
    "    print(\"\\nPrzeprowadzanie cross-walidacji dla modelu uwagi...\")\n",
    "\n",
    "    have_predictions = False\n",
    "    if 'oof_predictions' in globals() and oof_predictions is not None:\n",
    "        have_predictions = True\n",
    "        nn_cross_preds = oof_predictions[:, 0]\n",
    "        gnn_cross_preds = oof_predictions[:, 1] if oof_predictions.shape[1] > 1 else nn_cross_preds\n",
    "        print(f\"Używanie istniejących predykcji z {oof_predictions.shape[1]} modeli bazowych\")\n",
    "    else:\n",
    "        print(\"Nie znaleziono predykcji modeli bazowych - wychodzenie...\")\n",
    "\n",
    "    if have_predictions:\n",
    "        # cross-walidacja\n",
    "        mean_mae, std_mae, all_maes = cross_validate_attention_fusion(\n",
    "            nn_cross_preds,\n",
    "            gnn_cross_preds,\n",
    "            X_orig_processed,\n",
    "            y,\n",
    "            hidden_size=128,\n",
    "            dropout=0.2,\n",
    "            lr=0.001,\n",
    "            n_splits=5,\n",
    "            epochs=50,\n",
    "            patience=10\n",
    "        )\n",
    "\n",
    "        # Porównanie z wcześniejszymi wynikami\n",
    "        cv_results = {}\n",
    "\n",
    "        if 'model_mae_scores' in globals() and 'NN' in model_mae_scores:\n",
    "            cv_results['NN Model (CV)'] = model_mae_scores['NN']\n",
    "\n",
    "        if 'final_enhanced_mae' in globals():\n",
    "            cv_results['GNN Model (CV)'] = final_enhanced_mae\n",
    "\n",
    "        # Dodanie wyniku modelu uwagi\n",
    "        cv_results['Attention Fusion (CV)'] = mean_mae\n",
    "\n",
    "        # Wizualizacja porównania\n",
    "        if cv_results:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            colors = ['skyblue', 'green', 'red']\n",
    "            plt.bar(\n",
    "                range(len(cv_results)),\n",
    "                list(cv_results.values()),\n",
    "                color=colors[:len(cv_results)]\n",
    "            )\n",
    "            plt.xticks(range(len(cv_results)), list(cv_results.keys()), rotation=45)\n",
    "            plt.title('Porównanie MAE w cross-walidacji')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc22f06bfbb58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predykcje\n",
    "def space_formatter(x, pos):\n",
    "    return f\"{int(x):,}\".replace(',', ' ')\n",
    "\n",
    "predictions_combined = []\n",
    "\n",
    "if is_test_iteration:\n",
    "    print(\"Wykonywanie predykcji z wykorzystaniem pełnego modelu kaskadowego z mechanizmem uwagi...\")\n",
    "\n",
    "    # WARSTWA 1: Predykcje bazowych modeli\n",
    "    print(\"Warstwa 1: Predykcje modeli bazowych...\")\n",
    "    preds_xgb = xgb_model.predict(X_valid_end)\n",
    "    preds_cat = cat_model.predict(X_valid_end)\n",
    "    preds_lgb = lgb_model.predict(X_valid_end)\n",
    "    preds_ada = ada_pipeline.predict(X_valid_end)\n",
    "    preds_hgbm = hgbm_model.predict(X_valid_end)\n",
    "\n",
    "    print(f\"XGB MAE: {mean_absolute_error(y_valid_end, preds_xgb):.4f}\")\n",
    "    print(f\"CatBoost MAE: {mean_absolute_error(y_valid_end, preds_cat):.4f}\")\n",
    "    print(f\"LGBM MAE: {mean_absolute_error(y_valid_end, preds_lgb):.4f}\")\n",
    "    print(f\"AdaBoost MAE: {mean_absolute_error(y_valid_end, preds_ada):.4f}\")\n",
    "    print(f\"HGBM MAE: {mean_absolute_error(y_valid_end, preds_hgbm):.4f}\")\n",
    "\n",
    "    layer1_features = np.column_stack((preds_xgb, preds_cat, preds_lgb, preds_ada, preds_hgbm))\n",
    "\n",
    "    layer1_features_scaled = scaler.transform(layer1_features)\n",
    "\n",
    "    # WARSTWA 2: Predykcje modeli drugiej warstwy\n",
    "    print(\"Warstwa 2: Predykcje z modeli pośrednich...\")\n",
    "    layer2_predictions = {}\n",
    "\n",
    "    for model_name, model in models_dict.items():\n",
    "        if model_name == \"NN\":\n",
    "            preds = model.predict(layer1_features_scaled, verbose=0).flatten()\n",
    "        elif model_name == \"MLP\":\n",
    "            preds = model.predict(layer1_features_scaled)\n",
    "        else:\n",
    "            preds = model.predict(layer1_features)\n",
    "\n",
    "        layer2_predictions[model_name] = preds\n",
    "        print(f\"  - Model {model_name}: MAE = {mean_absolute_error(y_valid_end, layer2_predictions[model_name]):.4f}\")\n",
    "\n",
    "    layer2_features = np.column_stack(list(layer2_predictions.values()))\n",
    "\n",
    "    # WARSTWA 3: Trenowanie finalnych modeli meta z wykorzystaniem predykcji z warstwy 2\n",
    "    print(\"Warstwa 3: Generowanie predykcji z finalnych modeli...\")\n",
    "\n",
    "    nn_test_preds = predict_with_nn_model(final_meta_model, scaler_final.transform(layer2_features)).reshape(-1, 1)\n",
    "    print(f\"  - Model NN (sieć neuronowa): MAE = {mean_absolute_error(y_valid_end, nn_test_preds):.4f}\")\n",
    "\n",
    "    X_valid_orig_processed = orig_preprocessor.transform(X_valid_end)\n",
    "\n",
    "    predictions = predict_with_enhanced_gnn(\n",
    "        final_enhanced_model,\n",
    "        layer2_features,\n",
    "        X_valid_orig_processed,\n",
    "        scaler_pred\n",
    "    ).reshape(-1, 1)\n",
    "    print(f\"  - Model GNN (grafowa sieć neuronowa): MAE = {mean_absolute_error(y_valid_end, predictions):.4f}\")\n",
    "\n",
    "    predictions = predictions.flatten()\n",
    "\n",
    "    # Porównanie wyników wszystkich głównych modeli\n",
    "    comparison_results = {\n",
    "        \"NN (sieć neuronowa)\": mean_absolute_error(y_valid_end, nn_test_preds),\n",
    "        \"GNN (grafowa sieć)\": mean_absolute_error(y_valid_end, predictions),\n",
    "    }\n",
    "\n",
    "    # Dodaj najlepsze modele z warstwy 2\n",
    "    best_layer2_models = sorted([(name, mean_absolute_error(y_valid_end, preds))\n",
    "                                for name, preds in layer2_predictions.items()],\n",
    "                               key=lambda x: x[1])[:3]\n",
    "\n",
    "    for name, mae in best_layer2_models:\n",
    "        comparison_results[f\"Warstwa 2: {name}\"] = mae\n",
    "\n",
    "    # Wizualizacja porównania wyników\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    names = list(comparison_results.keys())\n",
    "    values = list(comparison_results.values())\n",
    "    colors = ['skyblue', 'green', 'red'] + ['gray'] * len(best_layer2_models)\n",
    "\n",
    "    plt.bar(range(len(comparison_results)), values, color=colors)\n",
    "    plt.xticks(range(len(comparison_results)), names, rotation=45, ha='right')\n",
    "    plt.title('Porównanie MAE różnych modeli na zbiorze testowym')\n",
    "    plt.ylabel('MAE (niższe = lepsze)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Losowy wybór przypadków\n",
    "    random_indices = np.random.choice(len(y_valid_end), min(len(y_valid_end), len(y_valid_end)), replace=False)\n",
    "\n",
    "    # Tabela z wartościami\n",
    "    result_table = pd.DataFrame({\n",
    "        'Indeks': random_indices,\n",
    "        'NN Predykcja': nn_test_preds[random_indices, 0],\n",
    "        'GNN Predykcja': gnn_test_preds[random_indices, 0],\n",
    "        'Finalna Predykcja': predictions[random_indices],\n",
    "        'Wartość Rzeczywista': y_valid_end.iloc[random_indices].values if isinstance(y_valid_end, pd.Series) else\n",
    "        y_valid_end[random_indices]\n",
    "    })\n",
    "\n",
    "    display(result_table.round(4))\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.hist(predictions, bins=20, alpha=0.7, color='blue', label='Predykcje')\n",
    "    plt.xlabel(\"Wartość błędu\")\n",
    "    plt.ylabel(\"Liczebność\")\n",
    "    plt.title(\"Histogram predykcji\")\n",
    "    plt.ticklabel_format(style='plain', axis='x')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(space_formatter))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Wykres porównawczy predykcji vs wartości rzeczywiste\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(y_valid_end, predictions, alpha=0.5)\n",
    "    plt.plot([y_valid_end.min(), y_valid_end.max()], [y_valid_end.min(), y_valid_end.max()], 'r--')\n",
    "    plt.xlabel('Wartości rzeczywiste')\n",
    "    plt.ylabel('Predykcje')\n",
    "    plt.title('Porównanie predykcji z wartościami rzeczywistymi')\n",
    "    plt.ticklabel_format(style='plain')\n",
    "\n",
    "    # Formatowanie osi\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(space_formatter))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(space_formatter))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9998da652c2e985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_test_iteration:\n",
    "    x = np.arange(len(y_valid_end))\n",
    "    y_valid_array = y_valid_end.to_numpy().flatten()\n",
    "    predictions_array = np.array(predictions).flatten()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Wykres punktowy dla wartości rzeczywistych (niebieski)\n",
    "    plt.scatter(x, y_valid_end, color='blue', label='Wartość rzeczywista')\n",
    "\n",
    "    # Wykres punktowy dla wartości estymowanych (czerwony)\n",
    "    plt.scatter(x, predictions, color='red', label='Wartość estymowana')\n",
    "\n",
    "    # Dla każdej pary linia łącząca punkty\n",
    "    for i in range(len(x)):\n",
    "        plt.plot([x[i], x[i]], [y_valid_array[i], predictions_array[i]], color='gray', linewidth=0.5)\n",
    "\n",
    "    plt.xlabel('Indeks')\n",
    "    plt.ylabel('Wartość')\n",
    "    plt.title(f'Porównanie wartości rzeczywistych i estymowanych')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(space_formatter))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294053240eb1fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_test_iteration:\n",
    "    valid_preds = pd.read_csv('data/Test.csv')\n",
    "\n",
    "    corrections = {\n",
    "        'SWIZERLAND': 'SWITZERLAND',\n",
    "        'UNITED STATES OF AMERICA': 'UNITED STATES',\n",
    "        'COMORO': 'COMOROS',\n",
    "        'MALT': 'MALTA',\n",
    "        'UAE': 'UNITED ARAB EMIRATES',\n",
    "        'UKRAIN': 'UKRAINE',\n",
    "        'DRC': 'CONGO (DEMOCRATIC REPUBLIC OF THE)',\n",
    "        'SWAZILAND': 'ESWATINI',\n",
    "        'COSTARICA': 'COSTA RICA',\n",
    "        'SCOTLAND': 'UNITED KINGDOM',\n",
    "        'PHILIPINES': 'PHILIPPINES',\n",
    "        'BOSNIA': 'BOSNIA AND HERZEGOVINA',\n",
    "        'CAPE VERDE': 'CABO VERDE',\n",
    "        'MORROCO': 'MOROCCO',\n",
    "        'SOMALI': 'SOMALIA',\n",
    "        'KOREA': 'SOUTH KOREA',\n",
    "        'SAUD ARABIA': 'SAUDI ARABIA',\n",
    "    }\n",
    "\n",
    "    valid_preds['country'] = valid_preds['country'].replace(corrections)\n",
    "\n",
    "    valid_preds = pd.merge(valid_preds, df_regions, how='left', left_on='country', right_on='name')\n",
    "    valid_preds = valid_preds.drop(columns=['name'])\n",
    "    print(valid_preds.columns)\n",
    "    print(df_regions.columns)\n",
    "\n",
    "    # Check before cleansing\n",
    "    for column in valid_preds.columns:\n",
    "        empty_count = valid_preds[column].isna().sum()\n",
    "\n",
    "        if empty_count > 0:\n",
    "            print(f\"Column '{column}' has {empty_count} empty fields (NaN).\")\n",
    "\n",
    "    valid_preds.loc[valid_preds['most_impressing'].isna(), 'most_impressing'] = 'No comments'\n",
    "\n",
    "    # Travel with imputation\n",
    "    features_tw = ['region', 'age_group', 'total_female',\n",
    "           'total_male', 'purpose', 'main_activity', 'info_source',\n",
    "           'tour_arrangement', 'package_transport_int', 'package_accomodation',\n",
    "           'package_food', 'package_transport_tz', 'package_sightseeing',\n",
    "           'package_guided_tour', 'package_insurance', 'night_mainland',\n",
    "           'night_zanzibar', 'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "    features_cat_tw = ['region', 'age_group', 'purpose', 'main_activity', 'info_source', 'tour_arrangement',\n",
    "                    'package_transport_int', 'package_accomodation', 'package_food', 'package_transport_tz',\n",
    "                    'package_sightseeing', 'package_guided_tour', 'package_insurance',\n",
    "                    'payment_mode', 'first_trip_tz', 'most_impressing']\n",
    "\n",
    "\n",
    "    # Filtrowanie danych treningowych - uwzględniamy tylko dozwolone kategorie\n",
    "    valid_preds_no_nan = valid_preds.dropna()\n",
    "\n",
    "    allowed_categories = ['Children', 'Friends/Relatives', 'Spouse', 'Spouse and Children']\n",
    "    valid_preds_imp = valid_preds_no_nan[valid_preds_no_nan['travel_with'].isin(allowed_categories)].copy()\n",
    "    X_train_tw = pd.get_dummies(valid_preds_imp[features_tw], columns=features_cat_tw)\n",
    "    y_train_tw = valid_preds_imp['travel_with']\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    rf_tw = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_tw.fit(X_train_tw, y_train_tw)\n",
    "\n",
    "    print(\"Cleansing test dataset\")\n",
    "    for index, row in valid_preds[valid_preds.isna().any(axis=1)].iterrows():\n",
    "        if pd.isna(row['travel_with']) & (row['total_male'] + row['total_female'] == 1) & (not pd.isna(row['total_male'])) & (not pd.isna(row['total_female'])):\n",
    "            valid_preds.loc[index, 'travel_with'] = 'Alone'\n",
    "        if pd.isna(row['travel_with']) & (row['total_male'] + row['total_female'] > 1) & (not pd.isna(row['total_male'])) & (not pd.isna(row['total_female'])):\n",
    "            dummy_df = pd.get_dummies(row[features_tw], columns=features_cat_tw)\n",
    "            dummy_df.columns = dummy_df.columns.astype(str)\n",
    "            dummy_df = dummy_df.reindex(columns=X_train_tw.columns, fill_value=0)\n",
    "\n",
    "            predicted_value = rf_tw.predict(dummy_df)\n",
    "\n",
    "            print(f\"\\nrow\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\"\n",
    "                  f\"\\n{predicted_value[0]}\")\n",
    "\n",
    "            valid_preds.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "        if (row['travel_with'] == 'Alone') & ((pd.isna(row['total_male'])) | (pd.isna(row['total_female']))):\n",
    "            if pd.isna(row['total_female']):\n",
    "                valid_preds.loc[index, 'total_female'] = 0\n",
    "\n",
    "                print(f\"\\nrow\"\n",
    "                      f\"\\n{row['travel_with']}\"\n",
    "                      f\"\\n{row['total_male']}\"\n",
    "                      f\"\\n{row['total_female']}\"\n",
    "                      f\"\\n{0}\")\n",
    "            elif pd.isna(row['total_female']):\n",
    "                valid_preds.loc[index, 'total_male'] = 0\n",
    "\n",
    "                print(f\"\\nrow\"\n",
    "                      f\"\\n{row['travel_with']}\"\n",
    "                      f\"\\n{row['total_male']}\"\n",
    "                      f\"\\n{row['total_female']}\"\n",
    "                      f\"\\n{0}\")\n",
    "            elif pd.isna(row['total_male']) & pd.isna(row['total_female']):\n",
    "                # Gentleman\n",
    "                valid_preds.loc[index, 'total_male'] = 0\n",
    "                valid_preds.loc[index, 'total_female'] = 1\n",
    "\n",
    "        elif (pd.isna(row['total_female'])) & (row['total_male'] > 0):\n",
    "\n",
    "            if row['total_male'] > 1:\n",
    "                valid_preds.loc[index, 'total_female'] = 0\n",
    "            else:\n",
    "                valid_preds.loc[index, 'total_female'] = 1\n",
    "\n",
    "            print(f\"\\nrow\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\"\n",
    "                  f\"\\n{0}\")\n",
    "\n",
    "        elif (pd.isna(row['total_male'])) & (row['total_female'] > 0):\n",
    "\n",
    "            if row['total_female'] > 1:\n",
    "                valid_preds.loc[index, 'total_male'] = 0\n",
    "            else:\n",
    "                valid_preds.loc[index, 'total_male'] = 1\n",
    "\n",
    "            print(f\"\\nrow\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\"\n",
    "                  f\"\\n{0}\")\n",
    "\n",
    "        if (row['total_male'] + row['total_female'] == 0) & (not pd.isna(row['total_male'])) & (not pd.isna(row['total_female'])):\n",
    "\n",
    "            # Gentleman\n",
    "            valid_preds.loc[index, 'total_female'] = 1\n",
    "\n",
    "            if pd.isna(row['travel_with']):\n",
    "                dummy_df = pd.get_dummies(row[features_tw], columns=features_cat_tw)\n",
    "                dummy_df.columns = dummy_df.columns.astype(str)\n",
    "                dummy_df = dummy_df.reindex(columns=X_train_tw.columns, fill_value=0)\n",
    "\n",
    "                predicted_value = rf_tw.predict(dummy_df)\n",
    "\n",
    "                valid_preds.loc[index, 'travel_with'] = predicted_value[0]\n",
    "\n",
    "            print(f\"\\nrow widmo\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\")\n",
    "\n",
    "    # Check after cleansing\n",
    "    for index, row in valid_preds[valid_preds.isna().any(axis=1)].iterrows():\n",
    "        if pd.isna(row['travel_with']) | pd.isna(row['total_male']) | pd.isna(row['total_female']):\n",
    "            print(f\"\\nrow\"\n",
    "                  f\"\\n{row['travel_with']}\"\n",
    "                  f\"\\n{row['total_male']}\"\n",
    "                  f\"\\n{row['total_female']}\")\n",
    "\n",
    "    valid_preds['total_people'] = valid_preds['total_male'] + valid_preds['total_female']\n",
    "    valid_preds['night_total'] = valid_preds['night_zanzibar'] + valid_preds['night_mainland']\n",
    "\n",
    "    for column in valid_preds.columns:\n",
    "        empty_count = valid_preds[column].isna().sum()\n",
    "\n",
    "        if empty_count > 0:\n",
    "            print(f\"Column '{column}' has {empty_count} empty fields (NaN).\")\n",
    "\n",
    "\n",
    "    unique_countries_nan = valid_preds[valid_preds['region'].isna()]['country'].unique()\n",
    "    print(\"\\nPaństwa bez regionu: \", unique_countries_nan)\n",
    "\n",
    "    for col in valid_preds.select_dtypes(include=['object']).columns:\n",
    "        if col != \"ID\":\n",
    "            valid_preds[col] = valid_preds[col].astype('category')\n",
    "            new_col = \"cat_\" + str(col)\n",
    "            valid_preds.rename(columns={col: new_col}, inplace=True)\n",
    "    print(valid_preds.columns)\n",
    "    valid_preds.drop(columns=['cat_most_impressing', 'cat_country'], inplace=True)\n",
    "    print(valid_preds.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a21a5971fd217",
   "metadata": {},
   "source": [
    "Challange predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e5be0615358343",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_test_iteration:\n",
    "    id_valid = valid_preds['ID']\n",
    "    X_valid_preds = valid_preds.drop('ID', axis=1)\n",
    "\n",
    "    # WARSTWA 1: Predykcje bazowych modeli\n",
    "    print(\"Warstwa 1: Predykcje modeli bazowych...\")\n",
    "    preds_xgb = xgb_model.predict(X_valid_preds)\n",
    "    preds_cat = cat_model.predict(X_valid_preds)\n",
    "    preds_lgb = lgb_model.predict(X_valid_preds)\n",
    "    preds_ada = ada_pipeline.predict(X_valid_preds)\n",
    "    preds_hgbm = hgbm_model.predict(X_valid_preds)\n",
    "\n",
    "    layer1_features = np.column_stack((preds_xgb, preds_cat, preds_lgb, preds_ada, preds_hgbm))\n",
    "    layer1_features_scaled = scaler.transform(layer1_features)\n",
    "\n",
    "    # WARSTWA 2: Predykcje modeli drugiej warstwy\n",
    "    print(\"Warstwa 2: Predykcje z modeli pośrednich...\")\n",
    "    layer2_predictions = {}\n",
    "\n",
    "    for model_name, model in models_dict.items():\n",
    "        if model_name == \"NN\":\n",
    "            preds = model.predict(layer1_features_scaled, verbose=0).flatten()\n",
    "        elif model_name == \"MLP\":\n",
    "            preds = model.predict(layer1_features_scaled)\n",
    "        else:\n",
    "            preds = model.predict(layer1_features)\n",
    "\n",
    "        layer2_predictions[model_name] = preds\n",
    "\n",
    "    layer2_features = np.column_stack(list(layer2_predictions.values()))\n",
    "\n",
    "\n",
    "    # WARSTWA 3: Generowanie predykcji z finalnych modeli\n",
    "    print(\"Warstwa 3: Generowanie predykcji z finalnych modeli...\")\n",
    "\n",
    "    # 1. Predykcje z modelu NN\n",
    "    nn_final_preds = predict_with_nn_model(final_meta_model, scaler_final.transform(layer2_features)).reshape(-1, 1)\n",
    "    nn_final_preds = nn_final_preds.flatten()\n",
    "\n",
    "    # Tworzenie i zapis pliku submission_nn.csv\n",
    "    submission_nn = pd.DataFrame({\n",
    "        'ID': id_valid,\n",
    "        'total_cost': nn_final_preds\n",
    "    })\n",
    "    submission_nn.to_csv('data/submission_nn.csv', index=False)\n",
    "    print(f\"Zapisano predykcje modelu NN do pliku 'data/submission_nn.csv'\")\n",
    "\n",
    "    # 2. Predykcje z modelu GNN\n",
    "    # Przetworzenie oryginalnych cech dla GNN\n",
    "    X_valid_orig_processed = orig_preprocessor.transform(X_valid_preds)\n",
    "\n",
    "    gnn_final_preds = predict_with_enhanced_gnn(\n",
    "        final_enhanced_model,\n",
    "        layer2_features,\n",
    "        X_valid_orig_processed,\n",
    "        scaler_pred\n",
    "    ).reshape(-1, 1)\n",
    "    gnn_final_preds = gnn_final_preds.flatten()\n",
    "\n",
    "    # Tworzenie i zapis pliku submission_gnn.csv\n",
    "    submission_gnn = pd.DataFrame({\n",
    "        'ID': id_valid,\n",
    "        'total_cost': gnn_final_preds\n",
    "    })\n",
    "    submission_gnn.to_csv('data/submission_gnn.csv', index=False)\n",
    "    print(f\"Zapisano predykcje modelu GNN do pliku 'data/submission_gnn.csv'\")\n",
    "\n",
    "    # Statystyki predykcji\n",
    "    print(\"\\nStatystyki predykcji:\")\n",
    "    print(f\"NN - min: {nn_final_preds.min():.2f}, max: {nn_final_preds.max():.2f}, mean: {nn_final_preds.mean():.2f}\")\n",
    "    print(f\"GNN - min: {gnn_final_preds.min():.2f}, max: {gnn_final_preds.max():.2f}, mean: {gnn_final_preds.mean():.2f}\")\n",
    "\n",
    "    # testy: średnia z obu modeli jako ensemble\n",
    "    ensemble_preds = (nn_final_preds + gnn_final_preds) / 2\n",
    "    submission_ensemble = pd.DataFrame({\n",
    "        'ID': id_valid,\n",
    "        'total_cost': ensemble_preds\n",
    "    })\n",
    "    submission_ensemble.to_csv('data/submission_ensemble.csv', index=False)\n",
    "    print(f\"Zapisano predykcje ensembla (średnia NN+GNN) do pliku 'data/submission_ensemble.csv'\")\n",
    "\n",
    "\n",
    "    # Łączenie predykcji dla meta modelu\n",
    "    # meta_features_new = np.column_stack((preds_xgb, preds_cat, preds_lgb, preds_ada, preds_hgbm))\n",
    "\n",
    "    # Przeskalowanie\n",
    "    # meta_features_new_scaled = scaler.transform(meta_features_new)\n",
    "\n",
    "    # Predykcja finalna\n",
    "    # predictions = meta_model.predict(meta_features_new_scaled)\n",
    "\n",
    "    # if predictions.ndim > 1 and predictions.shape[1] == 1:\n",
    "    #     predictions = predictions.ravel()\n",
    "    #\n",
    "    # # Finalne dane predykcji\n",
    "    # results = pd.DataFrame({\n",
    "    #     'ID': id_valid,\n",
    "    #     'total_cost': predictions\n",
    "    # })\n",
    "    #\n",
    "    #\n",
    "    # results.to_csv('data/submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ae387",
   "metadata": {},
   "source": [
    "Sieć neuronowa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845497df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, LSTM, Flatten\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"R2\": r2_score(y_true, y_pred),\n",
    "        \"MAPE\": mape(y_true, y_pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X = train.drop(columns=[\"ID\", \"total_cost\"])\n",
    "y = train[\"total_cost\"]\n",
    "X_test_final = test.drop(columns=[\"ID\"])\n",
    "\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test_final)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56852dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, name):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "    end_time = time.time()\n",
    "    y_pred = model.predict(X_val).flatten()\n",
    "    metrics = evaluate_metrics(y_val, y_pred)\n",
    "    metrics['Model'] = name\n",
    "    metrics['TrainTime'] = end_time - start_time\n",
    "    return metrics, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17560884",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = np.expand_dims(X_train.toarray() if hasattr(X_train, 'toarray') else X_train, axis=-1)\n",
    "X_val_seq = np.expand_dims(X_val.toarray() if hasattr(X_val, 'toarray') else X_val, axis=-1)\n",
    "input_shape_seq = X_train_seq.shape[1:]\n",
    "input_shape_flat = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ebd85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp():\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape_flat,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_cnn():\n",
    "    model = Sequential([\n",
    "        Conv1D(64, 3, activation='relu', input_shape=input_shape_seq),\n",
    "        MaxPooling1D(2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_lstm():\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=input_shape_seq),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_resnet():\n",
    "    inputs = tf.keras.Input(shape=(input_shape_flat,))\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    residual = x\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.add([x, residual])\n",
    "    outputs = Dense(1)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_autoencoder():\n",
    "    input_layer = tf.keras.Input(shape=(input_shape_flat,))\n",
    "    encoded = Dense(64, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_shape_flat, activation='linear')(encoded)\n",
    "    autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "model_builders = [\n",
    "    (build_mlp, \"MLP\"),\n",
    "    (build_cnn, \"CNN\"),\n",
    "    (build_lstm, \"LSTM\"),\n",
    "    (build_resnet, \"ResNetMLP\")\n",
    "]\n",
    "\n",
    "for builder, name in model_builders:\n",
    "    model = builder()\n",
    "    X_tr = X_train_seq if \"CNN\" in name or \"LSTM\" in name else X_train\n",
    "    X_vl = X_val_seq if \"CNN\" in name or \"LSTM\" in name else X_val\n",
    "    metrics, _ = train_model(model, X_tr, y_train, X_vl, y_val, name)\n",
    "    results.append(metrics)\n",
    "\n",
    "autoencoder = build_autoencoder()\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), verbose=0)\n",
    "X_train_encoded = autoencoder.predict(X_train)\n",
    "X_val_encoded = autoencoder.predict(X_val)\n",
    "mlp_on_ae = build_mlp()\n",
    "metrics, _ = train_model(mlp_on_ae, X_train_encoded, y_train, X_val_encoded, y_val, \"Autoencoder+MLP\")\n",
    "results.append(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "esults_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x=\"Model\", y=\"R2\")\n",
    "plt.title(\"R² Scores of Models\")\n",
    "plt.show()\n",
    "\n",
    "results_df.to_csv(\"model_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69669291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, LSTM, Flatten, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.ensemble import AdaBoostRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class BaseModel:\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return self.model.fit(X, y, **kwargs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        return self.model.evaluate(X, y)\n",
    "\n",
    "\n",
    "class SimpleMLP(BaseModel):\n",
    "    def __init__(self, input_shape, output_dim=1):\n",
    "        self.model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "            Dropout(0.2),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(output_dim)\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "class ConvNet1D(BaseModel):\n",
    "    def __init__(self, input_shape, output_dim=1):\n",
    "        self.model = Sequential([\n",
    "            Conv1D(64, 3, activation='relu', input_shape=input_shape),\n",
    "            MaxPooling1D(2),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(output_dim)\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "class LSTMRegressor(BaseModel):\n",
    "    def __init__(self, input_shape, output_dim=1):\n",
    "        self.model = Sequential([\n",
    "            LSTM(64, input_shape=input_shape),\n",
    "            Dense(output_dim)\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "class ResNetMLP(BaseModel):\n",
    "    def __init__(self, input_shape, output_dim=1):\n",
    "        inputs = tf.keras.Input(shape=(input_shape,))\n",
    "        x = Dense(128, activation='relu')(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        residual = x\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.add([x, residual])\n",
    "        outputs = Dense(output_dim)(x)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "class ConfigurableNeuralNetwork(BaseModel):\n",
    "    def __init__(self, input_shape, network_type='mlp', output_dim=1,\n",
    "                 hidden_units=[128, 64], dropout_rate=0.2, optimizer='adam'):\n",
    "        self.input_shape = input_shape\n",
    "        self.network_type = network_type\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        if self.network_type == 'mlp':\n",
    "            model.add(Dense(self.hidden_units[0], activation='relu', input_shape=(self.input_shape,)))\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "            for units in self.hidden_units[1:]:\n",
    "                model.add(Dense(units, activation='relu'))\n",
    "                model.add(Dropout(self.dropout_rate))\n",
    "        elif self.network_type == 'cnn':\n",
    "            model.add(Conv1D(64, 3, activation='relu', input_shape=self.input_shape))\n",
    "            model.add(MaxPooling1D(2))\n",
    "            model.add(Flatten())\n",
    "        elif self.network_type == 'lstm':\n",
    "            model.add(LSTM(64, input_shape=self.input_shape))\n",
    "        elif self.network_type == 'cnn_lstm':\n",
    "            model.add(Conv1D(64, 3, activation='relu', input_shape=self.input_shape))\n",
    "            model.add(MaxPooling1D(2))\n",
    "            model.add(LSTM(64))\n",
    "        model.add(Dense(self.output_dim))\n",
    "        model.compile(optimizer=self.optimizer, loss='mse')\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(patience=3)\n",
    "        ]\n",
    "        return self.model.fit(X, y, callbacks=callbacks, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, **kwargs):\n",
    "        model = tf.keras.models.load_model(filepath)\n",
    "        input_shape = model.input_shape[1:]\n",
    "        output_dim = model.output_shape[-1]\n",
    "        instance = cls(input_shape=input_shape, output_dim=output_dim, **kwargs)\n",
    "        instance.model = model\n",
    "        return instance\n",
    "\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "cat_model = CatBoostRegressor(verbose=0)\n",
    "lgb_model = LGBMRegressor()\n",
    "ada_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', AdaBoostRegressor())\n",
    "])\n",
    "hgbm_model = HistGradientBoostingRegressor()\n",
    "\n",
    "ridge_stack = Ridge()\n",
    "\n",
    "final_stack = LinearRegression()\n",
    "\n",
    "\n",
    "# xgb_model.fit(X_train_preds, y_train_preds)\n",
    "# preds_xgb = xgb_model.predict(X_valid_preds)\n",
    "\n",
    "# layer1_features = np.column_stack((preds_xgb, preds_cat, preds_lgb, preds_ada, preds_hgbm))\n",
    "# ridge_stack.fit(layer1_features, y_valid_preds)\n",
    "# preds_ridge = ridge_stack.predict(layer1_features)\n",
    "\n",
    "# final_stack.fit(np.column_stack((preds_ridge,)), y_valid_preds)\n",
    "# preds_final = final_stack.predict(np.column_stack((preds_ridge,)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
